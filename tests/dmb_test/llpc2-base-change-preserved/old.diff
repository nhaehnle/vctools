diff --git a/lgc/builder/DescBuilder.cpp b/lgc/builder/DescBuilder.cpp
index c09f82663..b28c29a5d 100644
--- a/lgc/builder/DescBuilder.cpp
+++ b/lgc/builder/DescBuilder.cpp
@@ -38,52 +38,60 @@
 #include "llvm/IR/IntrinsicsAMDGPU.h"
 
 #define DEBUG_TYPE "lgc-builder-impl-desc"
 
 using namespace lgc;
 using namespace llvm;
 
 // =====================================================================================================================
 // Create a load of a buffer descriptor.
 //
+// If descSet = -1, this is an internal user data, which is a plain 64-bit pointer, flags must be 'BufferFlagAddress'
+// i64 address is returned.
+//
 // @param descSet : Descriptor set
 // @param binding : Descriptor binding
 // @param descIndex : Descriptor index
 // @param flags : BufferFlag* bit settings
 // @param pointeeTy : Type that the returned pointer should point to.
 // @param instName : Name to give instruction(s)
 Value *DescBuilder::CreateLoadBufferDesc(unsigned descSet, unsigned binding, Value *descIndex, unsigned flags,
                                          Type *const pointeeTy, const Twine &instName) {
   Value *desc = nullptr;
+  bool return64Address = false;
   descIndex = scalarizeIfUniform(descIndex, flags & BufferFlagNonUniform);
 
   // Mark the shader as reading and writing (if applicable) a resource.
   auto resUsage = getPipelineState()->getShaderResourceUsage(m_shaderStage);
   resUsage->resourceRead = true;
   if (flags & BufferFlagWritten)
     resUsage->resourceWrite = true;
+  else if (flags & BufferFlagAddress)
+    return64Address = true;
 
   // Find the descriptor node. If doing a shader compilation with no user data layout provided, don't bother to
   // look. Later code will use relocs.
   const ResourceNode *topNode = nullptr;
   const ResourceNode *node = nullptr;
   if (!m_pipelineState->isUnlinked() || !m_pipelineState->getUserDataNodes().empty()) {
     // We have the user data layout. Find the node.
     ResourceNodeType abstractType = ResourceNodeType::Unknown;
     if (flags & BufferFlagConst)
       abstractType = ResourceNodeType::DescriptorConstBuffer;
     else if (flags & BufferFlagNonConst)
       abstractType = ResourceNodeType::DescriptorBuffer;
     else if (flags & BufferFlagShaderResource)
       abstractType = ResourceNodeType::DescriptorResource;
     else if (flags & BufferFlagSampler)
       abstractType = ResourceNodeType::DescriptorSampler;
+    else if (flags & BufferFlagAddress)
+      abstractType = ResourceNodeType::DescriptorBufferCompact;
 
     std::tie(topNode, node) = m_pipelineState->findResourceNode(abstractType, descSet, binding);
     assert(node && "missing resource node");
 
     if (node == topNode && isa<Constant>(descIndex) && node->concreteType != ResourceNodeType::InlineBuffer) {
       // Handle a descriptor in the root table (a "dynamic descriptor") specially, as long as it is not variably
       // indexed and is not an InlineBuffer. This lgc.root.descriptor call is by default lowered in
       // PatchEntryPointMutate into a load from the spill table, but it might be able to "unspill" it to
       // directly use shader entry SGPRs.
       // TODO: Handle root InlineBuffer specially in a similar way to PushConst. The default handling is
@@ -94,51 +102,91 @@ Value *DescBuilder::CreateLoadBufferDesc(unsigned descSet, unsigned binding, Val
       unsigned dwordSize = descTy->getPrimitiveSizeInBits() / 32;
       unsigned dwordOffset = cast<ConstantInt>(descIndex)->getZExtValue() * dwordSize;
       if (dwordOffset + dwordSize > node->sizeInDwords) {
         // Index out of range
         desc = UndefValue::get(descTy);
       } else {
         dwordOffset += node->offsetInDwords;
         dwordOffset += (binding - node->binding) * node->stride;
         desc = CreateNamedCall(callName, descTy, getInt32(dwordOffset), Attribute::ReadNone);
       }
+      if (return64Address) {
+        assert(node->concreteType == ResourceNodeType::DescriptorBufferCompact);
+        return CreateBitCast(desc, getInt64Ty());
+      }
     } else if (node->concreteType == ResourceNodeType::InlineBuffer) {
       // Handle an inline buffer specially. Get a pointer to it, then expand to a descriptor.
       Value *descPtr = getDescPtr(node->concreteType, node->abstractType, descSet, binding, topNode, node);
       desc = buildInlineBufferDesc(descPtr);
     }
   }
 
   if (!desc) {
-    // Not handled by either of the special cases above...
-    // Get a pointer to the descriptor, as a pointer to i8.
-    // For shader compilation with no user data layout provided, we assume we want a DescriptorBuffer, as
-    // DescriptorConstBuffer is not used in that case.
-    ResourceNodeType resType = node ? node->concreteType : ResourceNodeType::DescriptorBuffer;
-    ResourceNodeType abstractType = node ? node->abstractType : resType;
-    Value *descPtr = getDescPtr(resType, abstractType, descSet, binding, topNode, node);
-    // Index it.
-    if (descIndex != getInt32(0)) {
-      descIndex = CreateMul(descIndex, getStride(resType, descSet, binding, node));
-      descPtr = CreateGEP(getInt8Ty(), descPtr, descIndex);
+    if (node) {
+      ResourceNodeType resType = node->concreteType;
+      ResourceNodeType abstractType = node->abstractType;
+      Value *descPtr = getDescPtr(resType, abstractType, descSet, binding, topNode, node);
+      // Index it.
+      if (descIndex != getInt32(0)) {
+        descIndex = CreateMul(descIndex, getStride(resType, descSet, binding, node));
+        descPtr = CreateGEP(getInt8Ty(), descPtr, descIndex);
+      }
+      // Cast it to the right type.
+      descPtr = CreateBitCast(descPtr, getDescPtrTy(resType));
+      // Load the descriptor.
+      desc = CreateLoad(getDescTy(resType), descPtr);
+    } else {
+      // For shader compilation with no user data layout provided, we don't know if the buffer is dynamic descriptor,
+      // We need to load two dwords for DescriptorBufferCompact, 4 dwords for DescriptorBuffer. To avoid out of bound,
+      // we will use two loads and load two dwords for each time. If the resource type is really DescriptorBuffer, the
+      // address of the second load will add 8 bytes, otherwise the address is the same as the first, it means we load
+      // the same data twice, but the data is not used.
+
+      // Get the descriptor pointer which is from ResourceMapping, ignore the resource type.
+      ResourceNodeType resType = ResourceNodeType::DescriptorBuffer;
+      ResourceNodeType abstractType = resType;
+      Value *descPtr = getDescPtr(resType, abstractType, descSet, binding, nullptr, nullptr);
+      // Index it.
+      if (descIndex != getInt32(0)) {
+        descIndex = CreateMul(descIndex, getStride(resType, descSet, binding, nullptr));
+        descPtr = CreateGEP(getInt8Ty(), descPtr, descIndex);
+      }
+
+      auto descPtrLo = CreateBitCast(descPtr, FixedVectorType::get(getInt32Ty(), 2)->getPointerTo(ADDR_SPACE_CONST));
+      // The first load
+      auto descLo = CreateLoad(FixedVectorType::get(getInt32Ty(), 2), descPtrLo);
+      auto compactBufferDesc = buildBufferCompactDesc(descLo);
+
+      // If descriptor set is -1, this is a internal resource node, it is a root node
+      // and its type is ResourceNodeType::DescriptorBufferCompact.
+      if (descSet == -1) {
+        assert(return64Address);
+        return CreateBitCast(descLo, getInt64Ty());
+      } else {
+        // Add offset
+        Value *descPtrHi = CreateAddByteOffset(descPtr, getInt32(8));
+        auto reloc = CreateRelocationConstant(reloc::CompactBuffer + Twine(descSet) + "_" + Twine(binding));
+        auto isCompactBuffer = CreateICmpNE(reloc, getInt32(0));
+        // Select the address
+        descPtrHi = CreateSelect(isCompactBuffer, descPtr, descPtrHi);
+        descPtrHi = CreateBitCast(descPtrHi, FixedVectorType::get(getInt32Ty(), 2)->getPointerTo(ADDR_SPACE_CONST));
+        // The second load
+        auto descHi = CreateLoad(FixedVectorType::get(getInt32Ty(), 2), descPtrHi);
+        // Merge the whole descriptor for DescriptorBuffer
+        auto bufferDesc = CreateShuffleVector(descLo, descHi, {0, 1, 2, 3});
+        // Select
+        desc = CreateSelect(isCompactBuffer, compactBufferDesc, bufferDesc);
+      }
     }
-    // Cast it to the right type.
-    descPtr = CreateBitCast(descPtr, getDescPtrTy(resType));
-    // Load the descriptor.
-    desc = CreateLoad(getDescTy(resType), descPtr);
   }
-
-  // If it is a compact buffer descriptor, expand it. (That can only happen when user data layout is available;
-  // compact buffer descriptors are disallowed when using shader compilation with no user data layout).
-  if (node && node->concreteType == ResourceNodeType::DescriptorBufferCompact)
-    desc = buildBufferCompactDesc(desc);
-  else if (node && node->concreteType == ResourceNodeType::DescriptorConstBufferCompact)
+  if (node && (node->concreteType == ResourceNodeType::DescriptorBufferCompact ||
+               node->concreteType == ResourceNodeType::DescriptorConstBufferCompact))
     desc = buildBufferCompactDesc(desc);
 
   if (!instName.isTriviallyEmpty())
     desc->setName(instName);
 
   // Convert to fat pointer.
   desc = CreateNamedCall(lgcName::LateLaunderFatPointer, getInt8Ty()->getPointerTo(ADDR_SPACE_BUFFER_FAT_POINTER), desc,
                          Attribute::ReadNone);
   return CreateBitCast(desc, getBufferDescTy(pointeeTy));
 }
@@ -381,45 +429,55 @@ Value *DescBuilder::getDescPtr(ResourceNodeType concreteType, ResourceNodeType a
     Value *useShadowReloc = CreateRelocationConstant(reloc::ShadowDescriptorTableEnabled);
     Value *useShadowTable = CreateICmpNE(useShadowReloc, getInt32(0));
     return CreateSelect(useShadowTable, shadowAddr, nonShadowAddr);
   };
 
   // Get the descriptor table pointer.
   if (node && node == topNode) {
     // Ensure we mark spill table usage.
     descPtr = GetSpillTablePtr();
     getPipelineState()->getPalMetadata()->setUserDataSpillUsage(node->offsetInDwords);
-  } else if (!node && !topNode && concreteType == ResourceNodeType::DescriptorBuffer) {
+  } else if (!node && !topNode &&
+             (concreteType == ResourceNodeType::DescriptorBuffer ||
+              concreteType == ResourceNodeType::DescriptorBufferCompact)) {
     // If we do not have user data layout info (topNode and node are nullptr), then
     // we do not know at compile time whether a DescriptorBuffer is in the root table or the table for its
     // descriptor set, so we need to generate a select between the two, where the condition is a reloc.
     // If the descriptor ends up in the root table (top-level), a value from the spill table will be used.
     // The linking code has to take care of marking PAL metadata for user spill usage.
 
     // Since the descriptor pointers will be later formed by bitcasting v2i32 to i8* we bitcast them to v2i32
     // here. This enables the middle-end to eliminate i8* before doing the instruction selection and reason about high
     // and low parts of the pointers producing better code overall.
     Value *spillDescPtr = GetSpillTablePtr();
-    // Bitcast the pointer to v2i32.
-    spillDescPtr = CreatePtrToInt(spillDescPtr, getInt64Ty());
-    spillDescPtr = CreateBitCast(spillDescPtr, FixedVectorType::get(getInt32Ty(), 2));
-
-    Value *descriptorTableDescPtr = GetDescriptorSetPtr();
-    // Bitcast the pointer to v2i32.
-    descriptorTableDescPtr = CreatePtrToInt(descriptorTableDescPtr, getInt64Ty());
-    descriptorTableDescPtr = CreateBitCast(descriptorTableDescPtr, FixedVectorType::get(getInt32Ty(), 2));
-
-    Value *reloc = CreateRelocationConstant(reloc::DescriptorUseSpillTable + Twine(descSet) + "_" + Twine(binding));
-    Value *useSpillTable = CreateICmpNE(reloc, getInt32(0));
-    descPtr = CreateSelect(useSpillTable, spillDescPtr, descriptorTableDescPtr);
-    descPtr = CreateBitCast(descPtr, getInt64Ty());
-    descPtr = CreateIntToPtr(descPtr, getInt8Ty()->getPointerTo(ADDR_SPACE_CONST));
+
+    if (descSet == -1) {
+      // If descriptor set is -1, this is a internal resource node, it is a root node
+      // and its type is ResourceNodeType::DescriptorBufferCompact. We use spillTable to load it.
+      descPtr = spillDescPtr;
+      m_pipelineState->getPalMetadata()->setUserDataSpillUsage(0);
+    } else {
+      // Bitcast the pointer to v2i32.
+      spillDescPtr = CreatePtrToInt(spillDescPtr, getInt64Ty());
+      spillDescPtr = CreateBitCast(spillDescPtr, FixedVectorType::get(getInt32Ty(), 2));
+
+      Value *descriptorTableDescPtr = GetDescriptorSetPtr();
+      // Bitcast the pointer to v2i32.
+      descriptorTableDescPtr = CreatePtrToInt(descriptorTableDescPtr, getInt64Ty());
+      descriptorTableDescPtr = CreateBitCast(descriptorTableDescPtr, FixedVectorType::get(getInt32Ty(), 2));
+
+      Value *reloc = CreateRelocationConstant(reloc::DescriptorUseSpillTable + Twine(descSet) + "_" + Twine(binding));
+      Value *useSpillTable = CreateICmpNE(reloc, getInt32(0));
+      descPtr = CreateSelect(useSpillTable, spillDescPtr, descriptorTableDescPtr);
+      descPtr = CreateBitCast(descPtr, getInt64Ty());
+      descPtr = CreateIntToPtr(descPtr, getInt8Ty()->getPointerTo(ADDR_SPACE_CONST));
+    }
   } else {
     descPtr = GetDescriptorSetPtr();
   }
 
   // Add on the byte offset of the descriptor.
   Value *offset = nullptr;
   if (!node) {
     // Shader compilation with no user data layout. Get the offset for the descriptor using a reloc. The
     // reloc symbol name needs to contain the descriptor set and binding, and, for image, fmask or sampler,
     // whether it is a sampler.
diff --git a/lgc/elfLinker/RelocHandler.cpp b/lgc/elfLinker/RelocHandler.cpp
index fab21f675..48e61a009 100644
--- a/lgc/elfLinker/RelocHandler.cpp
+++ b/lgc/elfLinker/RelocHandler.cpp
@@ -98,22 +98,20 @@ bool RelocHandler::getValue(StringRef name, uint64_t &value) {
     unsigned descSet = 0;
     unsigned binding = 0;
     ResourceNodeType type = ResourceNodeType::Unknown;
     if (parseDescSetBinding(name.drop_front(strlen(reloc::DescriptorOffset)), descSet, binding, type)) {
       const ResourceNode *outerNode = nullptr;
       const ResourceNode *node = nullptr;
       std::tie(outerNode, node) = getPipelineState()->findResourceNode(type, descSet, binding);
 
       if (!node)
         report_fatal_error("No resource node for " + name);
-      if (node->concreteType == ResourceNodeType::DescriptorBufferCompact)
-        getPipelineState()->setError("Cannot relocate to compact buffer descriptor");
 
       value = node->offsetInDwords * 4;
       if (type == ResourceNodeType::DescriptorSampler &&
           node->concreteType == ResourceNodeType::DescriptorCombinedTexture)
         value += DescriptorSizeResource;
       return true;
     }
   }
 
   if (name.startswith(reloc::DescriptorTableOffset)) {
@@ -146,21 +144,20 @@ bool RelocHandler::getValue(StringRef name, uint64_t &value) {
     StringRef suffix = name.drop_front(strlen(reloc::DescriptorUseSpillTable));
     if (parseDescSetBinding(suffix, descSet, binding, type)) {
       const ResourceNode *outerNode = nullptr;
       const ResourceNode *node = nullptr;
       std::tie(outerNode, node) =
           getPipelineState()->findResourceNode(ResourceNodeType::DescriptorBuffer, descSet, binding);
 
       if (!node)
         report_fatal_error("No resource node for " + name);
 
-      assert(node->concreteType == ResourceNodeType::DescriptorBuffer);
       // Check if this is a top-level node.
       value = (node == outerNode) ? 1 : 0;
 
       // Mark access to spill table.
       if (value == 1)
         m_pipelineState->getPalMetadata()->setUserDataSpillUsage(0);
 
       return true;
     }
   }
@@ -174,20 +171,39 @@ bool RelocHandler::getValue(StringRef name, uint64_t &value) {
       const ResourceNode *outerNode = nullptr;
       const ResourceNode *node = nullptr;
       std::tie(outerNode, node) = getPipelineState()->findResourceNode(type, descSet, binding);
       if (!node)
         report_fatal_error("No resource node for " + name);
       value = node->stride * sizeof(uint32_t);
       return true;
     }
   }
 
+  if (name.startswith(reloc::CompactBuffer)) {
+    // Descriptor stride in bytes.
+    unsigned descSet = 0;
+    unsigned binding = 0;
+    ResourceNodeType type = ResourceNodeType::Unknown;
+    if (parseDescSetBinding(name.drop_front(strlen(reloc::CompactBuffer)), descSet, binding, type)) {
+      const ResourceNode *outerNode = nullptr;
+      const ResourceNode *node = nullptr;
+      std::tie(outerNode, node) = getPipelineState()->findResourceNode(type, descSet, binding);
+      if (!node)
+        report_fatal_error("No resource node for " + name);
+      value = (node->concreteType == ResourceNodeType::DescriptorBufferCompact ||
+               node->concreteType == ResourceNodeType::DescriptorConstBufferCompact)
+                  ? 1
+                  : 0;
+      return true;
+    }
+  }
+
   if (name == reloc::NumSamples) {
     value = m_pipelineState->getRasterizerState().numSamples;
     return true;
   }
   if (name == reloc::SamplePatternIdx) {
     value = m_pipelineState->getRasterizerState().samplePatternIdx;
     return true;
   }
   if (name == reloc::DeviceIdx) {
     value = m_pipelineState->getDeviceIndex();
diff --git a/lgc/include/lgc/patch/VertexFetch.h b/lgc/include/lgc/patch/VertexFetch.h
index e715a1f9f..721985ead 100644
--- a/lgc/include/lgc/patch/VertexFetch.h
+++ b/lgc/include/lgc/patch/VertexFetch.h
@@ -42,20 +42,23 @@ class BuilderBase;
 class VertexFetch {
 public:
   virtual ~VertexFetch() {}
 
   // Create a VertexFetch
   static VertexFetch *create(LgcContext *lgcContext);
 
   // Generate code to fetch a vertex value
   virtual llvm::Value *fetchVertex(llvm::Type *inputTy, const VertexInputDescription *description, unsigned location,
                                    unsigned compIdx, BuilderBase &builder) = 0;
+
+  // Generate code to fetch a vertex value for uber shader
+  virtual llvm::Value *fetchVertex(llvm::CallInst *vertexFetches, llvm::Value *inputDesc, BuilderBase &builder) = 0;
 };
 
 // =====================================================================================================================
 // Pass to lower vertex fetch calls
 class LowerVertexFetch : public llvm::PassInfoMixin<LowerVertexFetch> {
 public:
   llvm::PreservedAnalyses run(llvm::Module &module, llvm::ModuleAnalysisManager &analysisManager);
 
   bool runImpl(llvm::Module &module, PipelineState *pipelineState);
 
diff --git a/lgc/include/lgc/state/AbiUnlinked.h b/lgc/include/lgc/state/AbiUnlinked.h
index 874103e6b..a44c99f02 100644
--- a/lgc/include/lgc/state/AbiUnlinked.h
+++ b/lgc/include/lgc/state/AbiUnlinked.h
@@ -72,20 +72,28 @@ const static char DescriptorTableOffset[] = "descset_";
 // Descriptor stride is "dstride_X_Y" where:
 // * X is the descriptor set number
 // * Y is the binding number
 //
 // The value of the relocation is stride in bytes of the requested array of descriptors in its descriptor set
 // table. It is illegal for the specified descriptor not to exist, and it must be a resource, fmask or sampler
 // descriptor. (The reason the stride needs to be a reloc is that the requested descriptor might or might not
 // be part of a combined texture descriptor.)
 const static char DescriptorStride[] = "dstride_";
 
+// The buffer type is "compactbuffer_X_Y" where:
+// * X is the descriptor set number
+// * Y is the binding number
+//
+
+// The value of the relocation is whether resource is ResourceNodeType::DescriptorBufferCompact.
+const static char CompactBuffer[] = "compactbuffer_";
+
 // Number of samples is "$numsamples".
 // The value of the relocation is numSamples from the rasterizer state.
 const static char NumSamples[] = "$numsamples";
 
 // Sample pattern index is "$samplePatternIdx".
 // The value of the relocation is samplePatternIdx from the rasterizer state.
 const static char SamplePatternIdx[] = "$samplePatternIdx";
 
 // Device index is "$deviceIdx"
 // The value of the relocation is deviceIdx from the pipeline state.
diff --git a/lgc/interface/lgc/Builder.h b/lgc/interface/lgc/Builder.h
index 9e038b68c..7a050e4b0 100644
--- a/lgc/interface/lgc/Builder.h
+++ b/lgc/interface/lgc/Builder.h
@@ -697,28 +697,32 @@ public:
 
   // Bit settings for flags argument in CreateLoadBufferDesc.
   enum {
     BufferFlagNonUniform = 1, // Descriptor index is non-uniform
     BufferFlagWritten = 2,    // Buffer is (or might be) written to
     BufferFlagConst = 4,      // Const buffer: Find a DescriptorConstBuffer/DescriptorConstBufferCompact/InlineBuffer
                               //  descriptor entry, rather than DescriptorBuffer/DescriptorBufferCompact
     BufferFlagNonConst = 8,   // Non-const buffer: Find a DescriptorBuffer/DescriptorBufferCompact descriptor
                               //  entry, rather than DescriptorConstBuffer/DescriptorConstBufferCompact/InlineBuffer
     BufferFlagShaderResource = 16, // Flag to find a Descriptor Resource
-    BufferFlagSampler = 32         // Flag to find Descriptor Sampler
+    BufferFlagSampler = 32,        // Flag to find Descriptor Sampler
+    BufferFlagAddress = 64         // Flag to return an i64 address of the descriptor
   };
 
   // Get the type of pointer returned by CreateLoadBufferDesc.
   llvm::PointerType *getBufferDescTy(llvm::Type *pointeeTy);
 
   // Create a load of a buffer descriptor.
   //
+  // If descSet = -1, this is an internal user data, which is a plain 64-bit pointer, flags must be 'BufferFlagAddress'
+  // i64 address is returned.
+  //
   // @param descSet : Descriptor set
   // @param binding : Descriptor binding
   // @param descIndex : Descriptor index
   // @param flags : BufferFlag* bit settings
   // @param pointeeTy : Type that the returned pointer should point to.
   // @param instName : Name to give instruction(s)
   virtual llvm::Value *CreateLoadBufferDesc(unsigned descSet, unsigned binding, llvm::Value *descIndex, unsigned flags,
                                             llvm::Type *pointeeTy, const llvm::Twine &instName = "") = 0;
 
   // Get the type of a descriptor
diff --git a/lgc/interface/lgc/Pipeline.h b/lgc/interface/lgc/Pipeline.h
index 385b33127..44f616e6a 100644
--- a/lgc/interface/lgc/Pipeline.h
+++ b/lgc/interface/lgc/Pipeline.h
@@ -145,20 +145,21 @@ struct Options {
   ResourceLayoutScheme resourceLayoutScheme;     // Resource layout scheme
   ThreadGroupSwizzleMode threadGroupSwizzleMode; // Thread group swizzle mode
   unsigned reverseThreadGroupBufferDescSet;      // Descriptor set ID of the internal buffer for reverse thread group
                                                  // optimization
   unsigned reverseThreadGroupBufferBinding; // Binding ID of the internal buffer for reverse thread group optimization
 #if VKI_RAY_TRACING
   bool internalRtShaders; // Enable internal RT shader intrinsics
 #else
   bool reserved15;
 #endif
+  bool enableUberFetchShader; // Enable UberShader
 };
 
 /// Represent a pipeline option which can be automatic as well as explicitly set.
 enum InvariantLoadsOption : unsigned { Auto = 0, EnableOptimization = 1, DisableOptimization = 2, ClearInvariants = 3 };
 
 // Middle-end per-shader options to pass to SetShaderOptions.
 // Note: new fields must be added to the end of this structure to maintain test compatibility.
 struct ShaderOptions {
   uint64_t hash[2];     // Shader hash to set in ELF PAL metadata
   unsigned trapPresent; // Indicates a trap handler will be present when this pipeline is executed,
@@ -401,20 +402,36 @@ struct VertexInputDescription {
   unsigned offset;    // Byte offset of the input in the binding's vertex buffer
   unsigned stride;    // Byte stride of per-vertex/per-instance elements in the vertex buffer, 0 if unknown.
                       // The stride is passed only to ensure that a valid load is used, not to actually calculate
                       // the load address. Instead, we use the index as the index in a structured tbuffer load
                       // instruction, and rely on the driver setting up the descriptor with the correct stride.
   BufDataFormat dfmt; // Data format of input; one of the BufDataFormat* values
   BufNumFormat nfmt;  // Numeric format of input; one of the BufNumFormat* values
   unsigned inputRate; // Vertex input rate for the binding
 };
 
+// Represents assistant info for each vertex attribute in uber fetch shader
+struct UberFetchShaderAttribInfo {
+  uint32_t binding : 8;       //< Attribute binding in vertex buffer table
+  uint32_t perInstance : 1;   //< Whether vertex input rate is per-instance
+  uint32_t isCurrent : 1;     //< Whether it is a current attribute
+  uint32_t isPacked : 1;      //< Whether it is a packed format
+  uint32_t isFixed : 1;       //< Whether it is a fixed format
+  uint32_t componentSize : 4; //< Byte size per component
+  uint32_t componentMask : 4; //< Component mask of this attribute.
+  uint32_t isBgra : 1;        //< Whether is BGRA format
+  uint32_t reserved : 11;     //< reserved bits in DWORD 0
+  uint32_t offset;            //< Attribute offset
+  uint32_t instanceDivisor;   //< Reciprocal of instance divisor
+  uint32_t bufferFormat;      //< Buffer format info. it is a copy of buffer SRD DWORD3.
+};
+
 // A single color export format.
 struct ColorExportFormat {
   BufDataFormat dfmt;            // Data format
   BufNumFormat nfmt;             // Numeric format
   unsigned blendEnable;          // Blend will be enabled for this target at draw time
   unsigned blendSrcAlphaToColor; // Whether source alpha is blended to color channels for this target
                                  //  at draw time
 };
 
 // Struct to pass to SetColorExportState
diff --git a/lgc/patch/PassRegistry.inc b/lgc/patch/PassRegistry.inc
index f04ef980b..4affe32b4 100644
--- a/lgc/patch/PassRegistry.inc
+++ b/lgc/patch/PassRegistry.inc
@@ -56,16 +56,17 @@ LLPC_FUNCTION_PASS("lgc-patch-read-first-lane", PatchReadFirstLane)
 LLPC_MODULE_PASS("lgc-patch-llvm-ir-inclusion", PatchLlvmIrInclusion)
 LLPC_MODULE_PASS("lgc-patch-wave-size-adjust", PatchWaveSizeAdjust)
 LLPC_FUNCTION_PASS("lgc-patch-peephole-opt", PatchPeepholeOpt)
 LLPC_MODULE_PASS("lgc-patch-entry-point-mutate", PatchEntryPointMutate)
 LLPC_MODULE_PASS("lgc-patch-check-shader-cache", PatchCheckShaderCache)
 LLPC_LOOP_PASS("lgc-patch-loop-metadata", PatchLoopMetadata)
 LLPC_FUNCTION_PASS("lgc-patch-buffer-op", PatchBufferOp)
 LLPC_MODULE_PASS("lgc-patch-workarounds", PatchWorkarounds)
 LLPC_FUNCTION_PASS("lgc-patch-load-scalarizer", PatchLoadScalarizer)
 LLPC_MODULE_PASS("lgc-patch-null-frag-shader", PatchNullFragShader)
+LLPC_MODULE_PASS("lgc-vertex-fetch", LowerVertexFetch)
 
 #undef LLPC_PASS
 #undef LLPC_LOOP_PASS
 #undef LLPC_FUNCTION_PASS
 #undef LLPC_MODULE_PASS
 #undef LLPC_MODULE_PASS_WITH_PARSER
diff --git a/lgc/patch/PatchEntryPointMutate.cpp b/lgc/patch/PatchEntryPointMutate.cpp
index 50d5c39ed..a57845625 100644
--- a/lgc/patch/PatchEntryPointMutate.cpp
+++ b/lgc/patch/PatchEntryPointMutate.cpp
@@ -407,20 +407,24 @@ void PatchEntryPointMutate::gatherUserDataUsage(Module *module) {
 //
 // @param module : IR module
 void PatchEntryPointMutate::fixupUserDataUses(Module &module) {
   BuilderBase builder(module.getContext());
 
   // For each function definition...
   for (Function &func : module) {
     if (func.isDeclaration())
       continue;
 
+    // Only entrypoint and amd_gfx functions use user data, others don't use.
+    if (!isShaderEntryPoint(&func) && (func.getCallingConv() != CallingConv::AMDGPU_Gfx))
+      continue;
+
     ShaderStage stage = getShaderStage(&func);
     auto userDataUsage = getUserDataUsage(stage);
 
     // If needed, generate code for the spill table pointer (as pointer to i8) at the start of the function.
     Instruction *spillTable = nullptr;
     AddressExtender addressExtender(&func);
     if (userDataUsage->spillTable.entryArgIdx != 0) {
       builder.SetInsertPoint(addressExtender.getFirstInsertionPt());
       Argument *arg = getFunctionArgument(&func, userDataUsage->spillTable.entryArgIdx);
       spillTable = addressExtender.extend(arg, builder.getInt32(HighAddrPc),
diff --git a/lgc/patch/ShaderInputs.cpp b/lgc/patch/ShaderInputs.cpp
index 8f90dd0bc..b478c6ac9 100644
--- a/lgc/patch/ShaderInputs.cpp
+++ b/lgc/patch/ShaderInputs.cpp
@@ -314,20 +314,24 @@ void ShaderInputs::gatherUsage(Module &module) {
 // =====================================================================================================================
 // Fix up uses of shader inputs to use entry args directly
 //
 // @param module : IR module
 void ShaderInputs::fixupUses(Module &module, PipelineState *pipelineState) {
   // For each function definition...
   for (Function &func : module) {
     if (func.isDeclaration())
       continue;
 
+    // Only entrypoint and amd_gfx functions use user data, others don't use.
+    if (!isShaderEntryPoint(&func) && (func.getCallingConv() != CallingConv::AMDGPU_Gfx))
+      continue;
+
     ShaderStage stage = getShaderStage(&func);
     ShaderInputsUsage *inputsUsage = getShaderInputsUsage(stage);
     for (unsigned kind = 0; kind != static_cast<unsigned>(ShaderInput::Count); ++kind) {
       ShaderInputUsage *inputUsage = inputsUsage->inputs[kind].get();
       if (inputUsage && inputUsage->entryArgIdx != 0) {
         Argument *arg = getFunctionArgument(&func, inputUsage->entryArgIdx);
         arg->setName(getInputName(static_cast<ShaderInput>(kind)));
         for (Instruction *&call : inputUsage->users) {
           if (call && call->getFunction() == &func) {
             call->replaceAllUsesWith(arg);
diff --git a/lgc/patch/VertexFetch.cpp b/lgc/patch/VertexFetch.cpp
index bd7e217e0..05ee3fd6f 100644
--- a/lgc/patch/VertexFetch.cpp
+++ b/lgc/patch/VertexFetch.cpp
@@ -22,45 +22,51 @@
  *  SOFTWARE.
  *
  **********************************************************************************************************************/
 /**
  ***********************************************************************************************************************
  * @file  VertexFetch.cpp
  * @brief LGC source file: Vertex fetch manager, and pass that uses it
  ***********************************************************************************************************************
  */
 #include "lgc/patch/VertexFetch.h"
+#include "lgc/Builder.h"
 #include "lgc/LgcContext.h"
 #include "lgc/patch/Patch.h"
 #include "lgc/patch/ShaderInputs.h"
 #include "lgc/state/IntrinsDefs.h"
 #include "lgc/state/PalMetadata.h"
 #include "lgc/state/PipelineState.h"
 #include "lgc/state/TargetInfo.h"
 #include "lgc/util/BuilderBase.h"
 #include "lgc/util/Internal.h"
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/Instructions.h"
+#include "llvm/IR/IntrinsicsAMDGPU.h"
 #include "llvm/IR/Module.h"
 
 #define DEBUG_TYPE "lgc-vertex-fetch"
 
 using namespace lgc;
 using namespace llvm;
 
 namespace lgc {
 class BuilderBase;
 class PipelineState;
 } // namespace lgc
 
 namespace {
 
+// Map vkgc
+static constexpr unsigned InternalDescriptorSetId = static_cast<unsigned>(-1);
+static constexpr unsigned FetchShaderInternalBufferBinding = 5;
+
 // Represents vertex format info corresponding to vertex attribute format (VkFormat).
 struct VertexFormatInfo {
   BufNumFormat nfmt;    // Numeric format of vertex buffer
   BufDataFormat dfmt;   // Data format of vertex buffer
   unsigned numChannels; // Valid number of channels
 };
 
 // Represents vertex component info corresponding to vertex data format (BufDataFormat).
 //
 // NOTE: This info is used by vertex fetch instructions. We split vertex fetch into its per-component fetches when
@@ -78,20 +84,23 @@ struct VertexCompFormatInfo {
 class VertexFetchImpl : public VertexFetch {
 public:
   VertexFetchImpl(LgcContext *lgcContext);
   VertexFetchImpl(const VertexFetchImpl &) = delete;
   VertexFetchImpl &operator=(const VertexFetchImpl &) = delete;
 
   // Generate code to fetch a vertex value
   Value *fetchVertex(Type *inputTy, const VertexInputDescription *description, unsigned location, unsigned compIdx,
                      BuilderBase &builder) override;
 
+  // Generate code to fetch a vertex value for uber shader
+  Value *fetchVertex(llvm::CallInst *callInst, Value *descPtr, BuilderBase &builder);
+
 private:
   void initialize(PipelineState *pipelineState);
 
   static VertexFormatInfo getVertexFormatInfo(const VertexInputDescription *description);
 
   // Gets variable corresponding to vertex index
   Value *getVertexIndex() { return m_vertexIndex; }
 
   // Gets variable corresponding to instance index
   Value *getInstanceIndex() { return m_instanceIndex; }
@@ -104,25 +113,29 @@ private:
 
   void addVertexFetchInst(Value *vbDesc, unsigned numChannels, bool is16bitFetch, Value *vbIndex, unsigned offset,
                           unsigned stride, unsigned dfmt, unsigned nfmt, Instruction *insertPos, Value **ppFetch) const;
 
   bool needPostShuffle(const VertexInputDescription *inputDesc, std::vector<Constant *> &shuffleMask) const;
 
   bool needPatchA2S(const VertexInputDescription *inputDesc) const;
 
   bool needSecondVertexFetch(const VertexInputDescription *inputDesc) const;
 
+  Function *generateFetchFunction(bool is64bitFetch, Module *module);
+
   LgcContext *m_lgcContext = nullptr;   // LGC context
   LLVMContext *m_context = nullptr;     // LLVM context
   Value *m_vertexBufTablePtr = nullptr; // Vertex buffer table pointer
   Value *m_vertexIndex = nullptr;       // Vertex index
   Value *m_instanceIndex = nullptr;     // Instance index
+  Function *m_fetchVertex64 = nullptr;  // 64-bit fetch vertex function
+  Function *m_fetchVertex32 = nullptr;  // 32-bit fetch vertex function
 
   static const VertexCompFormatInfo m_vertexCompFormatInfo[]; // Info table of vertex component format
   static const unsigned char m_vertexFormatMapGfx10[][8];     // Info table of vertex format mapping for GFX10
 
   // Default values for vertex fetch (<4 x i32> or <8 x i32>)
   struct {
     Constant *int8;     // < 0, 0, 0, 1 >
     Constant *int16;    // < 0, 0, 0, 1 >
     Constant *int32;    // < 0, 0, 0, 1 >
     Constant *int64;    // < 0, 0, 0, 0, 0, 0, 0, 1 >
@@ -383,20 +396,45 @@ bool LowerVertexFetch::runImpl(Module &module, PipelineState *pipelineState) {
   BuilderBase builder(module.getContext());
   for (auto &func : module) {
     if (!func.isDeclaration() || !func.getName().startswith(lgcName::InputImportVertex))
       continue;
     for (auto user : func.users())
       vertexFetches.push_back(cast<CallInst>(user));
   }
   if (vertexFetches.empty())
     return false;
 
+  if (pipelineState->getOptions().enableUberFetchShader) {
+    // Some formats are not supported before gfx9.
+    assert(pipelineState->getLgcContext()->getTargetInfo().getGfxIpVersion().major > 9);
+
+    std::unique_ptr<lgc::Builder> desBuilder(Builder::createBuilderImpl(pipelineState->getLgcContext(), pipelineState));
+    desBuilder->setShaderStage(ShaderStageVertex);
+    desBuilder->SetInsertPoint(&(*vertexFetches[0]->getFunction()->front().getFirstInsertionPt()));
+    auto desc =
+        desBuilder->CreateLoadBufferDesc(InternalDescriptorSetId, FetchShaderInternalBufferBinding,
+                                         desBuilder->getInt32(0), Builder::BufferFlagAddress, desBuilder->getInt8Ty());
+
+    // The size of each input descriptor is sizeof(UberFetchShaderAttribInfo). vector4
+    auto uberFetchAttrType = FixedVectorType::get(builder.getInt32Ty(), 4);
+    auto descPtr = desBuilder->CreateIntToPtr(desc, PointerType::get(uberFetchAttrType, ADDR_SPACE_CONST));
+
+    for (CallInst *call : vertexFetches) {
+      builder.SetInsertPoint(call);
+      Value *vertex = vertexFetch->fetchVertex(call, descPtr, builder);
+      // Replace and erase this call.
+      call->replaceAllUsesWith(vertex);
+      call->eraseFromParent();
+    }
+    return true;
+  }
+
   if (!pipelineState->isUnlinked() || !pipelineState->getVertexInputDescriptions().empty()) {
     // Whole-pipeline compilation (or shader compilation where we were given the vertex input descriptions).
     // Lower each vertex fetch.
     for (CallInst *call : vertexFetches) {
       Value *vertex = nullptr;
 
       // Find the vertex input description.
       unsigned location = cast<ConstantInt>(call->getArgOperand(0))->getZExtValue();
       unsigned component = cast<ConstantInt>(call->getArgOperand(1))->getZExtValue();
       const VertexInputDescription *description = pipelineState->findVertexInputDescription(location);
@@ -468,20 +506,474 @@ bool LowerVertexFetch::runImpl(Module &module, PipelineState *pipelineState) {
       }
     }
     vertex->setName("vertex" + Twine(info[idx].location) + "." + Twine(info[idx].component));
     call->replaceAllUsesWith(vertex);
     call->eraseFromParent();
   }
 
   return true;
 }
 
+// =====================================================================================================================
+// Generate vertex fetch function.
+//
+// @param is64bitFetch : Whether is 64 bit fetch
+// @param [in/out] module : Module
+// @returns : Generated function
+Function *VertexFetchImpl::generateFetchFunction(bool is64bitFetch, Module *module) {
+  BuilderBase builder(*m_context);
+  // Helper to create basic block
+  auto createBlock = [&](Twine blockName, Function *parent) {
+    return BasicBlock::Create(*m_context, blockName, parent);
+  };
+
+  auto fetch64Type = FixedVectorType::get(Type::getInt32Ty(*m_context), 2);
+  auto fetchType = FixedVectorType::get(Type::getInt32Ty(*m_context), 4);
+
+  auto createFunction = [&] {
+    // Function args
+    Type *argTypes[] = {
+        FixedVectorType::get(builder.getInt32Ty(), 4), // vbdesc
+        builder.getInt32Ty(),                          // vbIndex
+        builder.getInt32Ty(),                          // vertexOffset
+        builder.getInt32Ty(),                          // component size in byte
+        builder.getInt1Ty(),                           // isPacked
+        builder.getInt1Ty(),                           // isBgr
+        builder.getInt1Ty(),                           // Y component mask
+        builder.getInt1Ty(),                           // Z component mask
+        builder.getInt1Ty(),                           // W component mask
+    };
+    // Return type
+    Type *retTy = FixedVectorType::get(builder.getInt32Ty(), is64bitFetch ? 8 : 4);
+
+    StringRef funcName = is64bitFetch ? "FetchVertex64" : "FetchVertex32";
+    FunctionType *const funcTy = FunctionType::get(retTy, argTypes, false);
+    Function *func = Function::Create(funcTy, GlobalValue::InternalLinkage, funcName, module);
+    func->setCallingConv(CallingConv::C);
+    func->addFnAttr(Attribute::AlwaysInline);
+
+    // Name args
+    auto argIt = func->arg_begin();
+    Value *vbDesc = argIt++;
+    vbDesc->setName("vbDesc");
+
+    Value *vbIndex = argIt++;
+    vbIndex->setName("vbIndex");
+
+    Value *vertexOffset = argIt++;
+    vertexOffset->setName("vertexOffset");
+
+    Value *compByteSize = argIt++;
+    compByteSize->setName("compByteSize");
+
+    Value *isPacked = argIt++;
+    isPacked->setName("isPacked");
+
+    Value *isBgr = argIt++;
+    isBgr->setName("isBgr");
+
+    Value *yMask = argIt++;
+    yMask->setName("yMask");
+    Value *zMask = argIt++;
+    zMask->setName("zMask");
+    Value *wMask = argIt++;
+    wMask->setName("wMask");
+
+    auto entry = createBlock(".entry", func);
+    auto wholeVertex = createBlock(".wholeVertex", func);
+    auto comp0Block = createBlock(".comp0Block", func);
+    auto comp1Block = createBlock(".comp1Block", func);
+    auto comp2Block = createBlock(".comp2Block", func);
+    auto comp3Block = createBlock(".comp3Block", func);
+    auto endfun = createBlock(".endfun", func);
+
+    Value *args[] = {
+        vbDesc,              // rsrc
+        vbIndex,             // vindex
+        vertexOffset,        // offset
+        builder.getInt32(0), // soffset
+        builder.getInt32(0)  // glc, slc
+    };
+
+    // .entry
+    {
+      builder.SetInsertPoint(entry);
+      // If ispacked is false, we require per-component fetch
+      builder.CreateCondBr(isPacked, wholeVertex, comp0Block);
+    }
+
+    // .wholeVertex
+    {
+      builder.SetInsertPoint(wholeVertex);
+      Value *vertex = builder.CreateIntrinsic(Intrinsic::amdgcn_struct_buffer_load_format, fetchType, args, {});
+      if (is64bitFetch) {
+        // If it is 64-bit, we need the second fetch
+        args[2] = builder.CreateAdd(args[2], builder.getInt32(SizeOfVec4));
+        auto secondFetch = builder.CreateIntrinsic(Intrinsic::amdgcn_struct_buffer_load_format, fetchType, args, {});
+        std::vector<Constant *> shuffleMask;
+        for (unsigned i = 0; i < 8; ++i)
+          shuffleMask.push_back(ConstantInt::get(Type::getInt32Ty(*m_context), i));
+        vertex = builder.CreateShuffleVector(vertex, secondFetch, ConstantVector::get(shuffleMask));
+      }
+      builder.CreateRet(vertex);
+    }
+
+    // return value
+    Value *lastVert = UndefValue::get(retTy);
+    Value *comp0 = nullptr;
+    Value *comp1 = nullptr;
+    Value *comp2 = nullptr;
+    Value *comp3 = nullptr;
+    // Per-component fetch
+    // reset
+    args[2] = vertexOffset;
+
+    // X channel
+    // .comp0Block
+    {
+      builder.SetInsertPoint(comp0Block);
+      if (is64bitFetch) {
+        Value *comp = builder.CreateIntrinsic(Intrinsic::amdgcn_struct_buffer_load_format, fetch64Type, args, {});
+        Value *elem = builder.CreateExtractElement(comp, uint64_t(0));
+        lastVert = builder.CreateInsertElement(lastVert, elem, uint64_t(0));
+        elem = builder.CreateExtractElement(comp, 1);
+        lastVert = builder.CreateInsertElement(lastVert, elem, 1);
+        comp0 = lastVert;
+      } else {
+        comp0 = builder.CreateIntrinsic(Intrinsic::amdgcn_struct_buffer_load_format, builder.getInt32Ty(), args, {});
+        lastVert = builder.CreateInsertElement(lastVert, comp0, uint64_t(0));
+        comp0 = lastVert;
+      }
+      // If Y channel is 0, we will fetch the second component.
+      builder.CreateCondBr(yMask, comp1Block, endfun);
+    }
+
+    // Y channel
+    // .comp1Block
+    {
+      builder.SetInsertPoint(comp1Block);
+      // Add offset. offset = offset + componentSize
+      args[2] = builder.CreateAdd(args[2], compByteSize);
+      if (is64bitFetch) {
+        Value *comp = builder.CreateIntrinsic(Intrinsic::amdgcn_struct_buffer_load_format, fetch64Type, args, {});
+        Value *elem = builder.CreateExtractElement(comp, uint64_t(0));
+        lastVert = builder.CreateInsertElement(lastVert, elem, 2);
+        elem = builder.CreateExtractElement(comp, 1);
+        lastVert = builder.CreateInsertElement(lastVert, elem, 3);
+        comp1 = lastVert;
+      } else {
+        comp1 = builder.CreateIntrinsic(Intrinsic::amdgcn_struct_buffer_load_format, Type::getInt32Ty(*m_context), args,
+                                        {});
+        lastVert = builder.CreateInsertElement(lastVert, comp1, 1);
+        comp1 = lastVert;
+      }
+      builder.CreateCondBr(zMask, comp2Block, endfun);
+    }
+
+    // Z channel
+    // .comp2Block
+    {
+      builder.SetInsertPoint(comp2Block);
+      args[2] = builder.CreateAdd(args[2], compByteSize);
+      if (is64bitFetch) {
+        Value *comp = builder.CreateIntrinsic(Intrinsic::amdgcn_struct_buffer_load_format, fetch64Type, args, {});
+        Value *elem = builder.CreateExtractElement(comp, uint64_t(0));
+        lastVert = builder.CreateInsertElement(lastVert, elem, 4);
+        elem = builder.CreateExtractElement(comp, 1);
+        lastVert = builder.CreateInsertElement(lastVert, elem, 5);
+        comp2 = lastVert;
+      } else {
+        comp2 = builder.CreateIntrinsic(Intrinsic::amdgcn_struct_buffer_load_format, Type::getInt32Ty(*m_context), args,
+                                        {});
+        lastVert = builder.CreateInsertElement(lastVert, comp2, 2);
+        comp2 = lastVert;
+      }
+      builder.CreateCondBr(wMask, comp3Block, endfun);
+    }
+
+    // W channel
+    // .comp3Block
+    {
+      builder.SetInsertPoint(comp3Block);
+      args[2] = builder.CreateAdd(args[2], compByteSize);
+      if (is64bitFetch) {
+        Value *comp = builder.CreateIntrinsic(Intrinsic::amdgcn_struct_buffer_load_format, fetch64Type, args, {});
+        Value *elem = builder.CreateExtractElement(comp, uint64_t(0));
+        lastVert = builder.CreateInsertElement(lastVert, elem, 6);
+        elem = builder.CreateExtractElement(comp, 1);
+        lastVert = builder.CreateInsertElement(lastVert, elem, 7);
+        comp3 = lastVert;
+      } else {
+        comp3 = builder.CreateIntrinsic(Intrinsic::amdgcn_struct_buffer_load_format, Type::getInt32Ty(*m_context), args,
+                                        {});
+        lastVert = builder.CreateInsertElement(lastVert, comp3, 3);
+        comp3 = lastVert;
+      }
+      builder.CreateBr(endfun);
+    }
+
+    // .endfun
+    {
+      builder.SetInsertPoint(endfun);
+      auto phiInst = builder.CreatePHI(lastVert->getType(), 4);
+      phiInst->addIncoming(comp0, comp0Block);
+      phiInst->addIncoming(comp1, comp1Block);
+      phiInst->addIncoming(comp2, comp2Block);
+      phiInst->addIncoming(comp3, comp3Block);
+      Value *vertex = phiInst;
+      // If the format is bgr, fix the order. It only is included in 32-bit format.
+      if (!is64bitFetch) {
+        std::vector<Constant *> shuffleMask;
+        shuffleMask.push_back(builder.getInt32(2));
+        shuffleMask.push_back(builder.getInt32(1));
+        shuffleMask.push_back(builder.getInt32(0));
+        shuffleMask.push_back(builder.getInt32(3));
+        auto fixedVertex = builder.CreateShuffleVector(vertex, vertex, ConstantVector::get(shuffleMask));
+        vertex = builder.CreateSelect(isBgr, fixedVertex, vertex);
+      }
+      builder.CreateRet(vertex);
+    }
+    return func;
+  };
+
+  if (is64bitFetch) {
+    if (!m_fetchVertex64)
+      m_fetchVertex64 = createFunction();
+    return m_fetchVertex64;
+  }
+
+  if (!m_fetchVertex32)
+    m_fetchVertex32 = createFunction();
+  return m_fetchVertex32;
+}
+
+// =====================================================================================================================
+// This is an lgc.input.import.vertex operation.
+// Executes vertex fetch operations based on the uber shader buffer
+//
+// @param callInst : call instruction
+// @param descPtr : 64bit address of buffer
+// @param builder : Builder to use to insert vertex fetch instructions
+// @returns : vertex
+Value *VertexFetchImpl::fetchVertex(CallInst *callInst, llvm::Value *descPtr, BuilderBase &builder) {
+  unsigned location = cast<ConstantInt>(callInst->getArgOperand(0))->getZExtValue();
+  unsigned compIdx = cast<ConstantInt>(callInst->getArgOperand(1))->getZExtValue();
+  auto zero = builder.getInt32(0);
+
+  if (!m_vertexIndex) {
+    auto savedInsertPoint = builder.saveIP();
+    builder.SetInsertPoint(&*callInst->getFunction()->front().getFirstInsertionPt());
+    m_vertexIndex = ShaderInputs::getVertexIndex(builder, *m_lgcContext);
+    builder.restoreIP(savedInsertPoint);
+  }
+
+  if (!m_instanceIndex) {
+    auto savedInsertPoint = builder.saveIP();
+    builder.SetInsertPoint(&*callInst->getFunction()->front().getFirstInsertionPt());
+    m_instanceIndex = ShaderInputs::getInstanceIndex(builder, *m_lgcContext);
+    builder.restoreIP(savedInsertPoint);
+  }
+
+  // Get the vertex buffer table pointer as pointer to v4i32 descriptor.
+  Type *vbDescTy = FixedVectorType::get(Type::getInt32Ty(*m_context), 4);
+  if (!m_vertexBufTablePtr) {
+    auto savedInsertPoint = builder.saveIP();
+    builder.SetInsertPoint(&*callInst->getFunction()->front().getFirstInsertionPt());
+    m_vertexBufTablePtr =
+        ShaderInputs::getSpecialUserDataAsPointer(UserDataMapping::VertexBufferTable, vbDescTy, builder);
+    builder.restoreIP(savedInsertPoint);
+  }
+
+  // The size of each input descriptor is sizeof(UberFetchShaderAttribInfo). vector4
+  auto uberFetchAttrType = FixedVectorType::get(Type::getInt32Ty(*m_context), 4);
+  descPtr = builder.CreateGEP(uberFetchAttrType, descPtr, {builder.getInt32(location)});
+  auto uberFetchAttr = builder.CreateLoad(vbDescTy, descPtr);
+
+  // The first DWord
+  auto attr = builder.CreateExtractElement(uberFetchAttr, uint64_t(0));
+
+  // The second DWord
+  auto byteOffset = builder.CreateExtractElement(uberFetchAttr, 1);
+
+  // The third DWord
+  auto inputRate = builder.CreateExtractElement(uberFetchAttr, 2);
+
+  // The fourth DWord
+  auto bufferFormat = builder.CreateExtractElement(uberFetchAttr, 3);
+
+  // attr[0~7]
+  auto descBinding = builder.CreateAnd(attr, builder.getInt32(0xFF));
+
+  // attr[8]
+  auto perInstance = builder.CreateAnd(attr, builder.getInt32(0x100));
+
+  // attr[10]
+  auto isPacked = builder.CreateAnd(attr, builder.getInt32(0x400));
+  isPacked = builder.CreateICmpNE(isPacked, zero);
+
+  // attr[12~15]
+  auto componentSize = builder.CreateIntrinsic(Intrinsic::amdgcn_ubfe, builder.getInt32Ty(),
+                                               {attr, builder.getInt32(12), builder.getInt32(4)});
+
+  auto xMask = builder.CreateAnd(attr, builder.getInt32(0x10000u));
+  auto yMask = builder.CreateAnd(attr, builder.getInt32(0x20000u));
+  auto zMask = builder.CreateAnd(attr, builder.getInt32(0x40000u));
+  auto wMask = builder.CreateAnd(attr, builder.getInt32(0x80000u));
+  xMask = builder.CreateICmpNE(xMask, zero);
+  yMask = builder.CreateICmpNE(yMask, zero);
+  zMask = builder.CreateICmpNE(zMask, zero);
+  wMask = builder.CreateICmpNE(wMask, zero);
+
+  // attr[20]
+  auto isBgr = builder.CreateAnd(attr, builder.getInt32(0x0100000));
+  isBgr = builder.CreateICmpNE(isBgr, zero);
+
+  // Load VbDesc
+  Value *vbDescPtr = builder.CreateGEP(vbDescTy, m_vertexBufTablePtr, descBinding);
+  LoadInst *loadInst = builder.CreateLoad(vbDescTy, vbDescPtr);
+  loadInst->setMetadata(LLVMContext::MD_invariant_load, MDNode::get(loadInst->getContext(), {}));
+  loadInst->setAlignment(Align(16));
+  Value *vbDesc = loadInst;
+  // Replace buffer format
+  vbDesc = builder.CreateInsertElement(vbDesc, bufferFormat, 3);
+
+  auto isPerInstance = builder.CreateICmpNE(perInstance, zero);
+
+  // PerInstance
+  auto vbIndexInstance = ShaderInputs::getInput(ShaderInput::InstanceId, builder, *m_lgcContext);
+  vbIndexInstance = builder.CreateUDiv(vbIndexInstance, inputRate);
+  vbIndexInstance =
+      builder.CreateAdd(vbIndexInstance, ShaderInputs::getSpecialUserData(UserDataMapping::BaseInstance, builder));
+
+  // Select VbIndex
+  Value *vbIndex = builder.CreateSelect(isPerInstance, vbIndexInstance, m_vertexIndex);
+
+  auto inputType = callInst->getType();
+  auto inputTy = callInst->getType();
+  Type *basicTy = inputTy->isVectorTy() ? cast<VectorType>(inputTy)->getElementType() : inputTy;
+  const unsigned bitWidth = basicTy->getScalarSizeInBits();
+  assert(bitWidth == 8 || bitWidth == 16 || bitWidth == 32 || bitWidth == 64);
+
+  bool is64Fetch = bitWidth == 64;
+  auto func = generateFetchFunction(is64Fetch, callInst->getFunction()->getParent());
+  Value *lastVert =
+      builder.CreateCall(func, {vbDesc, vbIndex, byteOffset, componentSize, isPacked, isBgr, yMask, zMask, wMask});
+
+  // Get default fetch values
+  Constant *defaults = nullptr;
+
+  if (basicTy->isIntegerTy()) {
+    if (bitWidth == 8)
+      defaults = m_fetchDefaults.int8;
+    else if (bitWidth == 16)
+      defaults = m_fetchDefaults.int16;
+    else if (bitWidth == 32)
+      defaults = m_fetchDefaults.int32;
+    else {
+      assert(bitWidth == 64);
+      defaults = m_fetchDefaults.int64;
+    }
+  } else if (basicTy->isFloatingPointTy()) {
+    if (bitWidth == 16)
+      defaults = m_fetchDefaults.float16;
+    else if (bitWidth == 32)
+      defaults = m_fetchDefaults.float32;
+    else {
+      assert(bitWidth == 64);
+      defaults = m_fetchDefaults.double64;
+    }
+  } else
+    llvm_unreachable("Should never be called!");
+
+  const unsigned defaultCompCount = cast<FixedVectorType>(defaults->getType())->getNumElements();
+  std::vector<Value *> defaultValues(defaultCompCount);
+
+  for (unsigned i = 0; i < defaultValues.size(); ++i) {
+    defaultValues[i] = builder.CreateExtractElement(defaults, ConstantInt::get(Type::getInt32Ty(*m_context), i));
+  }
+
+  // Get vertex fetch values
+  const unsigned fetchCompCount =
+      lastVert->getType()->isVectorTy() ? cast<FixedVectorType>(lastVert->getType())->getNumElements() : 1;
+  std::vector<Value *> fetchValues(fetchCompCount);
+
+  if (fetchCompCount == 1)
+    fetchValues[0] = lastVert;
+  else {
+    for (unsigned i = 0; i < fetchCompCount; ++i) {
+      fetchValues[i] = builder.CreateExtractElement(lastVert, ConstantInt::get(Type::getInt32Ty(*m_context), i));
+    }
+  }
+
+  // Construct vertex fetch results
+  const unsigned inputCompCount = inputTy->isVectorTy() ? cast<FixedVectorType>(inputTy)->getNumElements() : 1;
+  const unsigned vertexCompCount = inputCompCount * (bitWidth == 64 ? 2 : 1);
+
+  std::vector<Value *> vertexValues(vertexCompCount);
+
+  // NOTE: Original component index is based on the basic scalar type.
+  compIdx *= (bitWidth == 64 ? 2 : 1);
+
+  // Vertex input might take values from vertex fetch values or default fetch values
+  for (unsigned i = 0; i < vertexCompCount; i++) {
+    if (compIdx + i < fetchCompCount)
+      vertexValues[i] = fetchValues[compIdx + i];
+    else if (compIdx + i < defaultCompCount)
+      vertexValues[i] = defaultValues[compIdx + i];
+    else {
+      llvm_unreachable("Should never be called!");
+      vertexValues[i] = UndefValue::get(Type::getInt32Ty(*m_context));
+    }
+  }
+  Value *vertex = nullptr;
+
+  if (vertexCompCount == 1)
+    vertex = vertexValues[0];
+  else {
+    Type *vertexTy = FixedVectorType::get(Type::getInt32Ty(*m_context), vertexCompCount);
+    vertex = UndefValue::get(vertexTy);
+
+    for (unsigned i = 0; i < vertexCompCount; ++i)
+      vertex = builder.CreateInsertElement(vertex, vertexValues[i], ConstantInt::get(Type::getInt32Ty(*m_context), i));
+  }
+
+  const bool is8bitFetch = (inputType->getScalarSizeInBits() == 8);
+  const bool is16bitFetch = (inputType->getScalarSizeInBits() == 16);
+
+  if (is8bitFetch) {
+    // NOTE: The vertex fetch results are represented as <n x i32> now. For 8-bit vertex fetch, we have to
+    // convert them to <n x i8> and the 24 high bits is truncated.
+    assert(inputTy->isIntOrIntVectorTy()); // Must be integer type
+
+    Type *vertexTy = vertex->getType();
+    Type *truncTy = Type::getInt8Ty(*m_context);
+    truncTy = vertexTy->isVectorTy()
+                  ? cast<Type>(FixedVectorType::get(truncTy, cast<FixedVectorType>(vertexTy)->getNumElements()))
+                  : truncTy;
+    vertex = builder.CreateTrunc(vertex, truncTy);
+  } else if (is16bitFetch) {
+    // NOTE: The vertex fetch results are represented as <n x i32> now. For 16-bit vertex fetch, we have to
+    // convert them to <n x i16> and the 16 high bits is truncated.
+    Type *vertexTy = vertex->getType();
+    Type *truncTy = Type::getInt16Ty(*m_context);
+    truncTy = vertexTy->isVectorTy()
+                  ? cast<Type>(FixedVectorType::get(truncTy, cast<FixedVectorType>(vertexTy)->getNumElements()))
+                  : truncTy;
+    vertex = builder.CreateTrunc(vertex, truncTy);
+  }
+
+  if (vertex->getType() != inputTy)
+    vertex = builder.CreateBitCast(vertex, inputTy);
+  vertex->setName("vertex");
+  return vertex;
+}
+
 // =====================================================================================================================
 // Create a VertexFetch
 VertexFetch *VertexFetch::create(LgcContext *lgcContext) {
   return new VertexFetchImpl(lgcContext);
 }
 
 // =====================================================================================================================
 // Constructor
 //
 // @param context : LLVM context
diff --git a/lgc/test/PackUnlinkVsGs.lgc b/lgc/test/PackUnlinkVsGs.lgc
index 2b3ed9b9a..f2967fcea 100644
--- a/lgc/test/PackUnlinkVsGs.lgc
+++ b/lgc/test/PackUnlinkVsGs.lgc
@@ -1,18 +1,19 @@
 ; Test that the output from the VS are removed when they are not used in the GS even for unlinked shaders.
 
 ; RUN: lgc -mcpu=gfx900 -o - - <%s | FileCheck --check-prefixes=CHECK %s
 
 ; In this simple case, the vertex shader will be the first basic block in _amdgpu_gs_main.  So we find that block, and
 ; make sure the only ds_write instructions are for the VS output that are used by the GS, and that there are no others.
 ;
 ; CHECK-LABEL: _amdgpu_gs_main:
+; CHECK: s_buffer_load_dwordx4
 ; CHECK-NOT: {{^[L\.]*}}BB
 ; CHECK-NOT: ds_write2_b32
 ; CHECK: ds_write2_b32 v{{[0-9]*}}, v{{[0-9]*}}, v{{[0-9]*}} offset1:1
 ; CHECK-NOT: ds_write2_b32
 ; CHECK: ds_write2_b32 v{{[0-9]*}}, v{{[0-9]*}}, v{{[0-9]*}} offset0:2 offset1:3
 ; CHECK-NOT: ds_write2_b32
 ; CHECK: {{^[L\.]*}}BB
 
 ; ModuleID = 'lgcPipeline'
 source_filename = "lgcPipeline"
diff --git a/lgc/test/UberFetchShader.lgc b/lgc/test/UberFetchShader.lgc
new file mode 100644
index 000000000..0bd1dca60
--- /dev/null
+++ b/lgc/test/UberFetchShader.lgc
@@ -0,0 +1,101 @@
+; RUN: lgc -mcpu=gfx1030 -o - -passes=lgc-vertex-fetch -enable-opaque-pointers %s | FileCheck --check-prefixes=CHECK %s
+
+; CHECK-LABEL: define dllexport spir_func void @lgc.shader.VS.main()
+; Get the descriptor of Uber Fetch Shader buffer
+; CHECK: [[Desc:%[0-9]*]] = call <2 x i32> @lgc.root.descriptor.v2i32(i32 1)
+
+; CHECK: [[INT64DESC:%[0-9]*]] = bitcast <2 x i32> [[Desc]] to i64
+; CHECK: [[DESCPTR:%[0-9]*]] = inttoptr i64 [[INT64DESC]] to ptr addrspace(4)
+; CHECK: [[UBERINFOPTR:%[0-9]*]] = getelementptr <4 x i32>, ptr addrspace(4) [[DESCPTR]], i32 0
+; CHECK: [[UBERINFO:%[0-9]*]] = load <4 x i32>, ptr addrspace(4) [[UBERINFOPTR]], align 16
+
+; Read the first dword: vertex attribute
+; CHECK: [[attr:%[0-9]*]] = extractelement <4 x i32> [[UBERINFO]], i64 0
+
+; Read the second dword: Attribute offset
+; CHECK: [[offset:%[0-9]*]] = extractelement <4 x i32> [[UBERINFO]], i64 1
+
+; Read the third dword: Reciprocal of instance divisor
+; CHECK: [[divisor:%[0-9]*]] = extractelement <4 x i32> [[UBERINFO]], i64 2
+
+; Read the fourth dword: Buffer format
+; CHECK:[[format:%[0-9]*]] = extractelement <4 x i32> [[UBERINFO]], i64 3
+
+; parse vertex attribute
+; Attribute binding in vertex buffer table (attr & 0xFF)
+; CHECK: and i32 [[attr]], 255
+; Whether vertex input rate is per-instance (attr & 0x100)
+; CHECK: and i32 [[attr]], 256
+; Whether it is a packed format (attr & 0x400)
+; CHECK: and i32 [[attr]], 1024
+; Byte size per component (attr[12~15])
+; CHECK: call i32 @llvm.amdgcn.ubfe.i32(i32 [[attr]], i32 12, i32 4)
+; x channel mask (attr & 0x100000)
+; CHECK: and i32 [[attr]], 65536
+; y channel mask (attr & 0x200000)
+; CHECK: and i32 [[attr]], 131072
+; z channel mask (attr & 0x400000)
+; CHECK: and i32 [[attr]], 262144
+; w channel mask (attr & 0x800000)
+; CHECK: and i32 [[attr]], 524288
+; Whether is BGRA format (attr & 0x100000)
+; CHECK: and i32 [[attr]], 1048576
+
+; fetch vertex function
+; CHECK-LABEL: define internal <4 x i32> @FetchVertex32
+; Load the whole vertex
+; CHECK: call <4 x i32> @llvm.amdgcn.struct.buffer.load.format.v4i32(<4 x i32>
+; Load per channel, 4 channels
+; CHECK: call i32 @llvm.amdgcn.struct.buffer.load.format.i32(<4 x i32>
+; CHECK: call i32 @llvm.amdgcn.struct.buffer.load.format.i32(<4 x i32>
+; CHECK: call i32 @llvm.amdgcn.struct.buffer.load.format.i32(<4 x i32>
+; CHECK: call i32 @llvm.amdgcn.struct.buffer.load.format.i32(<4 x i32>
+
+define dllexport spir_func void @lgc.shader.VS.main() local_unnamed_addr #0 !spirv.ExecutionModel !10 !lgc.shaderstage !11 {
+.entry:
+  %0 = call <4 x float> @lgc.input.import.vertex.v4f32.i32.i32(i32 0, i32 0) #1
+  ret void
+}
+
+; Function Attrs: nounwind readonly willreturn
+declare <4 x float> @lgc.input.import.vertex.v4f32.i32.i32(i32, i32) #1
+
+; Function Attrs: nounwind
+declare void @lgc.output.export.builtin.CullDistance.i32.a1f32(i32, [1 x float]) #0
+
+; Function Attrs: nounwind
+declare void @lgc.output.export.builtin.ClipDistance.i32.a1f32(i32, [1 x float]) #0
+
+; Function Attrs: nounwind
+declare void @lgc.output.export.builtin.PointSize.i32.f32(i32, float) #0
+
+; Function Attrs: nounwind
+declare void @lgc.output.export.builtin.Position.i32.v4f32(i32, <4 x float>) #0
+
+attributes #0 = { nounwind }
+attributes #1 = { nounwind readonly willreturn }
+
+!lgc.client = !{!0}
+!lgc.options = !{!1}
+!lgc.options.VS = !{!2}
+!lgc.options.FS = !{!3}
+!lgc.user.data.nodes = !{!4, !5}
+!lgc.vertex.inputs = !{!6}
+!lgc.color.export.formats = !{!7}
+!lgc.input.assembly.state = !{!8}
+!amdgpu.pal.metadata.msgpack = !{!9}
+
+!0 = !{!"Vulkan"}
+!1 = !{i32 -1078354702, i32 -917677750, i32 -891297186, i32 -500497739, i32 1, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 1, i32 0, i32 0, i32 2, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 256}
+!2 = !{i32 225099809, i32 -29817230, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 64, i32 0, i32 0, i32 3, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 20, i32 1800}
+!3 = !{i32 2068278405, i32 41923448, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 64, i32 0, i32 0, i32 3, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 20, i32 1800}
+!4 = !{!"IndirectUserDataVaPtr", i32 8, i32 0, i32 1, i32 4}
+!5 = !{!"DescriptorBufferCompact", i32 10, i32 1, i32 2, i32 -1, i32 5, i32 2}
+!6 = !{i32 0, i32 0, i32 0, i32 12, i32 13, i32 7, i32 -1}
+!7 = !{i32 10}
+!8 = !{i32 4, i32 3}
+!9 = !{!"\82\B0amdpal.pipelines\91\84\AA.registers\80\B0.spill_threshold\CE\FF\FF\FF\FF\B0.user_data_limit\00\AF.xgl_cache_info\82\B3.128_bit_cache_hash\92\CF\FE\\\EC.\94m\CC0\CFT<\CD\D2rp1\F6\AD.llpc_version\A457.0\AEamdpal.version\92\02\03"}
+!10 = !{i32 0}
+!11 = !{i32 1}
+!12 = !{i32 4}
+!13 = !{i32 6}
diff --git a/llpc/context/llpcCompiler.cpp b/llpc/context/llpcCompiler.cpp
index 8c2e356ba..cb012767f 100644
--- a/llpc/context/llpcCompiler.cpp
+++ b/llpc/context/llpcCompiler.cpp
@@ -925,37 +925,31 @@ Result Compiler::buildPipelineWithRelocatableElf(Context *context, ArrayRef<cons
 // =====================================================================================================================
 // Returns true if node is of a descriptor type that is unsupported by relocatable shader compilation.
 //
 // @param [in] node : User data node
 static bool isUnrelocatableResourceMappingRootNode(const ResourceMappingNode *node) {
   switch (node->type) {
   case ResourceMappingNodeType::DescriptorTableVaPtr: {
     const ResourceMappingNode *startInnerNode = node->tablePtr.pNext;
     const ResourceMappingNode *endInnerNode = startInnerNode + node->tablePtr.nodeCount;
     for (const ResourceMappingNode *innerNode = startInnerNode; innerNode != endInnerNode; ++innerNode) {
-      if (innerNode->type == ResourceMappingNodeType::DescriptorBufferCompact)
-        // The code to handle a compact descriptor cannot be easily patched, so relocatable shaders assume there are
-        // no compact descriptors.
-        return true;
       if (innerNode->type == ResourceMappingNodeType::InlineBuffer) {
         // The code to handle an inline buffer cannot be easily patched, so relocatable shaders
         // assume there are no inline buffers.
         return true;
       }
     }
     break;
   }
-  case ResourceMappingNodeType::DescriptorResource:
   case ResourceMappingNodeType::DescriptorSampler:
   case ResourceMappingNodeType::DescriptorCombinedTexture:
   case ResourceMappingNodeType::DescriptorTexelBuffer:
-  case ResourceMappingNodeType::DescriptorBufferCompact:
     // Generic descriptors in the top level are not handled by the linker.
     return true;
   case ResourceMappingNodeType::InlineBuffer:
     // Loading from an inline buffer requires building a descriptor that is not handled by the linker.
     return true;
   default:
     break;
   }
   return false;
 }
diff --git a/llpc/context/llpcPipelineContext.cpp b/llpc/context/llpcPipelineContext.cpp
index e9147a422..9a2a7a731 100644
--- a/llpc/context/llpcPipelineContext.cpp
+++ b/llpc/context/llpcPipelineContext.cpp
@@ -332,59 +332,64 @@ void PipelineContext::setOptionsInPipeline(Pipeline *pipeline, Util::MetroHash64
   }
 
   // Shadow descriptor command line options override pipeline options.
   if (EnableShadowDescriptorTable.getNumOccurrences() > 0) {
     if (!EnableShadowDescriptorTable)
       options.shadowDescriptorTable = ShadowDescriptorTableDisable;
     else
       options.shadowDescriptorTable = ShadowDescTablePtrHigh;
   }
 
-  if (isGraphics() && getGfxIpVersion().major >= 10) {
-    // Only set NGG options for a GFX10+ graphics pipeline.
-    auto pipelineInfo = reinterpret_cast<const GraphicsPipelineBuildInfo *>(getPipelineBuildInfo());
-    const auto &nggState = pipelineInfo->nggState;
-    if (!nggState.enableNgg)
-      options.nggFlags |= NggFlagDisable;
-    else {
-      options.nggFlags = (nggState.enableGsUse ? NggFlagEnableGsUse : 0) |
-                         (nggState.forceCullingMode ? NggFlagForceCullingMode : 0) |
-                         (nggState.compactMode == NggCompactDisable ? NggFlagCompactDisable : 0) |
-                         (nggState.enableVertexReuse ? NggFlagEnableVertexReuse : 0) |
-                         (nggState.enableBackfaceCulling ? NggFlagEnableBackfaceCulling : 0) |
-                         (nggState.enableFrustumCulling ? NggFlagEnableFrustumCulling : 0) |
-                         (nggState.enableBoxFilterCulling ? NggFlagEnableBoxFilterCulling : 0) |
-                         (nggState.enableSphereCulling ? NggFlagEnableSphereCulling : 0) |
-                         (nggState.enableSmallPrimFilter ? NggFlagEnableSmallPrimFilter : 0) |
-                         (nggState.enableCullDistanceCulling ? NggFlagEnableCullDistanceCulling : 0);
-      options.nggBackfaceExponent = nggState.backfaceExponent;
-
-      // Use a static cast from Vkgc NggSubgroupSizingType to LGC NggSubgroupSizing, and static assert that
-      // that is valid.
-      static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::Auto) == NggSubgroupSizing::Auto, "Mismatch");
-      static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::MaximumSize) ==
-                        NggSubgroupSizing::MaximumSize,
-                    "Mismatch");
-      static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::HalfSize) == NggSubgroupSizing::HalfSize,
-                    "Mismatch");
-      static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::OptimizeForVerts) ==
-                        NggSubgroupSizing::OptimizeForVerts,
-                    "Mismatch");
-      static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::OptimizeForPrims) ==
-                        NggSubgroupSizing::OptimizeForPrims,
-                    "Mismatch");
-      static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::Explicit) == NggSubgroupSizing::Explicit,
-                    "Mismatch");
-      options.nggSubgroupSizing = static_cast<NggSubgroupSizing>(nggState.subgroupSizing);
-
-      options.nggVertsPerSubgroup = nggState.vertsPerSubgroup;
-      options.nggPrimsPerSubgroup = nggState.primsPerSubgroup;
+  if (isGraphics()) {
+    options.enableUberFetchShader =
+        reinterpret_cast<const GraphicsPipelineBuildInfo *>(getPipelineBuildInfo())->enableUberFetchShader;
+    if (getGfxIpVersion().major >= 10) {
+      // Only set NGG options for a GFX10+ graphics pipeline.
+      auto pipelineInfo = reinterpret_cast<const GraphicsPipelineBuildInfo *>(getPipelineBuildInfo());
+      const auto &nggState = pipelineInfo->nggState;
+      if (!nggState.enableNgg)
+        options.nggFlags |= NggFlagDisable;
+      else {
+        options.nggFlags = (nggState.enableGsUse ? NggFlagEnableGsUse : 0) |
+                           (nggState.forceCullingMode ? NggFlagForceCullingMode : 0) |
+                           (nggState.compactMode == NggCompactDisable ? NggFlagCompactDisable : 0) |
+                           (nggState.enableVertexReuse ? NggFlagEnableVertexReuse : 0) |
+                           (nggState.enableBackfaceCulling ? NggFlagEnableBackfaceCulling : 0) |
+                           (nggState.enableFrustumCulling ? NggFlagEnableFrustumCulling : 0) |
+                           (nggState.enableBoxFilterCulling ? NggFlagEnableBoxFilterCulling : 0) |
+                           (nggState.enableSphereCulling ? NggFlagEnableSphereCulling : 0) |
+                           (nggState.enableSmallPrimFilter ? NggFlagEnableSmallPrimFilter : 0) |
+                           (nggState.enableCullDistanceCulling ? NggFlagEnableCullDistanceCulling : 0);
+        options.nggBackfaceExponent = nggState.backfaceExponent;
+
+        // Use a static cast from Vkgc NggSubgroupSizingType to LGC NggSubgroupSizing, and static assert that
+        // that is valid.
+        static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::Auto) == NggSubgroupSizing::Auto,
+                      "Mismatch");
+        static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::MaximumSize) ==
+                          NggSubgroupSizing::MaximumSize,
+                      "Mismatch");
+        static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::HalfSize) == NggSubgroupSizing::HalfSize,
+                      "Mismatch");
+        static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::OptimizeForVerts) ==
+                          NggSubgroupSizing::OptimizeForVerts,
+                      "Mismatch");
+        static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::OptimizeForPrims) ==
+                          NggSubgroupSizing::OptimizeForPrims,
+                      "Mismatch");
+        static_assert(static_cast<NggSubgroupSizing>(NggSubgroupSizingType::Explicit) == NggSubgroupSizing::Explicit,
+                      "Mismatch");
+        options.nggSubgroupSizing = static_cast<NggSubgroupSizing>(nggState.subgroupSizing);
+
+        options.nggVertsPerSubgroup = nggState.vertsPerSubgroup;
+        options.nggPrimsPerSubgroup = nggState.primsPerSubgroup;
+      }
     }
   }
 
   options.allowNullDescriptor = getPipelineOptions()->extendedRobustness.nullDescriptor;
   options.disableImageResourceCheck = getPipelineOptions()->disableImageResourceCheck;
   options.enableInterpModePatch = getPipelineOptions()->enableInterpModePatch;
   options.pageMigrationEnabled = getPipelineOptions()->pageMigrationEnabled;
   options.resourceLayoutScheme = static_cast<lgc::ResourceLayoutScheme>(getPipelineOptions()->resourceLayoutScheme);
 
   // Driver report full subgroup lanes for compute shader, here we just set fullSubgroups as default options
diff --git a/llpc/test/shaderdb/general/PipelineVsFs_TestUberShader.pipe b/llpc/test/shaderdb/general/PipelineVsFs_TestUberShader.pipe
new file mode 100644
index 000000000..bef74d1eb
--- /dev/null
+++ b/llpc/test/shaderdb/general/PipelineVsFs_TestUberShader.pipe
@@ -0,0 +1,107 @@
+// Test uber shader
+
+; BEGIN_SHADERTEST
+; RUN: amdllpc -enable-opaque-pointers=true -v -gfxip=10.3 %s | FileCheck -check-prefix=SHADERTEST %s
+; SHADERTEST-LABEL: {{^// LLPC}} pipeline patching results
+; Load input descriptor
+
+; SHADERTEST: [[DESCPTR:%[0-9]*]] = bitcast <2 x i32> %rootDesc2 to i64
+; SHADERTEST: [[INTDESCPTR:%[0-9]*]] = inttoptr i64 [[DESCPTR]] to ptr addrspace(4)
+; SHADERTEST: [[UBERINFO:%[0-9]*]] = load <4 x i32>, ptr addrspace(4) [[INTDESCPTR]], align 16
+
+; Load vertex
+; SHADERTEST-COUNT-5: call i32 @llvm.amdgcn.struct.buffer.load.format
+; SHADERTEST: AMDLLPC SUCCESS
+; END_SHADERTEST
+
+[Version]
+version = 57
+
+[VsGlsl]
+#version 440
+layout(constant_id = 0) const int numAttributes = 16;
+layout(location = 0) in int attr0;
+layout(location = 0) out highp vec4 vtxColor;
+out gl_PerVertex {
+  vec4 gl_Position;
+};
+void main (void)
+{
+	int okCount = 0;
+	if (attr0 == -(1 * gl_InstanceIndex + 0))
+		okCount++;
+
+	if (okCount == 1)
+	{
+		if (gl_InstanceIndex == 0)
+			vtxColor = vec4(1.0, 0.0, 0.0, 1.0);
+		else
+			vtxColor = vec4(0.0, 0.0, 1.0, 1.0);
+	}
+	else
+	{
+		vtxColor = vec4(okCount / float(1), 0.0f, 0.0f, 1.0);
+	}
+
+	if (gl_InstanceIndex == 0)
+	{
+		if (gl_VertexIndex == 0) gl_Position = vec4(-1.0, -1.0, 0.0, 1.0);
+		else if (gl_VertexIndex == 1) gl_Position = vec4(0.0, -1.0, 0.0, 1.0);
+		else if (gl_VertexIndex == 2) gl_Position = vec4(-1.0, 1.0, 0.0, 1.0);
+		else if (gl_VertexIndex == 3) gl_Position = vec4(0.0, 1.0, 0.0, 1.0);
+		else gl_Position = vec4(0.0);
+	}
+	else
+	{
+		if (gl_VertexIndex == 0) gl_Position = vec4(0.0, -1.0, 0.0, 1.0);
+		else if (gl_VertexIndex == 1) gl_Position = vec4(1.0, -1.0, 0.0, 1.0);
+		else if (gl_VertexIndex == 2) gl_Position = vec4(0.0, 1.0, 0.0, 1.0);
+		else if (gl_VertexIndex == 3) gl_Position = vec4(1.0, 1.0, 0.0, 1.0);
+		else gl_Position = vec4(0.0);
+	}
+}
+
+[VsInfo]
+entryPoint = main
+
+[FsGlsl]
+#version 440
+layout(location = 0) in highp vec4 vtxColor;
+layout(location = 0) out highp vec4 fragColor;
+void main (void)
+{
+   fragColor = vtxColor;
+}
+
+[FsInfo]
+entryPoint = main
+
+
+[ResourceMapping]
+userDataNode[0].visibility = 4
+userDataNode[0].type = StreamOutTableVaPtr
+userDataNode[0].offsetInDwords = 0
+userDataNode[0].sizeInDwords = 1
+userDataNode[1].visibility = 2
+userDataNode[1].type = IndirectUserDataVaPtr
+userDataNode[1].offsetInDwords = 1
+userDataNode[1].sizeInDwords = 1
+userDataNode[1].indirectUserDataCount = 4
+userDataNode[2].visibility = 2
+userDataNode[2].type = DescriptorBufferCompact
+userDataNode[2].offsetInDwords = 2
+userDataNode[2].sizeInDwords = 2
+userDataNode[2].set = 0xFFFFFFFF
+userDataNode[2].binding = 5
+
+[GraphicsPipelineState]
+enableUberFetchShader = 1
+
+[VertexInputState]
+binding[0].binding = 0
+binding[0].stride = 2
+binding[0].inputRate = VK_VERTEX_INPUT_RATE_INSTANCE
+attribute[0].location = 0
+attribute[0].binding = 0
+attribute[0].format = VK_FORMAT_R16_SINT
+attribute[0].offset = 0
diff --git a/llpc/test/shaderdb/gfx9/PipelineVsFs_TestFetchSingleInput.pipe b/llpc/test/shaderdb/gfx9/PipelineVsFs_TestFetchSingleInput.pipe
index 20d66f070..3c52a7652 100644
--- a/llpc/test/shaderdb/gfx9/PipelineVsFs_TestFetchSingleInput.pipe
+++ b/llpc/test/shaderdb/gfx9/PipelineVsFs_TestFetchSingleInput.pipe
@@ -29,21 +29,22 @@
 ; BEGIN_SHADERTEST
 ; Check that the fetch shader loads the inputs into the correct registers.
 ; RUN: amdllpc -use-relocatable-shader-elf -o %t.elf %gfxip %s && llvm-objdump --triple=amdgcn --mcpu=gfx900 -d %t.elf | FileCheck -check-prefix=SHADERTEST2 %s
 ; SHADERTEST2: Disassembly of section .text:
 ; SHADERTEST2: 0000000000000000 <_amdgpu_vs_main>
 ; SHADERTEST2-DAG: v_mov_b32_e32 v7, 1.0
 ; SHADERTEST2-DAG: tbuffer_load_format_x v4, v{{[0-9]*}}, s[{{[0-9]+}}:{{[0-9]+}}],  {{dfmt:4, nfmt:7, 0|0 format:\[BUF_DATA_FORMAT_32,BUF_NUM_FORMAT_FLOAT\]}} idxen
 ; SHADERTEST2-DAG: tbuffer_load_format_x v5, v{{[0-9]*}}, s[{{[0-9]+}}:{{[0-9]+}}],  {{dfmt:4, nfmt:7, 0|0 format:\[BUF_DATA_FORMAT_32,BUF_NUM_FORMAT_FLOAT\]}} idxen offset:4
 ; SHADERTEST2-DAG: tbuffer_load_format_x v6, v{{[0-9]*}}, s[{{[0-9]+}}:{{[0-9]+}}],  {{dfmt:4, nfmt:7, 0|0 format:\[BUF_DATA_FORMAT_32,BUF_NUM_FORMAT_FLOAT\]}} idxen offset:8
 ; Identify the start of the vertex shader
-; SHADERTEST2: s_getpc_b64 s[0:1]
+; SHADERTEST2: <_amdgpu_vs_main_fetchless>:
+; SHADERTEST2: s_getpc_b64 s[{{[0-9]+}}:{{[0-9]+}}]
 ; Identify the start of the fragment shader, and check its alignment at the same time.
 ; SHADERTEST2: 00 <_amdgpu_ps_main>
 ; END_SHADERTEST
 
 
 
 [Version]
 version = 40
 
 [VsSpirv]
diff --git a/llpc/test/shaderdb/relocatable_shaders/DescBufferReloc.vert b/llpc/test/shaderdb/relocatable_shaders/DescBufferReloc.vert
index 5d44ee06d..4fbcb2bb2 100644
--- a/llpc/test/shaderdb/relocatable_shaders/DescBufferReloc.vert
+++ b/llpc/test/shaderdb/relocatable_shaders/DescBufferReloc.vert
@@ -1,20 +1,20 @@
 // This test case checks that the unlinked shader add all of the appropriate relocations for buffer descriptors.
 
 // BEGIN_SHADERTEST
 /*
 ; RUN: amdllpc -o %t.elf %gfxip %s -v | FileCheck -check-prefix=SHADERTEST %s
 ; SHADERTEST-LABEL: _amdgpu_vs_main_fetchless:
 ; SHADERTEST: s_cmp_eq_u32 dusespill_0_0@abs32@lo, 0
 ; SHADERTEST: s_cselect_b32 s[[RELOCCOND:[0-9]+]], s[[ds0:[0-9]*]], s[[spill:[0-9]*]]
 ; SHADERTEST: s_mov_b32 s[[RELOREG:[0-9]+]], doff_0_0_b@abs32@lo
-; SHADERTEST: s_load_dwordx4 s[{{.*}}:{{.*}}], s[{{.*}}:{{.*}}], s[[RELOREG]]
+; SHADERTEST: s_load_dwordx2 s[{{.*}}:{{.*}}], s[{{.*}}:{{.*}}], s[[RELOREG]]
 // register is reserved for the user data node holding descriptor set 0.
 ; SHADERTEST: SPI_SHADER_USER_DATA_VS_[[ds0]]                     0x0000000080000000
 // register pointed to the user data spill table
 ; SHADERTEST: SPI_SHADER_USER_DATA_VS_[[spill]]                     0x0000000010000002
 */
 // END_SHADERTEST
 
 #version 450
 #extension GL_ARB_separate_shader_objects : enable
 
diff --git a/llpc/test/shaderdb/relocatable_shaders/DescPtrSingleSelect.spvasm b/llpc/test/shaderdb/relocatable_shaders/DescPtrSingleSelect.spvasm
index ea93e9de0..0da60a816 100644
--- a/llpc/test/shaderdb/relocatable_shaders/DescPtrSingleSelect.spvasm
+++ b/llpc/test/shaderdb/relocatable_shaders/DescPtrSingleSelect.spvasm
@@ -1,20 +1,20 @@
 ; Check that selecting between spill descriptor pointer and descriptor table descriptor pointer results in just a single scalar select instruction.
 
 ; BEGIN_SHADERTEST
 ; RUN: amdllpc -o %t.elf -gfxip=9 -amdgpu-transform-discard-to-demote %s && llvm-objdump --triple=amdgcn --mcpu=gfx900 -d %t.elf | FileCheck -check-prefix=SHADERTEST %s
 ; SHADERTEST-LABEL:_amdgpu_ps_main
-; SHADERTEST: s_cbranch_scc0
+; SHADERTEST: s_getpc_b64
 ; SHADERTEST-COUNT-1: s_cselect_b32
 ; SHADERTEST-NOT: v_cndmask_b32
 ; SHADERTEST-NOT: v_readfirstlane_b32
-; SHADERTEST: s_load_dwordx4
+; SHADERTEST: s_load_dwordx2
 ; END_SHADERTEST
 
 ; SPIR-V
 ; Version: 1.3
 ; Generator: Khronos SPIR-V Tools Assembler; 0
 ; Bound: 36
 ; Schema: 0
                OpCapability Shader
                OpCapability SampledCubeArray
                OpCapability GroupNonUniform
diff --git a/llpc/test/shaderdb/relocatable_shaders/PipelineCs_RelocConst.pipe b/llpc/test/shaderdb/relocatable_shaders/PipelineCs_RelocConst.pipe
index d1334e8f6..4dd59f16e 100644
--- a/llpc/test/shaderdb/relocatable_shaders/PipelineCs_RelocConst.pipe
+++ b/llpc/test/shaderdb/relocatable_shaders/PipelineCs_RelocConst.pipe
@@ -1,28 +1,27 @@
 ; This test case checks that descriptor offset relocation works for buffer descriptors in a compute pipeline.
 ; BEGIN_SHADERTEST
 ; RUN: amdllpc -enable-relocatable-shader-elf -o %t.elf %gfxip %s && llvm-objdump --arch=amdgcn --mcpu=gfx900 -d %t.elf | FileCheck -check-prefix=SHADERTEST %s
 ; SHADERTEST-LABEL: 0000000000000000 <_amdgpu_cs_main>:
 ; SHADERTEST: s_mov_b32 s[[RELOREG:[0-9]+]], 16 {{.*}}
-; SHADERTEST: s_load_dwordx4 s[{{.*}}:{{.*}}], s[{{.*}}:{{.*}}], s[[RELOREG]] //{{.*}}
+; SHADERTEST: s_load_dwordx2 s[{{.*}}:{{.*}}], s[{{.*}}:{{.*}}], s[[RELOREG]] //{{.*}}
 ; END_SHADERTEST
 
 ; BEGIN_SHADERTEST
 ; RUN: amdllpc -enable-opaque-pointers=false -enable-relocatable-shader-elf -v %gfxip %s | FileCheck -check-prefix=SHADERTEST1 %s
 ; RUN: amdllpc -enable-opaque-pointers=true -enable-relocatable-shader-elf -v %gfxip %s | FileCheck -check-prefix=SHADERTEST1 %s
 ; SHADERTEST1-LABEL: {{^// LLPC}} pipeline patching results
 ; SHADERTEST1-DAG: ![[METADATANODE:[0-9]+]] = !{!"doff_0_0_b"}
 ; SHADERTEST1-DAG: %[[RELOCONST:[0-9]+]] = call i32 @llvm.amdgcn.reloc.constant(metadata ![[METADATANODE]])
 ; SHADERTEST1: %[[RELOCONSTEXT:[0-9]+]] = zext i32 %[[RELOCONST]] to i64
 ; SHADERTEST1: %[[BUFFFERDESCADDR:[0-9]+]] = add i64 %{{[0-9]+}}, %[[RELOCONSTEXT]]
 ; SHADERTEST1: %[[BUFFERDESCPTR:[0-9]+]] = inttoptr i64 %[[BUFFFERDESCADDR]] {{.*}}
-; SHADERTEST1: %{{[0-9]+}} = load <4 x i32>, {{<4 x i32> addrspace\(4\)\*|ptr addrspace\(4\)}} %[[BUFFERDESCPTR]], align 16
 ; SHADERTEST1: COMPUTE_USER_DATA_{{.}} 0x0000000000000006
 ; SHADERTEST1: COMPUTE_USER_DATA_{{.}} 0x0000000000000007
 ; SHADERTEST1: AMDLLPC SUCCESS
 ; END_SHADERTEST
 
 [CsGlsl]
 #version 450
 #extension GL_ARB_separate_shader_objects : enable
 
 layout(binding = 0) uniform UniformBufferObject {
diff --git a/llpc/test/shaderdb/relocatable_shaders/PipelineVsFs_RelocConst.pipe b/llpc/test/shaderdb/relocatable_shaders/PipelineVsFs_RelocConst.pipe
index 9287a0029..d25e72768 100644
--- a/llpc/test/shaderdb/relocatable_shaders/PipelineVsFs_RelocConst.pipe
+++ b/llpc/test/shaderdb/relocatable_shaders/PipelineVsFs_RelocConst.pipe
@@ -1,41 +1,39 @@
 // This test case checks that descriptor offset and descriptor buffer pointer relocation works
 // for buffer descriptors in a vs/fs pipeline.
 // Also check that the user data limit is set correctly.
 
 ; BEGIN_SHADERTEST
 ; RUN: amdllpc -enable-relocatable-shader-elf -o %t.elf %gfxip %s && llvm-objdump --triple=amdgcn --mcpu=gfx900 -d %t.elf | FileCheck -check-prefix=SHADERTEST %s
 ; SHADERTEST-LABEL: 0000000000000000 <_amdgpu_vs_main>:
 ; SHADERTEST: s_cmp_eq_u32 0, 0 //{{.*}}
 ; SHADERTEST: s_cselect_b32 s[[RELOCCOND:[0-9]+]], s{{[0-9]+}}, s{{[0-9]+}} //{{.*}}
-; SHADERTEST: s_mov_b32 s[[RELOREG:[0-9]+]], 12 //{{.*}}
-; SHADERTEST: s_load_dwordx4 s[{{.*}}:{{.*}}], s[{{.*}}:{{.*}}], s[[RELOREG]] //{{.*}}
+; SHADERTEST: s_add_u32 s[[RELOREG:[0-9]+]], s[[RELOCCOND]], 12
 ; SHADERTEST: {{[0-9A-Za-z]+}} <_amdgpu_ps_main>:
 ; SHADERTEST: s_cmp_eq_u32 0, 0 //{{.*}}
 ; SHADERTEST: s_cselect_b32 s[[RELOCCOND1:[0-9]+]], s{{[0-9]+}}, s{{[0-9]+}} //{{.*}}
-; SHADERTEST: s_mov_b32 s[[RELOREG1:[0-9]+]], 12 //{{.*}}
-; SHADERTEST: s_load_dwordx4 s[{{.*}}:{{.*}}], s[{{.*}}:{{.*}}], s[[RELOREG1]] //{{.*}}
+; SHADERTEST: s_add_u32 s[[RELOREG1:[0-9]+]], s[[RELOCCOND1]], 12
+
 ; END_SHADERTEST
 
 ; BEGIN_SHADERTEST
 ; RUN: amdllpc -enable-relocatable-shader-elf -enable-opaque-pointers=true -v %gfxip %s | FileCheck -check-prefix=SHADERTEST1 %s
 ; RUN: amdllpc -enable-relocatable-shader-elf -enable-opaque-pointers=false -v %gfxip %s | FileCheck -check-prefix=SHADERTEST1 %s
 ; SHADERTEST1-LABEL: {{^// LLPC}} pipeline patching results
 ; SHADERTEST1-LABEL: _amdgpu_vs_main_fetchless
 ; SHADERTEST1: %[[SPILLCONST:.+]] = call i32 @llvm.amdgcn.reloc.constant(metadata ![[SPILLNODE:[0-9]+]])
 ; SHADERTEST1: %[[CONSTICMP:.+]] = icmp eq i32 %[[SPILLCONST]], 0
 ; SHADERTEST1: %[[DESCPTRSELECT:.+]] = select i1 %[[CONSTICMP]], {{.*}}
 ; SHADERTEST1: %[[OFFCONST:.+]] = call i32 @llvm.amdgcn.reloc.constant(metadata ![[OFFNODE:[0-9]+]])
 ; SHADERTEST1: %[[OFFCONSTEXT:.+]] = zext i32 %[[OFFCONST]] to i64
 ; SHADERTEST1: %[[BUFFFERDESCADDR:.+]] = add i64 %{{.+}}, %[[OFFCONSTEXT]]
 ; SHADERTEST1: %[[BUFFERDESCPTR:.+]] = inttoptr i64 %[[BUFFFERDESCADDR]] {{.*}}
-; SHADERTEST1: %{{.+}} = load <4 x i32>, {{<4 x i32> addrspace\(4\)\*|ptr addrspace\(4\)}} %[[BUFFERDESCPTR]], align 1
 ; SHADERTEST1: ![[SPILLNODE]] = !{!"dusespill_0_0"}
 ; SHADERTEST1: ![[OFFNODE]] = !{!"doff_0_0_b"}
 ; SHADERTEST1-LABEL: {{^// LLPC}} final pipeline module info
 ; SHADERTEST1-DAG: SPI_SHADER_USER_DATA_VS_{{.}} 0x000000000000000B
 ; SHADERTEST1-DAG: SPI_SHADER_USER_DATA_PS_{{.}} 0x000000000000000B
 ; SHADERTEST1: .spill_threshold: 0x00000000FFFFFFFF
 ; SHADERTEST1: .user_data_limit: 0x000000000000000C
 ; SHADERTEST1: AMDLLPC SUCCESS
 ; END_SHADERTEST
 
diff --git a/llpc/test/shaderdb/relocatable_shaders/PipelineVsFs_RelocDescriptorCompact.pipe b/llpc/test/shaderdb/relocatable_shaders/PipelineVsFs_RelocDescriptorCompact.pipe
new file mode 100644
index 000000000..6acd9aa30
--- /dev/null
+++ b/llpc/test/shaderdb/relocatable_shaders/PipelineVsFs_RelocDescriptorCompact.pipe
@@ -0,0 +1,115 @@
+// This test case checks the use of descriptor compact buffer:
+// 1. If set is -1, it is a internal buffer,use spill table to load it
+// 2. If set is not -1, it may be descriptorBufferCompact or descriptorBuffer if there is no pipeline layout
+
+; BEGIN_SHADERTEST
+; RUN: amdllpc -enable-relocatable-shader-elf -enable-opaque-pointers=true -v -gfxip=10.3 %s | FileCheck -check-prefix=SHADERTEST %s
+; SHADERTEST-LABEL: {{^}}// LLPC pipeline patching results
+; set = -1, binding = 5, this is a internal buffer
+; SHADERTEST:call i32 @llvm.amdgcn.reloc.constant(metadata ![[OFFSET_4294967295_5:[0-9]+]])
+; set = 0, binding = 1, this is a buffer that app creates.
+; SHADERTEST: [[USESPILL:%[0-9]*]] = call i32 @llvm.amdgcn.reloc.constant(metadata ![[NODE_0_1:[0-9]+]])
+; SHADERTEST: [[NOTUSESPILL:%[^,]*]] = icmp eq i32 [[USESPILL]], 0
+; SHADERTEST: select i1 [[NOTUSESPILL]], i32 %descTable0, i32 %spillTable
+; SHADERTEST: call i32 @llvm.amdgcn.reloc.constant(metadata
+; determine if it is a compact buffer
+; SHADERTEST: [[isCompact:%[0-9]*]] = call i32 @llvm.amdgcn.reloc.constant(metadata ![[COMPACTBUFFER_0_1:[0-9]+]])
+; SHADERTEST: icmp eq i32 [[isCompact]], 0
+; SHADERTEST: ![[OFFSET_4294967295_5]] = !{!"doff_4294967295_5_b"}
+; SHADERTEST: ![[NODE_0_1]] = !{!"dusespill_0_1"}
+; SHADERTEST: ![[COMPACTBUFFER_0_1]] = !{!"compactbuffer_0_1"}
+; SHADERTEST: AMDLLPC SUCCESS
+; END_SHADERTEST
+
+[Version]
+version = 57
+
+[VsGlsl]
+#version 450
+
+layout (location = 0) in vec3 inPos;
+layout (location = 1) in vec3 inColor;
+
+layout (binding = 1) uniform UboInstance 
+{
+	mat4 model; 
+} uboInstance;
+
+layout (location = 0) out vec3 outColor;
+
+out gl_PerVertex 
+{
+	vec4 gl_Position;
+};
+
+void main() 
+{
+	outColor = inColor;
+	gl_Position = uboInstance.model * vec4(inPos.xyz, 1.0);
+}
+
+[VsInfo]
+entryPoint = main
+
+[FsGlsl]
+#version 450
+
+layout (location = 0) in vec3 inColor;
+
+layout (location = 0) out vec4 outFragColor;
+
+void main() 
+{
+	outFragColor = vec4(inColor, 1.0);	
+}
+
+[FsInfo]
+entryPoint = main
+
+[ResourceMapping]
+userDataNode[0].visibility = 2
+userDataNode[0].type = DescriptorConstBufferCompact
+userDataNode[0].offsetInDwords = 0
+userDataNode[0].sizeInDwords = 2
+userDataNode[0].set = 0x00000000
+userDataNode[0].binding = 1
+userDataNode[1].visibility = 2
+userDataNode[1].type = DescriptorTableVaPtr
+userDataNode[1].offsetInDwords = 2
+userDataNode[1].sizeInDwords = 1
+userDataNode[1].next[0].type = DescriptorConstBuffer
+userDataNode[1].next[0].offsetInDwords = 0
+userDataNode[1].next[0].sizeInDwords = 4
+userDataNode[1].next[0].set = 0x00000000
+userDataNode[1].next[0].binding = 0
+userDataNode[2].visibility = 2
+userDataNode[2].type = IndirectUserDataVaPtr
+userDataNode[2].offsetInDwords = 3
+userDataNode[2].sizeInDwords = 1
+userDataNode[2].indirectUserDataCount = 4
+userDataNode[3].visibility = 2
+userDataNode[3].type = DescriptorBufferCompact
+userDataNode[3].offsetInDwords = 4
+userDataNode[3].sizeInDwords = 2
+userDataNode[3].set = 0xFFFFFFFF
+userDataNode[3].binding = 5
+
+[GraphicsPipelineState]
+colorBuffer[0].format = VK_FORMAT_B8G8R8A8_UNORM
+colorBuffer[0].channelWriteMask = 15
+colorBuffer[0].blendEnable = 0
+colorBuffer[0].blendSrcAlphaToColor = 0
+enableUberFetchShader = 1
+
+[VertexInputState]
+binding[0].binding = 0
+binding[0].stride = 24
+binding[0].inputRate = VK_VERTEX_INPUT_RATE_VERTEX
+attribute[0].location = 0
+attribute[0].binding = 0
+attribute[0].format = VK_FORMAT_R32G32B32_SFLOAT
+attribute[0].offset = 0
+attribute[1].location = 1
+attribute[1].binding = 0
+attribute[1].format = VK_FORMAT_R32G32B32_SFLOAT
+attribute[1].offset = 12
diff --git a/llpc/test/shaderdb/relocatable_shaders/PipelineVsFs_RelocTopLevelDescBuffer.pipe b/llpc/test/shaderdb/relocatable_shaders/PipelineVsFs_RelocTopLevelDescBuffer.pipe
index 2a5efac4f..66e570d9e 100644
--- a/llpc/test/shaderdb/relocatable_shaders/PipelineVsFs_RelocTopLevelDescBuffer.pipe
+++ b/llpc/test/shaderdb/relocatable_shaders/PipelineVsFs_RelocTopLevelDescBuffer.pipe
@@ -1,27 +1,23 @@
 // This test case checks that descriptor offset and descriptor buffer pointer relocation works
 // for top-level buffer descriptors in a vs/fs pipeline.
 //
 // Based on PipelineVsFS_RelocConst.pipe.
 
 ; BEGIN_SHADERTEST
 ; RUN: amdllpc -enable-relocatable-shader-elf -o %t.elf %gfxip %s && llvm-objdump --triple=amdgcn --mcpu=gfx900 -d %t.elf | FileCheck -check-prefix=SHADERTEST %s
-; SHADERTEST-LABEL: 0000000000000000 <_amdgpu_vs_main>:
-; SHADERTEST: s_cmp_eq_u32 1, 0 //{{.*}}
-; SHADERTEST: s_cselect_b32 s[[RELOCCOND:[0-9]+]], s{{[0-9]+}}, s{{[0-9]+}} //{{.*}}
-; SHADERTEST: s_mov_b32 s[[RELOREG:[0-9]+]], 0 //{{.*}}
-; SHADERTEST: s_load_dwordx4 s[{{.*}}:{{.*}}], s[{{.*}}:{{.*}}], s[[RELOREG]] //{{.*}}
+; SHADERTEST-LABEL: {{[0-9A-Za-z]+}} <_amdgpu_vs_main_fetchless>:
+; SHADERTEST: s_getpc_b64
+; SHADERTEST-NEXT: s_cmp_eq_u32 1, 0 //{{.*}}
 ; SHADERTEST: {{[0-9A-Za-z]+}} <_amdgpu_ps_main>:
-; SHADERTEST: s_cmp_eq_u32 1, 0 //{{.*}}
-; SHADERTEST: s_cselect_b32 s[[RELOCCOND1:[0-9]+]], s{{[0-9]+}}, s{{[0-9]+}} //{{.*}}
-; SHADERTEST: s_mov_b32 s[[RELOREG1:[0-9]+]], 0 //{{.*}}
-; SHADERTEST: s_load_dwordx4 s[{{.*}}:{{.*}}], s[{{.*}}:{{.*}}], s[[RELOREG1]] //{{.*}}
+; SHADERTEST: s_getpc_b64
+; SHADERTEST-NEXT: s_cmp_eq_u32 1, 0 //{{.*}}
 ; END_SHADERTEST
 
 [VsGlsl]
 #version 450
 #extension GL_ARB_separate_shader_objects : enable
 
 layout(binding = 0) uniform UniformBufferObject {
     mat4 model;
     mat4 view;
     vec4 proj;
