--- llvm1/old
+++ llvm1/new
@@ -1,4 +1,4 @@
-marker new
+marker old
 //===---- TargetInfo.cpp - Encapsulate target details -----------*- C++ -*-===//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
@@ -24,6 +24,7 @@
 #include "clang/Basic/CodeGenOptions.h"
 #include "clang/Basic/DiagnosticFrontend.h"
 #include "clang/CodeGen/CGFunctionInfo.h"
+#include "clang/CodeGen/SwiftCallingConv.h"
 #include "llvm/ADT/SmallBitVector.h"
 #include "llvm/ADT/StringExtras.h"
 #include "llvm/ADT/StringSwitch.h"
@@ -35,7 +36,7 @@
 #include "llvm/IR/Type.h"
 #include "llvm/Support/MathExtras.h"
 #include "llvm/Support/raw_ostream.h"
-#include <algorithm>
+#include <algorithm> // std::sort
 
 using namespace clang;
 using namespace CodeGen;
@@ -101,13 +102,8 @@
   return Address::invalid();
 }
 
-static llvm::Type *getVAListElementType(CodeGenFunction &CGF) {
-  return CGF.ConvertTypeForMem(
-      CGF.getContext().getBuiltinVaListType()->getPointeeType());
-}
-
 bool ABIInfo::isPromotableIntegerTypeForABI(QualType Ty) const {
-  if (getContext().isPromotableIntegerType(Ty))
+  if (Ty->isPromotableIntegerType())
     return true;
 
   if (const auto *EIT = Ty->getAs<BitIntType>())
@@ -117,10 +113,8 @@
   return false;
 }
 
-ABIInfo::~ABIInfo() = default;
+ABIInfo::~ABIInfo() {}
 
-SwiftABIInfo::~SwiftABIInfo() = default;
-
 /// Does the given lowering require more than the given number of
 /// registers when expanded?
 ///
@@ -142,7 +136,7 @@
     if (type->isPointerTy()) {
       intCount++;
     } else if (auto intTy = dyn_cast<llvm::IntegerType>(type)) {
-      auto ptrWidth = cgt.getTarget().getPointerWidth(LangAS::Default);
+      auto ptrWidth = cgt.getTarget().getPointerWidth(0);
       intCount += (intTy->getBitWidth() + ptrWidth - 1) / ptrWidth;
     } else {
       assert(type->isVectorTy() || type->isFloatingPointTy());
@@ -153,16 +147,12 @@
   return (intCount + fpCount > maxAllRegisters);
 }
 
-bool SwiftABIInfo::shouldPassIndirectly(ArrayRef<llvm::Type *> ComponentTys,
-                                        bool AsReturnValue) const {
-  return occupiesMoreThan(CGT, ComponentTys, /*total=*/4);
-}
-
-bool SwiftABIInfo::isLegalVectorType(CharUnits VectorSize, llvm::Type *EltTy,
-                                     unsigned NumElts) const {
+bool SwiftABIInfo::isLegalVectorTypeForSwift(CharUnits vectorSize,
+                                             llvm::Type *eltTy,
+                                             unsigned numElts) const {
   // The default implementation of this assumes that the target guarantees
   // 128-bit SIMD support but nothing more.
-  return (VectorSize.getQuantity() > 8 && VectorSize.getQuantity() <= 16);
+  return (vectorSize.getQuantity() > 8 && vectorSize.getQuantity() <= 16);
 }
 
 static CGCXXABI::RecordArgABI getRecordArgABI(const RecordType *RT,
@@ -246,11 +236,6 @@
   return false;
 }
 
-bool ABIInfo::isZeroLengthBitfieldPermittedInHomogeneousAggregate() const {
-  // For compatibility with GCC, ignore empty bitfields in C++ mode.
-  return getContext().getLangOpts().CPlusPlus;
-}
-
 LLVM_DUMP_METHOD void ABIArgInfo::dump() const {
   raw_ostream &OS = llvm::errs();
   OS << "(ABIArgInfo Kind=";
@@ -323,17 +308,13 @@
 ///   leaving one or more empty slots behind as padding.  If this
 ///   is false, the returned address might be less-aligned than
 ///   DirectAlign.
-/// \param ForceRightAdjust - Default is false. On big-endian platform and
-///   if the argument is smaller than a slot, set this flag will force
-///   right-adjust the argument in its slot irrespective of the type.
 static Address emitVoidPtrDirectVAArg(CodeGenFunction &CGF,
                                       Address VAListAddr,
                                       llvm::Type *DirectTy,
                                       CharUnits DirectSize,
                                       CharUnits DirectAlign,
                                       CharUnits SlotSize,
-                                      bool AllowHigherAlign,
-                                      bool ForceRightAdjust = false) {
+                                      bool AllowHigherAlign) {
   // Cast the element type to i8* if necessary.  Some platforms define
   // va_list as a struct containing an i8* instead of just an i8*.
   if (VAListAddr.getElementType() != CGF.Int8PtrTy)
@@ -345,9 +326,9 @@
   Address Addr = Address::invalid();
   if (AllowHigherAlign && DirectAlign > SlotSize) {
     Addr = Address(emitRoundPointerUpToAlignment(CGF, Ptr, DirectAlign),
-                   CGF.Int8Ty, DirectAlign);
+                                                 DirectAlign);
   } else {
-    Addr = Address(Ptr, CGF.Int8Ty, SlotSize);
+    Addr = Address(Ptr, SlotSize);
   }
 
   // Advance the pointer past the argument, then store that back.
@@ -359,7 +340,7 @@
   // If the argument is smaller than a slot, and this is a big-endian
   // target, the argument will be right-adjusted in its slot.
   if (DirectSize < SlotSize && CGF.CGM.getDataLayout().isBigEndian() &&
-      (!DirectTy->isStructTy() || ForceRightAdjust)) {
+      !DirectTy->isStructTy()) {
     Addr = CGF.Builder.CreateConstInBoundsByteGEP(Addr, SlotSize - DirectSize);
   }
 
@@ -380,15 +361,11 @@
 ///   an argument type with an alignment greater than the slot size
 ///   will be emitted on a higher-alignment address, potentially
 ///   leaving one or more empty slots behind as padding.
-/// \param ForceRightAdjust - Default is false. On big-endian platform and
-///   if the argument is smaller than a slot, set this flag will force
-///   right-adjust the argument in its slot irrespective of the type.
 static Address emitVoidPtrVAArg(CodeGenFunction &CGF, Address VAListAddr,
                                 QualType ValueTy, bool IsIndirect,
                                 TypeInfoChars ValueInfo,
                                 CharUnits SlotSizeAndAlign,
-                                bool AllowHigherAlign,
-                                bool ForceRightAdjust = false) {
+                                bool AllowHigherAlign) {
   // The size and alignment of the value that was passed directly.
   CharUnits DirectSize, DirectAlign;
   if (IsIndirect) {
@@ -400,19 +377,21 @@
   }
 
   // Cast the address we've calculated to the right type.
-  llvm::Type *DirectTy = CGF.ConvertTypeForMem(ValueTy), *ElementTy = DirectTy;
+  llvm::Type *DirectTy = CGF.ConvertTypeForMem(ValueTy);
   if (IsIndirect)
     DirectTy = DirectTy->getPointerTo(0);
 
-  Address Addr = emitVoidPtrDirectVAArg(CGF, VAListAddr, DirectTy, DirectSize,
-                                        DirectAlign, SlotSizeAndAlign,
-                                        AllowHigherAlign, ForceRightAdjust);
+  Address Addr = emitVoidPtrDirectVAArg(CGF, VAListAddr, DirectTy,
+                                        DirectSize, DirectAlign,
+                                        SlotSizeAndAlign,
+                                        AllowHigherAlign);
 
   if (IsIndirect) {
-    Addr = Address(CGF.Builder.CreateLoad(Addr), ElementTy, ValueInfo.Align);
+    Addr = Address(CGF.Builder.CreateLoad(Addr), ValueInfo.Align);
   }
 
   return Addr;
+
 }
 
 static Address complexTempStructure(CodeGenFunction &CGF, Address VAListAddr,
@@ -457,9 +436,6 @@
   return Address(PHI, Addr1.getElementType(), Align);
 }
 
-TargetCodeGenInfo::TargetCodeGenInfo(std::unique_ptr<ABIInfo> Info)
-    : Info(std::move(Info)) {}
-
 TargetCodeGenInfo::~TargetCodeGenInfo() = default;
 
 // If someone can figure out a general rule for this, that would be great.
@@ -716,11 +692,11 @@
     auto TyInfo = CGF.getContext().getTypeInfoInChars(Ty);
     CharUnits TyAlignForABI = TyInfo.Align;
 
-    llvm::Type *ElementTy = CGF.ConvertTypeForMem(Ty);
-    llvm::Type *BaseTy = llvm::PointerType::getUnqual(ElementTy);
+    llvm::Type *BaseTy =
+        llvm::PointerType::getUnqual(CGF.ConvertTypeForMem(Ty));
     llvm::Value *Addr =
         CGF.Builder.CreateVAArg(VAListAddr.getPointer(), BaseTy);
-    return Address(Addr, ElementTy, TyAlignForABI);
+    return Address(Addr, TyAlignForABI);
   } else {
     assert((AI.isDirect() || AI.isExtend()) &&
            "Unexpected ArgInfo Kind in generic VAArg emitter!");
@@ -735,8 +711,7 @@
            "Unexpected CoerceToType seen in arginfo in generic VAArg emitter!");
 
     Address Temp = CGF.CreateMemTemp(Ty, "varet");
-    Val = CGF.Builder.CreateVAArg(VAListAddr.getPointer(),
-                                  CGF.ConvertTypeForMem(Ty));
+    Val = CGF.Builder.CreateVAArg(VAListAddr.getPointer(), CGF.ConvertType(Ty));
     CGF.Builder.CreateStore(Val, Temp);
     return Temp;
   }
@@ -828,7 +803,7 @@
 // This is a very simple ABI that relies a lot on DefaultABIInfo.
 //===----------------------------------------------------------------------===//
 
-class WebAssemblyABIInfo final : public ABIInfo {
+class WebAssemblyABIInfo final : public SwiftABIInfo {
 public:
   enum ABIKind {
     MVP = 0,
@@ -841,7 +816,7 @@
 
 public:
   explicit WebAssemblyABIInfo(CodeGen::CodeGenTypes &CGT, ABIKind Kind)
-      : ABIInfo(CGT), defaultInfo(CGT), Kind(Kind) {}
+      : SwiftABIInfo(CGT), defaultInfo(CGT), Kind(Kind) {}
 
 private:
   ABIArgInfo classifyReturnType(QualType RetTy) const;
@@ -859,16 +834,22 @@
 
   Address EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
                     QualType Ty) const override;
+
+  bool shouldPassIndirectlyForSwift(ArrayRef<llvm::Type*> scalars,
+                                    bool asReturnValue) const override {
+    return occupiesMoreThan(CGT, scalars, /*total*/ 4);
+  }
+
+  bool isSwiftErrorInRegister() const override {
+    return false;
+  }
 };
 
 class WebAssemblyTargetCodeGenInfo final : public TargetCodeGenInfo {
 public:
   explicit WebAssemblyTargetCodeGenInfo(CodeGen::CodeGenTypes &CGT,
                                         WebAssemblyABIInfo::ABIKind K)
-      : TargetCodeGenInfo(std::make_unique<WebAssemblyABIInfo>(CGT, K)) {
-    SwiftInfo =
-        std::make_unique<SwiftABIInfo>(CGT, /*SwiftErrorInRegister=*/false);
-  }
+      : TargetCodeGenInfo(std::make_unique<WebAssemblyABIInfo>(CGT, K)) {}
 
   void setTargetAttributes(const Decl *D, llvm::GlobalValue *GV,
                            CodeGen::CodeGenModule &CGM) const override {
@@ -1144,7 +1125,7 @@
 };
 
 /// X86_32ABIInfo - The X86-32 ABI information.
-class X86_32ABIInfo : public ABIInfo {
+class X86_32ABIInfo : public SwiftABIInfo {
   enum Class {
     Integer,
     Float
@@ -1218,28 +1199,27 @@
   X86_32ABIInfo(CodeGen::CodeGenTypes &CGT, bool DarwinVectorABI,
                 bool RetSmallStructInRegABI, bool Win32StructABI,
                 unsigned NumRegisterParameters, bool SoftFloatABI)
-      : ABIInfo(CGT), IsDarwinVectorABI(DarwinVectorABI),
-        IsRetSmallStructInRegABI(RetSmallStructInRegABI),
-        IsWin32StructABI(Win32StructABI), IsSoftFloatABI(SoftFloatABI),
-        IsMCUABI(CGT.getTarget().getTriple().isOSIAMCU()),
-        IsLinuxABI(CGT.getTarget().getTriple().isOSLinux() ||
-                   CGT.getTarget().getTriple().isOSCygMing()),
-        DefaultNumRegisterParameters(NumRegisterParameters) {}
-};
+    : SwiftABIInfo(CGT), IsDarwinVectorABI(DarwinVectorABI),
+      IsRetSmallStructInRegABI(RetSmallStructInRegABI),
+      IsWin32StructABI(Win32StructABI), IsSoftFloatABI(SoftFloatABI),
+      IsMCUABI(CGT.getTarget().getTriple().isOSIAMCU()),
+      IsLinuxABI(CGT.getTarget().getTriple().isOSLinux() ||
+                 CGT.getTarget().getTriple().isOSCygMing()),
+      DefaultNumRegisterParameters(NumRegisterParameters) {}
 
-class X86_32SwiftABIInfo : public SwiftABIInfo {
-public:
-  explicit X86_32SwiftABIInfo(CodeGenTypes &CGT)
-      : SwiftABIInfo(CGT, /*SwiftErrorInRegister=*/false) {}
-
-  bool shouldPassIndirectly(ArrayRef<llvm::Type *> ComponentTys,
-                            bool AsReturnValue) const override {
+  bool shouldPassIndirectlyForSwift(ArrayRef<llvm::Type*> scalars,
+                                    bool asReturnValue) const override {
     // LLVM's x86-32 lowering currently only assigns up to three
     // integer registers and three fp registers.  Oddly, it'll use up to
     // four vector registers for vectors, but those can overlap with the
     // scalar registers.
-    return occupiesMoreThan(CGT, ComponentTys, /*total=*/3);
+    return occupiesMoreThan(CGT, scalars, /*total*/ 3);
   }
+
+  bool isSwiftErrorInRegister() const override {
+    // x86-32 lowering does not support passing swifterror in a register.
+    return false;
+  }
 };
 
 class X86_32TargetCodeGenInfo : public TargetCodeGenInfo {
@@ -1249,9 +1229,7 @@
                           unsigned NumRegisterParameters, bool SoftFloatABI)
       : TargetCodeGenInfo(std::make_unique<X86_32ABIInfo>(
             CGT, DarwinVectorABI, RetSmallStructInRegABI, Win32StructABI,
-            NumRegisterParameters, SoftFloatABI)) {
-    SwiftInfo = std::make_unique<X86_32SwiftABIInfo>(CGT);
-  }
+            NumRegisterParameters, SoftFloatABI)) {}
 
   static bool isStructReturnInRegABI(
       const llvm::Triple &Triple, const CodeGenOptions &Opts);
@@ -1375,8 +1353,8 @@
   ResultTruncRegTypes.push_back(CoerceTy);
 
   // Coerce the integer by bitcasting the return slot pointer.
-  ReturnSlot.setAddress(
-      CGF.Builder.CreateElementBitCast(ReturnSlot.getAddress(CGF), CoerceTy));
+  ReturnSlot.setAddress(CGF.Builder.CreateBitCast(ReturnSlot.getAddress(CGF),
+                                                  CoerceTy->getPointerTo()));
   ResultRegDests.push_back(ReturnSlot);
 
   rewriteInputConstraintReferences(NumOutputs, 1, AsmString);
@@ -1780,22 +1758,23 @@
 }
 
 bool X86_32ABIInfo::shouldPrimitiveUseInReg(QualType Ty, CCState &State) const {
-  bool IsPtrOrInt = (getContext().getTypeSize(Ty) <= 32) &&
-                    (Ty->isIntegralOrEnumerationType() || Ty->isPointerType() ||
-                     Ty->isReferenceType());
-
-  if (!IsPtrOrInt && (State.CC == llvm::CallingConv::X86_FastCall ||
-                      State.CC == llvm::CallingConv::X86_VectorCall))
-    return false;
-
   if (!updateFreeRegs(Ty, State))
     return false;
 
-  if (!IsPtrOrInt && State.CC == llvm::CallingConv::X86_RegCall)
+  if (IsMCUABI)
     return false;
 
-  // Return true to apply inreg to all legal parameters except for MCU targets.
-  return !IsMCUABI;
+  if (State.CC == llvm::CallingConv::X86_FastCall ||
+      State.CC == llvm::CallingConv::X86_VectorCall ||
+      State.CC == llvm::CallingConv::X86_RegCall) {
+    if (getContext().getTypeSize(Ty) > 32)
+      return false;
+
+    return (Ty->isIntegralOrEnumerationType() || Ty->isPointerType() ||
+        Ty->isReferenceType());
+  }
+
+  return true;
 }
 
 void X86_32ABIInfo::runVectorCallFirstPass(CGFunctionInfo &FI, CCState &State) const {
@@ -2260,7 +2239,7 @@
 }
 
 /// X86_64ABIInfo - The X86_64 ABI information.
-class X86_64ABIInfo : public ABIInfo {
+class X86_64ABIInfo : public SwiftABIInfo {
   enum Class {
     Integer = 0,
     SSE,
@@ -2315,8 +2294,6 @@
   /// \param isNamedArg - Whether the argument in question is a "named"
   /// argument, as used in AMD64-ABI 3.5.7.
   ///
-  /// \param IsRegCall - Whether the calling conversion is regcall.
-  ///
   /// If a word is unused its result will be NoClass; if a type should
   /// be passed in Memory then at least the classification of \arg Lo
   /// will be Memory.
@@ -2326,7 +2303,7 @@
   /// If the \arg Lo class is ComplexX87, then the \arg Hi class will
   /// also be ComplexX87.
   void classify(QualType T, uint64_t OffsetBase, Class &Lo, Class &Hi,
-                bool isNamedArg, bool IsRegCall = false) const;
+                bool isNamedArg) const;
 
   llvm::Type *GetByteVectorType(QualType Ty) const;
   llvm::Type *GetSSETypeAtOffset(llvm::Type *IRType,
@@ -2351,16 +2328,13 @@
 
   ABIArgInfo classifyArgumentType(QualType Ty, unsigned freeIntRegs,
                                   unsigned &neededInt, unsigned &neededSSE,
-                                  bool isNamedArg,
-                                  bool IsRegCall = false) const;
+                                  bool isNamedArg) const;
 
   ABIArgInfo classifyRegCallStructType(QualType Ty, unsigned &NeededInt,
-                                       unsigned &NeededSSE,
-                                       unsigned &MaxVectorWidth) const;
+                                       unsigned &NeededSSE) const;
 
   ABIArgInfo classifyRegCallStructTypeImpl(QualType Ty, unsigned &NeededInt,
-                                           unsigned &NeededSSE,
-                                           unsigned &MaxVectorWidth) const;
+                                           unsigned &NeededSSE) const;
 
   bool IsIllegalVectorType(QualType Ty) const;
 
@@ -2382,7 +2356,7 @@
       return false;
 
     const llvm::Triple &Triple = getTarget().getTriple();
-    if (Triple.isOSDarwin() || Triple.isPS())
+    if (Triple.isOSDarwin() || Triple.getOS() == llvm::Triple::PS4)
       return false;
     if (Triple.isOSFreeBSD() && Triple.getOSMajorVersion() >= 10)
       return false;
@@ -2406,9 +2380,10 @@
   bool Has64BitPointers;
 
 public:
-  X86_64ABIInfo(CodeGen::CodeGenTypes &CGT, X86AVXABILevel AVXLevel)
-      : ABIInfo(CGT), AVXLevel(AVXLevel),
-        Has64BitPointers(CGT.getDataLayout().getPointerSize(0) == 8) {}
+  X86_64ABIInfo(CodeGen::CodeGenTypes &CGT, X86AVXABILevel AVXLevel) :
+      SwiftABIInfo(CGT), AVXLevel(AVXLevel),
+      Has64BitPointers(CGT.getDataLayout().getPointerSize(0) == 8) {
+  }
 
   bool isPassedUsingAVXType(QualType type) const {
     unsigned neededInt, neededSSE;
@@ -2433,13 +2408,21 @@
   bool has64BitPointers() const {
     return Has64BitPointers;
   }
+
+  bool shouldPassIndirectlyForSwift(ArrayRef<llvm::Type*> scalars,
+                                    bool asReturnValue) const override {
+    return occupiesMoreThan(CGT, scalars, /*total*/ 4);
+  }
+  bool isSwiftErrorInRegister() const override {
+    return true;
+  }
 };
 
 /// WinX86_64ABIInfo - The Windows X86_64 ABI information.
-class WinX86_64ABIInfo : public ABIInfo {
+class WinX86_64ABIInfo : public SwiftABIInfo {
 public:
   WinX86_64ABIInfo(CodeGen::CodeGenTypes &CGT, X86AVXABILevel AVXLevel)
-      : ABIInfo(CGT), AVXLevel(AVXLevel),
+      : SwiftABIInfo(CGT), AVXLevel(AVXLevel),
         IsMingw64(getTarget().getTriple().isWindowsGNUEnvironment()) {}
 
   void computeInfo(CGFunctionInfo &FI) const override;
@@ -2458,6 +2441,15 @@
     return isX86VectorCallAggregateSmallEnough(NumMembers);
   }
 
+  bool shouldPassIndirectlyForSwift(ArrayRef<llvm::Type *> scalars,
+                                    bool asReturnValue) const override {
+    return occupiesMoreThan(CGT, scalars, /*total*/ 4);
+  }
+
+  bool isSwiftErrorInRegister() const override {
+    return true;
+  }
+
 private:
   ABIArgInfo classify(QualType Ty, unsigned &FreeSSERegs, bool IsReturnType,
                       bool IsVectorCall, bool IsRegCall) const;
@@ -2472,10 +2464,7 @@
 class X86_64TargetCodeGenInfo : public TargetCodeGenInfo {
 public:
   X86_64TargetCodeGenInfo(CodeGen::CodeGenTypes &CGT, X86AVXABILevel AVXLevel)
-      : TargetCodeGenInfo(std::make_unique<X86_64ABIInfo>(CGT, AVXLevel)) {
-    SwiftInfo =
-        std::make_unique<SwiftABIInfo>(CGT, /*SwiftErrorInRegister=*/true);
-  }
+      : TargetCodeGenInfo(std::make_unique<X86_64ABIInfo>(CGT, AVXLevel)) {}
 
   const X86_64ABIInfo &getABIInfo() const {
     return static_cast<const X86_64ABIInfo&>(TargetCodeGenInfo::getABIInfo());
@@ -2619,7 +2608,7 @@
   llvm::StringMap<bool> CalleeMap;
   unsigned ArgIndex = 0;
 
-  // We need to loop through the actual call arguments rather than the
+  // We need to loop through the actual call arguments rather than the the
   // function's parameters, in case this variadic.
   for (const CallArg &Arg : Args) {
     // The "avx" feature changes how vectors >128 in size are passed. "avx512f"
@@ -2717,10 +2706,7 @@
 public:
   WinX86_64TargetCodeGenInfo(CodeGen::CodeGenTypes &CGT,
                              X86AVXABILevel AVXLevel)
-      : TargetCodeGenInfo(std::make_unique<WinX86_64ABIInfo>(CGT, AVXLevel)) {
-    SwiftInfo =
-        std::make_unique<SwiftABIInfo>(CGT, /*SwiftErrorInRegister=*/true);
-  }
+      : TargetCodeGenInfo(std::make_unique<WinX86_64ABIInfo>(CGT, AVXLevel)) {}
 
   void setTargetAttributes(const Decl *D, llvm::GlobalValue *GV,
                            CodeGen::CodeGenModule &CGM) const override;
@@ -2843,8 +2829,8 @@
   return SSE;
 }
 
-void X86_64ABIInfo::classify(QualType Ty, uint64_t OffsetBase, Class &Lo,
-                             Class &Hi, bool isNamedArg, bool IsRegCall) const {
+void X86_64ABIInfo::classify(QualType Ty, uint64_t OffsetBase,
+                             Class &Lo, Class &Hi, bool isNamedArg) const {
   // FIXME: This code can be simplified by introducing a simple value class for
   // Class pairs with appropriate constructor methods for the various
   // situations.
@@ -2869,7 +2855,7 @@
     } else if (k >= BuiltinType::Bool && k <= BuiltinType::LongLong) {
       Current = Integer;
     } else if (k == BuiltinType::Float || k == BuiltinType::Double ||
-               k == BuiltinType::Float16 || k == BuiltinType::BFloat16) {
+               k == BuiltinType::Float16) {
       Current = SSE;
     } else if (k == BuiltinType::LongDouble) {
       const llvm::fltSemantics *LDF = &getTarget().getLongDoubleFormat();
@@ -3000,8 +2986,7 @@
         Current = Integer;
       else if (Size <= 128)
         Lo = Hi = Integer;
-    } else if (ET->isFloat16Type() || ET == getContext().FloatTy ||
-               ET->isBFloat16Type()) {
+    } else if (ET->isFloat16Type() || ET == getContext().FloatTy) {
       Current = SSE;
     } else if (ET == getContext().DoubleTy) {
       Lo = Hi = SSE;
@@ -3043,9 +3028,7 @@
 
     // AMD64-ABI 3.2.3p2: Rule 1. If the size of an object is larger
     // than eight eightbytes, ..., it has class MEMORY.
-    // regcall ABI doesn't have limitation to an object. The only limitation
-    // is the free registers, which will be checked in computeInfo.
-    if (!IsRegCall && Size > 512)
+    if (Size > 512)
       return;
 
     // AMD64-ABI 3.2.3p2: Rule 1. If ..., or it contains unaligned
@@ -3138,7 +3121,7 @@
     unsigned idx = 0;
     bool UseClang11Compat = getContext().getLangOpts().getClangABICompat() <=
                                 LangOptions::ClangABI::Ver11 ||
-                            getContext().getTargetInfo().getTriple().isPS();
+                            getContext().getTargetInfo().getTriple().isPS4();
     bool IsUnion = RT->isUnionType() && !UseClang11Compat;
 
     for (RecordDecl::field_iterator i = RD->field_begin(), e = RD->field_end();
@@ -3473,9 +3456,9 @@
   if (SourceSize > T0Size)
       T1 = getFPTypeAtOffset(IRType, IROffset + T0Size, TD);
   if (T1 == nullptr) {
-    // Check if IRType is a half/bfloat + float. float type will be in IROffset+4 due
+    // Check if IRType is a half + float. float type will be in IROffset+4 due
     // to its alignment.
-    if (T0->is16bitFPTy() && SourceSize > 4)
+    if (T0->isHalfTy() && SourceSize > 4)
       T1 = getFPTypeAtOffset(IRType, IROffset + 4, TD);
     // If we can't get a second FP type, return a simple half or float.
     // avx512fp16-abi.c:pr51813_2 shows it works to return float for
@@ -3487,7 +3470,7 @@
   if (T0->isFloatTy() && T1->isFloatTy())
     return llvm::FixedVectorType::get(T0, 2);
 
-  if (T0->is16bitFPTy() && T1->is16bitFPTy()) {
+  if (T0->isHalfTy() && T1->isHalfTy()) {
     llvm::Type *T2 = nullptr;
     if (SourceSize > 4)
       T2 = getFPTypeAtOffset(IRType, IROffset + 4, TD);
@@ -3496,7 +3479,7 @@
     return llvm::FixedVectorType::get(T0, 4);
   }
 
-  if (T0->is16bitFPTy() || T1->is16bitFPTy())
+  if (T0->isHalfTy() || T1->isHalfTy())
     return llvm::FixedVectorType::get(llvm::Type::getHalfTy(getVMContext()), 4);
 
   return llvm::Type::getDoubleTy(getVMContext());
@@ -3752,14 +3735,15 @@
   return ABIArgInfo::getDirect(ResType);
 }
 
-ABIArgInfo
-X86_64ABIInfo::classifyArgumentType(QualType Ty, unsigned freeIntRegs,
-                                    unsigned &neededInt, unsigned &neededSSE,
-                                    bool isNamedArg, bool IsRegCall) const {
+ABIArgInfo X86_64ABIInfo::classifyArgumentType(
+  QualType Ty, unsigned freeIntRegs, unsigned &neededInt, unsigned &neededSSE,
+  bool isNamedArg)
+  const
+{
   Ty = useFirstFieldIfTransparentUnion(Ty);
 
   X86_64ABIInfo::Class Lo, Hi;
-  classify(Ty, 0, Lo, Hi, isNamedArg, IsRegCall);
+  classify(Ty, 0, Lo, Hi, isNamedArg);
 
   // Check some invariants.
   // FIXME: Enforce these by construction.
@@ -3882,8 +3866,7 @@
 
 ABIArgInfo
 X86_64ABIInfo::classifyRegCallStructTypeImpl(QualType Ty, unsigned &NeededInt,
-                                             unsigned &NeededSSE,
-                                             unsigned &MaxVectorWidth) const {
+                                             unsigned &NeededSSE) const {
   auto RT = Ty->getAs<RecordType>();
   assert(RT && "classifyRegCallStructType only valid with struct types");
 
@@ -3898,8 +3881,7 @@
     }
 
     for (const auto &I : CXXRD->bases())
-      if (classifyRegCallStructTypeImpl(I.getType(), NeededInt, NeededSSE,
-                                        MaxVectorWidth)
+      if (classifyRegCallStructTypeImpl(I.getType(), NeededInt, NeededSSE)
               .isIndirect()) {
         NeededInt = NeededSSE = 0;
         return getIndirectReturnResult(Ty);
@@ -3908,27 +3890,20 @@
 
   // Sum up members
   for (const auto *FD : RT->getDecl()->fields()) {
-    QualType MTy = FD->getType();
-    if (MTy->isRecordType() && !MTy->isUnionType()) {
-      if (classifyRegCallStructTypeImpl(MTy, NeededInt, NeededSSE,
-                                        MaxVectorWidth)
+    if (FD->getType()->isRecordType() && !FD->getType()->isUnionType()) {
+      if (classifyRegCallStructTypeImpl(FD->getType(), NeededInt, NeededSSE)
               .isIndirect()) {
         NeededInt = NeededSSE = 0;
         return getIndirectReturnResult(Ty);
       }
     } else {
       unsigned LocalNeededInt, LocalNeededSSE;
-      if (classifyArgumentType(MTy, UINT_MAX, LocalNeededInt, LocalNeededSSE,
-                               true, true)
+      if (classifyArgumentType(FD->getType(), UINT_MAX, LocalNeededInt,
+                               LocalNeededSSE, true)
               .isIndirect()) {
         NeededInt = NeededSSE = 0;
         return getIndirectReturnResult(Ty);
       }
-      if (const auto *AT = getContext().getAsConstantArrayType(MTy))
-        MTy = AT->getElementType();
-      if (const auto *VT = MTy->getAs<VectorType>())
-        if (getContext().getTypeSize(VT) > MaxVectorWidth)
-          MaxVectorWidth = getContext().getTypeSize(VT);
       NeededInt += LocalNeededInt;
       NeededSSE += LocalNeededSSE;
     }
@@ -3937,17 +3912,14 @@
   return ABIArgInfo::getDirect();
 }
 
-ABIArgInfo
-X86_64ABIInfo::classifyRegCallStructType(QualType Ty, unsigned &NeededInt,
-                                         unsigned &NeededSSE,
-                                         unsigned &MaxVectorWidth) const {
+ABIArgInfo X86_64ABIInfo::classifyRegCallStructType(QualType Ty,
+                                                    unsigned &NeededInt,
+                                                    unsigned &NeededSSE) const {
 
   NeededInt = 0;
   NeededSSE = 0;
-  MaxVectorWidth = 0;
 
-  return classifyRegCallStructTypeImpl(Ty, NeededInt, NeededSSE,
-                                       MaxVectorWidth);
+  return classifyRegCallStructTypeImpl(Ty, NeededInt, NeededSSE);
 }
 
 void X86_64ABIInfo::computeInfo(CGFunctionInfo &FI) const {
@@ -3967,13 +3939,13 @@
   // Keep track of the number of assigned registers.
   unsigned FreeIntRegs = IsRegCall ? 11 : 6;
   unsigned FreeSSERegs = IsRegCall ? 16 : 8;
-  unsigned NeededInt = 0, NeededSSE = 0, MaxVectorWidth = 0;
+  unsigned NeededInt, NeededSSE;
 
   if (!::classifyReturnType(getCXXABI(), FI, *this)) {
     if (IsRegCall && FI.getReturnType()->getTypePtr()->isRecordType() &&
         !FI.getReturnType()->getTypePtr()->isUnionType()) {
-      FI.getReturnInfo() = classifyRegCallStructType(
-          FI.getReturnType(), NeededInt, NeededSSE, MaxVectorWidth);
+      FI.getReturnInfo() =
+          classifyRegCallStructType(FI.getReturnType(), NeededInt, NeededSSE);
       if (FreeIntRegs >= NeededInt && FreeSSERegs >= NeededSSE) {
         FreeIntRegs -= NeededInt;
         FreeSSERegs -= NeededSSE;
@@ -3996,8 +3968,6 @@
   // integer register.
   if (FI.getReturnInfo().isIndirect())
     --FreeIntRegs;
-  else if (NeededSSE && MaxVectorWidth > 0)
-    FI.setMaxVectorWidth(MaxVectorWidth);
 
   // The chain argument effectively gives us another free register.
   if (FI.isChainCall())
@@ -4012,8 +3982,7 @@
     bool IsNamedArg = ArgNo < NumRequiredArgs;
 
     if (IsRegCall && it->type->isStructureOrClassType())
-      it->info = classifyRegCallStructType(it->type, NeededInt, NeededSSE,
-                                           MaxVectorWidth);
+      it->info = classifyRegCallStructType(it->type, NeededInt, NeededSSE);
     else
       it->info = classifyArgumentType(it->type, FreeIntRegs, NeededInt,
                                       NeededSSE, IsNamedArg);
@@ -4025,8 +3994,6 @@
     if (FreeIntRegs >= NeededInt && FreeSSERegs >= NeededSSE) {
       FreeIntRegs -= NeededInt;
       FreeSSERegs -= NeededSSE;
-      if (MaxVectorWidth > FI.getMaxVectorWidth())
-        FI.setMaxVectorWidth(MaxVectorWidth);
     } else {
       it->info = getIndirectResult(it->type, FreeIntRegs);
     }
@@ -4589,7 +4556,7 @@
     Ty = EnumTy->getDecl()->getIntegerType();
 
   // Promotable integer types are required to be promoted by the ABI.
-  if (getContext().isPromotableIntegerType(Ty))
+  if (Ty->isPromotableIntegerType())
     return true;
 
   if (!Is64Bit)
@@ -4862,7 +4829,7 @@
 
   Builder.CreateCondBr(CC, UsingRegs, UsingOverflow);
 
-  llvm::Type *DirectTy = CGF.ConvertType(Ty), *ElementTy = DirectTy;
+  llvm::Type *DirectTy = CGF.ConvertType(Ty);
   if (isIndirect) DirectTy = DirectTy->getPointerTo(0);
 
   // Case 1: consume registers.
@@ -4871,7 +4838,7 @@
     CGF.EmitBlock(UsingRegs);
 
     Address RegSaveAreaPtr = Builder.CreateStructGEP(VAList, 4);
-    RegAddr = Address(Builder.CreateLoad(RegSaveAreaPtr), CGF.Int8Ty,
+    RegAddr = Address(Builder.CreateLoad(RegSaveAreaPtr),
                       CharUnits::fromQuantity(8));
     assert(RegAddr.getElementType() == CGF.Int8Ty);
 
@@ -4885,10 +4852,10 @@
     // registers we've used by the number of
     CharUnits RegSize = CharUnits::fromQuantity((isInt || IsSoftFloatABI) ? 4 : 8);
     llvm::Value *RegOffset =
-        Builder.CreateMul(NumRegs, Builder.getInt8(RegSize.getQuantity()));
-    RegAddr = Address(
-        Builder.CreateInBoundsGEP(CGF.Int8Ty, RegAddr.getPointer(), RegOffset),
-        CGF.Int8Ty, RegAddr.getAlignment().alignmentOfArrayElement(RegSize));
+      Builder.CreateMul(NumRegs, Builder.getInt8(RegSize.getQuantity()));
+    RegAddr = Address(Builder.CreateInBoundsGEP(CGF.Int8Ty,
+                                            RegAddr.getPointer(), RegOffset),
+                      RegAddr.getAlignment().alignmentOfArrayElement(RegSize));
     RegAddr = Builder.CreateElementBitCast(RegAddr, DirectTy);
 
     // Increase the used-register count.
@@ -4919,15 +4886,14 @@
     }
 
     Address OverflowAreaAddr = Builder.CreateStructGEP(VAList, 3);
-    Address OverflowArea =
-        Address(Builder.CreateLoad(OverflowAreaAddr, "argp.cur"), CGF.Int8Ty,
-                OverflowAreaAlign);
+    Address OverflowArea(Builder.CreateLoad(OverflowAreaAddr, "argp.cur"),
+                         OverflowAreaAlign);
     // Round up address of argument to alignment
     CharUnits Align = CGF.getContext().getTypeAlignInChars(Ty);
     if (Align > OverflowAreaAlign) {
       llvm::Value *Ptr = OverflowArea.getPointer();
       OverflowArea = Address(emitRoundPointerUpToAlignment(CGF, Ptr, Align),
-                             OverflowArea.getElementType(), Align);
+                                                           Align);
     }
 
     MemAddr = Builder.CreateElementBitCast(OverflowArea, DirectTy);
@@ -4946,7 +4912,7 @@
 
   // Load the pointer if the argument was passed indirectly.
   if (isIndirect) {
-    Result = Address(Builder.CreateLoad(Result, "aggr"), ElementTy,
+    Result = Address(Builder.CreateLoad(Result, "aggr"),
                      getContext().getTypeAlignInChars(Ty));
   }
 
@@ -4983,7 +4949,7 @@
 
 namespace {
 /// PPC64_SVR4_ABIInfo - The 64-bit PowerPC ELF (SVR4) ABI information.
-class PPC64_SVR4_ABIInfo : public ABIInfo {
+class PPC64_SVR4_ABIInfo : public SwiftABIInfo {
 public:
   enum ABIKind {
     ELFv1 = 0,
@@ -4998,7 +4964,7 @@
 public:
   PPC64_SVR4_ABIInfo(CodeGen::CodeGenTypes &CGT, ABIKind Kind,
                      bool SoftFloatABI)
-      : ABIInfo(CGT), Kind(Kind), IsSoftFloatABI(SoftFloatABI) {}
+      : SwiftABIInfo(CGT), Kind(Kind), IsSoftFloatABI(SoftFloatABI) {}
 
   bool isPromotableTypeForABI(QualType Ty) const;
   CharUnits getParamTypeAlignment(QualType Ty) const;
@@ -5039,6 +5005,15 @@
 
   Address EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
                     QualType Ty) const override;
+
+  bool shouldPassIndirectlyForSwift(ArrayRef<llvm::Type*> scalars,
+                                    bool asReturnValue) const override {
+    return occupiesMoreThan(CGT, scalars, /*total*/ 4);
+  }
+
+  bool isSwiftErrorInRegister() const override {
+    return false;
+  }
 };
 
 class PPC64_SVR4_TargetCodeGenInfo : public TargetCodeGenInfo {
@@ -5048,10 +5023,7 @@
                                PPC64_SVR4_ABIInfo::ABIKind Kind,
                                bool SoftFloatABI)
       : TargetCodeGenInfo(
-            std::make_unique<PPC64_SVR4_ABIInfo>(CGT, Kind, SoftFloatABI)) {
-    SwiftInfo =
-        std::make_unique<SwiftABIInfo>(CGT, /*SwiftErrorInRegister=*/false);
-  }
+            std::make_unique<PPC64_SVR4_ABIInfo>(CGT, Kind, SoftFloatABI)) {}
 
   int getDwarfEHStackPointer(CodeGen::CodeGenModule &M) const override {
     // This is recovered from gcc output.
@@ -5214,7 +5186,8 @@
       if (isEmptyRecord(getContext(), FT, true))
         continue;
 
-      if (isZeroLengthBitfieldPermittedInHomogeneousAggregate() &&
+      // For compatibility with GCC, ignore empty bitfields in C++ mode.
+      if (getContext().getLangOpts().CPlusPlus &&
           FD->isZeroLengthBitField(getContext()))
         continue;
 
@@ -5460,21 +5433,8 @@
   }
 
   // Otherwise, just use the general rule.
-  //
-  // The PPC64 ABI passes some arguments in integer registers, even to variadic
-  // functions. To allow va_list to use the simple "void*" representation,
-  // variadic calls allocate space in the argument area for the integer argument
-  // registers, and variadic functions spill their integer argument registers to
-  // this area in their prologues. When aggregates smaller than a register are
-  // passed this way, they are passed in the least significant bits of the
-  // register, which means that after spilling on big-endian targets they will
-  // be right-aligned in their argument slot. This is uncommon; for a variety of
-  // reasons, other big-endian targets don't end up right-aligning aggregate
-  // types this way, and so right-alignment only applies to fundamental types.
-  // So on PPC64, we must force the use of right-alignment even for aggregates.
-  return emitVoidPtrVAArg(CGF, VAListAddr, Ty, /*Indirect*/ false, TypeInfo,
-                          SlotSize, /*AllowHigher*/ true,
-                          /*ForceRightAdjust*/ true);
+  return emitVoidPtrVAArg(CGF, VAListAddr, Ty, /*Indirect*/ false,
+                          TypeInfo, SlotSize, /*AllowHigher*/ true);
 }
 
 bool
@@ -5498,7 +5458,7 @@
 
 namespace {
 
-class AArch64ABIInfo : public ABIInfo {
+class AArch64ABIInfo : public SwiftABIInfo {
 public:
   enum ABIKind {
     AAPCS = 0,
@@ -5510,7 +5470,8 @@
   ABIKind Kind;
 
 public:
-  AArch64ABIInfo(CodeGenTypes &CGT, ABIKind Kind) : ABIInfo(CGT), Kind(Kind) {}
+  AArch64ABIInfo(CodeGenTypes &CGT, ABIKind Kind)
+    : SwiftABIInfo(CGT), Kind(Kind) {}
 
 private:
   ABIKind getABIKind() const { return Kind; }
@@ -5523,7 +5484,6 @@
   bool isHomogeneousAggregateBaseType(QualType Ty) const override;
   bool isHomogeneousAggregateSmallEnough(const Type *Ty,
                                          uint64_t Members) const override;
-  bool isZeroLengthBitfieldPermittedInHomogeneousAggregate() const override;
 
   bool isIllegalVectorType(QualType Ty) const;
 
@@ -5558,26 +5518,26 @@
   Address EmitMSVAArg(CodeGenFunction &CGF, Address VAListAddr,
                       QualType Ty) const override;
 
+  bool shouldPassIndirectlyForSwift(ArrayRef<llvm::Type*> scalars,
+                                    bool asReturnValue) const override {
+    return occupiesMoreThan(CGT, scalars, /*total*/ 4);
+  }
+  bool isSwiftErrorInRegister() const override {
+    return true;
+  }
+
+  bool isLegalVectorTypeForSwift(CharUnits totalSize, llvm::Type *eltTy,
+                                 unsigned elts) const override;
+
   bool allowBFloatArgsAndRet() const override {
     return getTarget().hasBFloat16Type();
   }
 };
 
-class AArch64SwiftABIInfo : public SwiftABIInfo {
-public:
-  explicit AArch64SwiftABIInfo(CodeGenTypes &CGT)
-      : SwiftABIInfo(CGT, /*SwiftErrorInRegister=*/true) {}
-
-  bool isLegalVectorType(CharUnits VectorSize, llvm::Type *EltTy,
-                         unsigned NumElts) const override;
-};
-
 class AArch64TargetCodeGenInfo : public TargetCodeGenInfo {
 public:
   AArch64TargetCodeGenInfo(CodeGenTypes &CGT, AArch64ABIInfo::ABIKind Kind)
-      : TargetCodeGenInfo(std::make_unique<AArch64ABIInfo>(CGT, Kind)) {
-    SwiftInfo = std::make_unique<AArch64SwiftABIInfo>(CGT);
-  }
+      : TargetCodeGenInfo(std::make_unique<AArch64ABIInfo>(CGT, Kind)) {}
 
   StringRef getARCRetainAutoreleasedReturnValueMarker() const override {
     return "mov\tfp, fp\t\t// marker for objc_retainAutoreleaseReturnValue";
@@ -5599,15 +5559,14 @@
     if (TA == nullptr)
       return;
 
-    ParsedTargetAttr Attr =
-        CGM.getTarget().parseTargetAttr(TA->getFeaturesStr());
+    ParsedTargetAttr Attr = TA->parse();
     if (Attr.BranchProtection.empty())
       return;
 
     TargetInfo::BranchProtectionInfo BPI;
     StringRef Error;
-    (void)CGM.getTarget().validateBranchProtection(Attr.BranchProtection,
-                                                   Attr.CPU, BPI, Error);
+    (void)CGM.getTarget().validateBranchProtection(
+        Attr.BranchProtection, Attr.Architecture, BPI, Error);
     assert(Error.empty());
 
     auto *Fn = cast<llvm::Function>(GV);
@@ -5832,9 +5791,8 @@
       Alignment = getContext().getTypeUnadjustedAlign(Ty);
       Alignment = Alignment < 128 ? 64 : 128;
     } else {
-      Alignment =
-          std::max(getContext().getTypeAlign(Ty),
-                   (unsigned)getTarget().getPointerWidth(LangAS::Default));
+      Alignment = std::max(getContext().getTypeAlign(Ty),
+                           (unsigned)getTarget().getPointerWidth(0));
     }
     Size = llvm::alignTo(Size, Alignment);
 
@@ -5953,13 +5911,13 @@
   return false;
 }
 
-bool AArch64SwiftABIInfo::isLegalVectorType(CharUnits VectorSize,
-                                            llvm::Type *EltTy,
-                                            unsigned NumElts) const {
-  if (!llvm::isPowerOf2_32(NumElts))
+bool AArch64ABIInfo::isLegalVectorTypeForSwift(CharUnits totalSize,
+                                               llvm::Type *eltTy,
+                                               unsigned elts) const {
+  if (!llvm::isPowerOf2_32(elts))
     return false;
-  if (VectorSize.getQuantity() != 8 &&
-      (VectorSize.getQuantity() != 16 || NumElts == 1))
+  if (totalSize.getQuantity() != 8 &&
+      (totalSize.getQuantity() != 16 || elts == 1))
     return false;
   return true;
 }
@@ -5985,30 +5943,10 @@
   return Members <= 4;
 }
 
-bool AArch64ABIInfo::isZeroLengthBitfieldPermittedInHomogeneousAggregate()
-    const {
-  // AAPCS64 says that the rule for whether something is a homogeneous
-  // aggregate is applied to the output of the data layout decision. So
-  // anything that doesn't affect the data layout also does not affect
-  // homogeneity. In particular, zero-length bitfields don't stop a struct
-  // being homogeneous.
-  return true;
-}
-
 Address AArch64ABIInfo::EmitAAPCSVAArg(Address VAListAddr, QualType Ty,
                                        CodeGenFunction &CGF) const {
   ABIArgInfo AI = classifyArgumentType(Ty, /*IsVariadic=*/true,
                                        CGF.CurFnInfo->getCallingConvention());
-  // Empty records are ignored for parameter passing purposes.
-  if (AI.isIgnore()) {
-    uint64_t PointerSize = getTarget().getPointerWidth(LangAS::Default) / 8;
-    CharUnits SlotSize = CharUnits::fromQuantity(PointerSize);
-    VAListAddr = CGF.Builder.CreateElementBitCast(VAListAddr, CGF.Int8PtrTy);
-    auto *Load = CGF.Builder.CreateLoad(VAListAddr);
-    Address Addr = Address(Load, CGF.Int8Ty, SlotSize);
-    return CGF.Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(Ty));
-  }
-
   bool IsIndirect = AI.isIndirect();
 
   llvm::Type *BaseTy = CGF.ConvertType(Ty);
@@ -6123,9 +6061,9 @@
       CGF.Builder.CreateStructGEP(VAListAddr, reg_top_index, "reg_top_p");
   reg_top = CGF.Builder.CreateLoad(reg_top_p, "reg_top");
   Address BaseAddr(CGF.Builder.CreateInBoundsGEP(CGF.Int8Ty, reg_top, reg_offs),
-                   CGF.Int8Ty, CharUnits::fromQuantity(IsFPR ? 16 : 8));
+                   CharUnits::fromQuantity(IsFPR ? 16 : 8));
   Address RegAddr = Address::invalid();
-  llvm::Type *MemTy = CGF.ConvertTypeForMem(Ty), *ElementTy = MemTy;
+  llvm::Type *MemTy = CGF.ConvertTypeForMem(Ty);
 
   if (IsIndirect) {
     // If it's been passed indirectly (actually a struct), whatever we find from
@@ -6208,8 +6146,8 @@
 
     OnStackPtr = CGF.Builder.CreateIntToPtr(OnStackPtr, CGF.Int8PtrTy);
   }
-  Address OnStackAddr = Address(OnStackPtr, CGF.Int8Ty,
-                                std::max(CharUnits::fromQuantity(8), TyAlign));
+  Address OnStackAddr(OnStackPtr,
+                      std::max(CharUnits::fromQuantity(8), TyAlign));
 
   // All stack slots are multiples of 8 bytes.
   CharUnits StackSlotSize = CharUnits::fromQuantity(8);
@@ -6241,11 +6179,11 @@
   //=======================================
   CGF.EmitBlock(ContBlock);
 
-  Address ResAddr = emitMergePHI(CGF, RegAddr, InRegBlock, OnStackAddr,
-                                 OnStackBlock, "vaargs.addr");
+  Address ResAddr = emitMergePHI(CGF, RegAddr, InRegBlock,
+                                 OnStackAddr, OnStackBlock, "vaargs.addr");
 
   if (IsIndirect)
-    return Address(CGF.Builder.CreateLoad(ResAddr, "vaarg.addr"), ElementTy,
+    return Address(CGF.Builder.CreateLoad(ResAddr, "vaarg.addr"),
                    TyAlign);
 
   return ResAddr;
@@ -6259,13 +6197,12 @@
   if (!isAggregateTypeForABI(Ty) && !isIllegalVectorType(Ty))
     return EmitVAArgInstr(CGF, VAListAddr, Ty, ABIArgInfo::getDirect());
 
-  uint64_t PointerSize = getTarget().getPointerWidth(LangAS::Default) / 8;
+  uint64_t PointerSize = getTarget().getPointerWidth(0) / 8;
   CharUnits SlotSize = CharUnits::fromQuantity(PointerSize);
 
   // Empty records are ignored for parameter passing purposes.
   if (isEmptyRecord(getContext(), Ty, true)) {
-    Address Addr = Address(CGF.Builder.CreateLoad(VAListAddr, "ap.cur"),
-                           getVAListElementType(CGF), SlotSize);
+    Address Addr(CGF.Builder.CreateLoad(VAListAddr, "ap.cur"), SlotSize);
     Addr = CGF.Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(Ty));
     return Addr;
   }
@@ -6307,7 +6244,7 @@
 
 namespace {
 
-class ARMABIInfo : public ABIInfo {
+class ARMABIInfo : public SwiftABIInfo {
 public:
   enum ABIKind {
     APCS = 0,
@@ -6321,7 +6258,8 @@
   bool IsFloatABISoftFP;
 
 public:
-  ARMABIInfo(CodeGenTypes &CGT, ABIKind Kind) : ABIInfo(CGT), Kind(Kind) {
+  ARMABIInfo(CodeGenTypes &CGT, ABIKind _Kind)
+      : SwiftABIInfo(CGT), Kind(_Kind) {
     setCCs();
     IsFloatABISoftFP = CGT.getCodeGenOpts().FloatABI == "softfp" ||
         CGT.getCodeGenOpts().FloatABI == ""; // default
@@ -6373,7 +6311,6 @@
   bool isHomogeneousAggregateBaseType(QualType Ty) const override;
   bool isHomogeneousAggregateSmallEnough(const Type *Ty,
                                          uint64_t Members) const override;
-  bool isZeroLengthBitfieldPermittedInHomogeneousAggregate() const override;
 
   bool isEffectivelyAAPCS_VFP(unsigned callConvention, bool acceptHalf) const;
 
@@ -6385,23 +6322,22 @@
   llvm::CallingConv::ID getLLVMDefaultCC() const;
   llvm::CallingConv::ID getABIDefaultCC() const;
   void setCCs();
-};
 
-class ARMSwiftABIInfo : public SwiftABIInfo {
-public:
-  explicit ARMSwiftABIInfo(CodeGenTypes &CGT)
-      : SwiftABIInfo(CGT, /*SwiftErrorInRegister=*/true) {}
-
-  bool isLegalVectorType(CharUnits VectorSize, llvm::Type *EltTy,
-                         unsigned NumElts) const override;
+  bool shouldPassIndirectlyForSwift(ArrayRef<llvm::Type*> scalars,
+                                    bool asReturnValue) const override {
+    return occupiesMoreThan(CGT, scalars, /*total*/ 4);
+  }
+  bool isSwiftErrorInRegister() const override {
+    return true;
+  }
+  bool isLegalVectorTypeForSwift(CharUnits totalSize, llvm::Type *eltTy,
+                                 unsigned elts) const override;
 };
 
 class ARMTargetCodeGenInfo : public TargetCodeGenInfo {
 public:
   ARMTargetCodeGenInfo(CodeGenTypes &CGT, ARMABIInfo::ABIKind K)
-      : TargetCodeGenInfo(std::make_unique<ARMABIInfo>(CGT, K)) {
-    SwiftInfo = std::make_unique<ARMSwiftABIInfo>(CGT);
-  }
+      : TargetCodeGenInfo(std::make_unique<ARMABIInfo>(CGT, K)) {}
 
   const ARMABIInfo &getABIInfo() const {
     return static_cast<const ARMABIInfo&>(TargetCodeGenInfo::getABIInfo());
@@ -6439,13 +6375,13 @@
     auto *Fn = cast<llvm::Function>(GV);
 
     if (const auto *TA = FD->getAttr<TargetAttr>()) {
-      ParsedTargetAttr Attr =
-          CGM.getTarget().parseTargetAttr(TA->getFeaturesStr());
+      ParsedTargetAttr Attr = TA->parse();
       if (!Attr.BranchProtection.empty()) {
         TargetInfo::BranchProtectionInfo BPI;
         StringRef DiagMsg;
-        StringRef Arch =
-            Attr.CPU.empty() ? CGM.getTarget().getTargetOpts().CPU : Attr.CPU;
+        StringRef Arch = Attr.Architecture.empty()
+                             ? CGM.getTarget().getTargetOpts().CPU
+                             : Attr.Architecture;
         if (!CGM.getTarget().validateBranchProtection(Attr.BranchProtection,
                                                       Arch, BPI, DiagMsg)) {
           CGM.getDiags().Report(
@@ -6468,11 +6404,11 @@
         // If the Branch Protection attribute is missing, validate the target
         // Architecture attribute against Branch Protection command line
         // settings.
-        if (!CGM.getTarget().isBranchProtectionSupportedArch(Attr.CPU))
+        if (!CGM.getTarget().isBranchProtectionSupportedArch(Attr.Architecture))
           CGM.getDiags().Report(
               D->getLocation(),
               diag::warn_target_unsupported_branch_protection_attribute)
-              << Attr.CPU;
+              << Attr.Architecture;
       }
     }
 
@@ -6707,7 +6643,7 @@
   if (getABIKind() == ARMABIInfo::AAPCS_VFP ||
       getABIKind() == ARMABIInfo::AAPCS) {
     TyAlign = getContext().getTypeUnadjustedAlignInChars(Ty).getQuantity();
-    ABIAlign = std::clamp(TyAlign, (uint64_t)4, (uint64_t)8);
+    ABIAlign = std::min(std::max(TyAlign, (uint64_t)4), (uint64_t)8);
   } else {
     TyAlign = getContext().getTypeAlignInChars(Ty).getQuantity();
   }
@@ -7003,15 +6939,16 @@
   }
 }
 
-bool ARMSwiftABIInfo::isLegalVectorType(CharUnits VectorSize, llvm::Type *EltTy,
-                                        unsigned NumElts) const {
-  if (!llvm::isPowerOf2_32(NumElts))
+bool ARMABIInfo::isLegalVectorTypeForSwift(CharUnits vectorSize,
+                                           llvm::Type *eltTy,
+                                           unsigned numElts) const {
+  if (!llvm::isPowerOf2_32(numElts))
     return false;
-  unsigned size = CGT.getDataLayout().getTypeStoreSizeInBits(EltTy);
+  unsigned size = getDataLayout().getTypeStoreSizeInBits(eltTy);
   if (size > 64)
     return false;
-  if (VectorSize.getQuantity() != 8 &&
-      (VectorSize.getQuantity() != 16 || NumElts == 1))
+  if (vectorSize.getQuantity() != 8 &&
+      (vectorSize.getQuantity() != 16 || numElts == 1))
     return false;
   return true;
 }
@@ -7037,15 +6974,6 @@
   return Members <= 4;
 }
 
-bool ARMABIInfo::isZeroLengthBitfieldPermittedInHomogeneousAggregate() const {
-  // AAPCS32 says that the rule for whether something is a homogeneous
-  // aggregate is applied to the output of the data layout decision. So
-  // anything that doesn't affect the data layout also does not affect
-  // homogeneity. In particular, zero-length bitfields don't stop a struct
-  // being homogeneous.
-  return true;
-}
-
 bool ARMABIInfo::isEffectivelyAAPCS_VFP(unsigned callConvention,
                                         bool acceptHalf) const {
   // Give precedence to user-specified calling conventions.
@@ -7062,10 +6990,9 @@
 
   // Empty records are ignored for parameter passing purposes.
   if (isEmptyRecord(getContext(), Ty, true)) {
-    VAListAddr = CGF.Builder.CreateElementBitCast(VAListAddr, CGF.Int8PtrTy);
-    auto *Load = CGF.Builder.CreateLoad(VAListAddr);
-    Address Addr = Address(Load, CGF.Int8Ty, SlotSize);
-    return CGF.Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(Ty));
+    Address Addr(CGF.Builder.CreateLoad(VAListAddr), SlotSize);
+    Addr = CGF.Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(Ty));
+    return Addr;
   }
 
   CharUnits TySize = getContext().getTypeSizeInChars(Ty);
@@ -7396,13 +7323,13 @@
 
 namespace {
 
-class SystemZABIInfo : public ABIInfo {
+class SystemZABIInfo : public SwiftABIInfo {
   bool HasVector;
   bool IsSoftFloatABI;
 
 public:
   SystemZABIInfo(CodeGenTypes &CGT, bool HV, bool SF)
-      : ABIInfo(CGT), HasVector(HV), IsSoftFloatABI(SF) {}
+    : SwiftABIInfo(CGT), HasVector(HV), IsSoftFloatABI(SF) {}
 
   bool isPromotableIntegerTypeForABI(QualType Ty) const;
   bool isCompoundType(QualType Ty) const;
@@ -7413,59 +7340,31 @@
   ABIArgInfo classifyReturnType(QualType RetTy) const;
   ABIArgInfo classifyArgumentType(QualType ArgTy) const;
 
-  void computeInfo(CGFunctionInfo &FI) const override;
+  void computeInfo(CGFunctionInfo &FI) const override {
+    if (!getCXXABI().classifyReturnType(FI))
+      FI.getReturnInfo() = classifyReturnType(FI.getReturnType());
+    for (auto &I : FI.arguments())
+      I.info = classifyArgumentType(I.type);
+  }
+
   Address EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
                     QualType Ty) const override;
+
+  bool shouldPassIndirectlyForSwift(ArrayRef<llvm::Type*> scalars,
+                                    bool asReturnValue) const override {
+    return occupiesMoreThan(CGT, scalars, /*total*/ 4);
+  }
+  bool isSwiftErrorInRegister() const override {
+    return false;
+  }
 };
 
 class SystemZTargetCodeGenInfo : public TargetCodeGenInfo {
-  // These are used for speeding up the search for a visible vector ABI.
-  mutable bool HasVisibleVecABIFlag = false;
-  mutable std::set<const Type *> SeenTypes;
-
-  // Returns true (the first time) if Ty is or found to make use of a vector
-  // type (e.g. as a function argument).
-  bool isVectorTypeBased(const Type *Ty) const;
-
 public:
   SystemZTargetCodeGenInfo(CodeGenTypes &CGT, bool HasVector, bool SoftFloatABI)
       : TargetCodeGenInfo(
-            std::make_unique<SystemZABIInfo>(CGT, HasVector, SoftFloatABI)) {
-    SwiftInfo =
-        std::make_unique<SwiftABIInfo>(CGT, /*SwiftErrorInRegister=*/false);
-  }
+            std::make_unique<SystemZABIInfo>(CGT, HasVector, SoftFloatABI)) {}
 
-  // The vector ABI is different when the vector facility is present and when
-  // a module e.g. defines an externally visible vector variable, a flag
-  // indicating a visible vector ABI is added. Eventually this will result in
-  // a GNU attribute indicating the vector ABI of the module.  Ty is the type
-  // of a variable or function parameter that is globally visible.
-  void handleExternallyVisibleObjABI(const Type *Ty,
-                                     CodeGen::CodeGenModule &M) const {
-    if (!HasVisibleVecABIFlag && isVectorTypeBased(Ty)) {
-      M.getModule().addModuleFlag(llvm::Module::Warning,
-                                  "s390x-visible-vector-ABI", 1);
-      HasVisibleVecABIFlag = true;
-    }
-  }
-
-  void setTargetAttributes(const Decl *D, llvm::GlobalValue *GV,
-                           CodeGen::CodeGenModule &M) const override {
-    if (!D)
-      return;
-
-    // Check if the vector ABI becomes visible by an externally visible
-    // variable or function.
-    if (const auto *VD = dyn_cast<VarDecl>(D)) {
-      if (VD->isExternallyVisible())
-        handleExternallyVisibleObjABI(VD->getType().getTypePtr(), M);
-    }
-    else if (const FunctionDecl *FD = dyn_cast<FunctionDecl>(D)) {
-      if (FD->isExternallyVisible())
-        handleExternallyVisibleObjABI(FD->getType().getTypePtr(), M);
-    }
-  }
-
   llvm::Value *testFPKind(llvm::Value *V, unsigned BuiltinID,
                           CGBuilderTy &Builder,
                           CodeGenModule &CGM) const override {
@@ -7586,9 +7485,12 @@
 
     // Check the fields.
     for (const auto *FD : RD->fields()) {
+      // For compatibility with GCC, ignore empty bitfields in C++ mode.
       // Unlike isSingleElementStruct(), empty structure and array fields
       // do count.  So do anonymous bitfields that aren't zero-sized.
-
+      if (getContext().getLangOpts().CPlusPlus &&
+          FD->isZeroLengthBitField(getContext()))
+        continue;
       // Like isSingleElementStruct(), ignore C++20 empty data members.
       if (FD->hasAttr<NoUniqueAddressAttr>() &&
           isEmptyRecord(getContext(), FD->getType(), true))
@@ -7623,9 +7525,6 @@
   // Every non-vector argument occupies 8 bytes and is passed by preference
   // in either GPRs or FPRs.  Vector arguments occupy 8 or 16 bytes and are
   // always passed on the stack.
-  const SystemZTargetCodeGenInfo &SZCGI =
-      static_cast<const SystemZTargetCodeGenInfo &>(
-          CGT.getCGM().getTargetCodeGenInfo());
   Ty = getContext().getCanonicalType(Ty);
   auto TyInfo = getContext().getTypeInfoInChars(Ty);
   llvm::Type *ArgTy = CGF.ConvertTypeForMem(Ty);
@@ -7636,7 +7535,6 @@
   bool IsVector = false;
   CharUnits UnpaddedSize;
   CharUnits DirectAlign;
-  SZCGI.handleExternallyVisibleObjABI(Ty.getTypePtr(), CGT.getCGM());
   if (IsIndirect) {
     DirectTy = llvm::PointerType::getUnqual(DirectTy);
     UnpaddedSize = DirectAlign = CharUnits::fromQuantity(8);
@@ -7666,15 +7564,16 @@
     Address OverflowArgAreaPtr =
         CGF.Builder.CreateStructGEP(VAListAddr, 2, "overflow_arg_area_ptr");
     Address OverflowArgArea =
-        Address(CGF.Builder.CreateLoad(OverflowArgAreaPtr, "overflow_arg_area"),
-                CGF.Int8Ty, TyInfo.Align);
+      Address(CGF.Builder.CreateLoad(OverflowArgAreaPtr, "overflow_arg_area"),
+              TyInfo.Align);
     Address MemAddr =
-        CGF.Builder.CreateElementBitCast(OverflowArgArea, DirectTy, "mem_addr");
+      CGF.Builder.CreateElementBitCast(OverflowArgArea, DirectTy, "mem_addr");
 
     // Update overflow_arg_area_ptr pointer
-    llvm::Value *NewOverflowArgArea = CGF.Builder.CreateGEP(
-        OverflowArgArea.getElementType(), OverflowArgArea.getPointer(),
-        PaddedSizeV, "overflow_arg_area");
+    llvm::Value *NewOverflowArgArea =
+      CGF.Builder.CreateGEP(OverflowArgArea.getElementType(),
+                            OverflowArgArea.getPointer(), PaddedSizeV,
+                            "overflow_arg_area");
     CGF.Builder.CreateStore(NewOverflowArgArea, OverflowArgAreaPtr);
 
     return MemAddr;
@@ -7722,12 +7621,12 @@
   Address RegSaveAreaPtr =
       CGF.Builder.CreateStructGEP(VAListAddr, 3, "reg_save_area_ptr");
   llvm::Value *RegSaveArea =
-      CGF.Builder.CreateLoad(RegSaveAreaPtr, "reg_save_area");
-  Address RawRegAddr(
-      CGF.Builder.CreateGEP(CGF.Int8Ty, RegSaveArea, RegOffset, "raw_reg_addr"),
-      CGF.Int8Ty, PaddedSize);
+    CGF.Builder.CreateLoad(RegSaveAreaPtr, "reg_save_area");
+  Address RawRegAddr(CGF.Builder.CreateGEP(CGF.Int8Ty, RegSaveArea, RegOffset,
+                                           "raw_reg_addr"),
+                     PaddedSize);
   Address RegAddr =
-      CGF.Builder.CreateElementBitCast(RawRegAddr, DirectTy, "reg_addr");
+    CGF.Builder.CreateElementBitCast(RawRegAddr, DirectTy, "reg_addr");
 
   // Update the register count
   llvm::Value *One = llvm::ConstantInt::get(IndexTy, 1);
@@ -7743,10 +7642,10 @@
   Address OverflowArgAreaPtr =
       CGF.Builder.CreateStructGEP(VAListAddr, 2, "overflow_arg_area_ptr");
   Address OverflowArgArea =
-      Address(CGF.Builder.CreateLoad(OverflowArgAreaPtr, "overflow_arg_area"),
-              CGF.Int8Ty, PaddedSize);
+    Address(CGF.Builder.CreateLoad(OverflowArgAreaPtr, "overflow_arg_area"),
+            PaddedSize);
   Address RawMemAddr =
-      CGF.Builder.CreateConstByteGEP(OverflowArgArea, Padding, "raw_mem_addr");
+    CGF.Builder.CreateConstByteGEP(OverflowArgArea, Padding, "raw_mem_addr");
   Address MemAddr =
     CGF.Builder.CreateElementBitCast(RawMemAddr, DirectTy, "mem_addr");
 
@@ -7760,11 +7659,11 @@
 
   // Return the appropriate result.
   CGF.EmitBlock(ContBlock);
-  Address ResAddr = emitMergePHI(CGF, RegAddr, InRegBlock, MemAddr, InMemBlock,
-                                 "va_arg.addr");
+  Address ResAddr = emitMergePHI(CGF, RegAddr, InRegBlock,
+                                 MemAddr, InMemBlock, "va_arg.addr");
 
   if (IsIndirect)
-    ResAddr = Address(CGF.Builder.CreateLoad(ResAddr, "indirect_arg"), ArgTy,
+    ResAddr = Address(CGF.Builder.CreateLoad(ResAddr, "indirect_arg"),
                       TyInfo.Align);
 
   return ResAddr;
@@ -7831,51 +7730,6 @@
   return ABIArgInfo::getDirect(nullptr);
 }
 
-void SystemZABIInfo::computeInfo(CGFunctionInfo &FI) const {
-  const SystemZTargetCodeGenInfo &SZCGI =
-      static_cast<const SystemZTargetCodeGenInfo &>(
-          CGT.getCGM().getTargetCodeGenInfo());
-  if (!getCXXABI().classifyReturnType(FI))
-    FI.getReturnInfo() = classifyReturnType(FI.getReturnType());
-  unsigned Idx = 0;
-  for (auto &I : FI.arguments()) {
-    I.info = classifyArgumentType(I.type);
-    if (FI.isVariadic() && Idx++ >= FI.getNumRequiredArgs())
-      // Check if a vararg vector argument is passed, in which case the
-      // vector ABI becomes visible as the va_list could be passed on to
-      // other functions.
-      SZCGI.handleExternallyVisibleObjABI(I.type.getTypePtr(), CGT.getCGM());
-  }
-}
-
-bool SystemZTargetCodeGenInfo::isVectorTypeBased(const Type *Ty) const {
-  while (Ty->isPointerType() || Ty->isArrayType())
-    Ty = Ty->getPointeeOrArrayElementType();
-  if (!SeenTypes.insert(Ty).second)
-    return false;
-  if (Ty->isVectorType())
-    return true;
-  if (const auto *RecordTy = Ty->getAs<RecordType>()) {
-    const RecordDecl *RD = RecordTy->getDecl();
-    if (const CXXRecordDecl *CXXRD = dyn_cast<CXXRecordDecl>(RD))
-      if (CXXRD->hasDefinition())
-        for (const auto &I : CXXRD->bases())
-          if (isVectorTypeBased(I.getType().getTypePtr()))
-            return true;
-    for (const auto *FD : RD->fields())
-      if (isVectorTypeBased(FD->getType().getTypePtr()))
-        return true;
-  }
-  if (const auto *FT = Ty->getAs<FunctionType>())
-    if (isVectorTypeBased(FT->getReturnType().getTypePtr()))
-      return true;
-  if (const FunctionProtoType *Proto = Ty->getAs<FunctionProtoType>())
-    for (auto ParamType : Proto->getParamTypes())
-      if (isVectorTypeBased(ParamType.getTypePtr()))
-        return true;
-  return false;
-}
-
 //===----------------------------------------------------------------------===//
 // MSP430 ABI Implementation
 //===----------------------------------------------------------------------===//
@@ -7960,7 +7814,7 @@
 namespace {
 class MipsABIInfo : public ABIInfo {
   bool IsO32;
-  const unsigned MinABIStackAlignInBytes, StackAlignInBytes;
+  unsigned MinABIStackAlignInBytes, StackAlignInBytes;
   void CoerceToIntArgs(uint64_t TySize,
                        SmallVectorImpl<llvm::Type *> &ArgList) const;
   llvm::Type* HandleAggregates(QualType Ty, uint64_t TySize) const;
@@ -8137,8 +7991,8 @@
   uint64_t TySize = getContext().getTypeSize(Ty);
   uint64_t Align = getContext().getTypeAlign(Ty) / 8;
 
-  Align = std::clamp(Align, (uint64_t)MinABIStackAlignInBytes,
-                     (uint64_t)StackAlignInBytes);
+  Align = std::min(std::max(Align, (uint64_t)MinABIStackAlignInBytes),
+                   (uint64_t)StackAlignInBytes);
   unsigned CurrOffset = llvm::alignTo(Offset, Align);
   Offset = CurrOffset + llvm::alignTo(TySize, Align * 8) / 8;
 
@@ -8293,7 +8147,7 @@
   // Integer arguments are promoted to 32-bit on O32 and 64-bit on N32/N64.
   // Pointers are also promoted in the same way but this only matters for N32.
   unsigned SlotSizeInBits = IsO32 ? 32 : 64;
-  unsigned PtrWidth = getTarget().getPointerWidth(LangAS::Default);
+  unsigned PtrWidth = getTarget().getPointerWidth(0);
   bool DidPromote = false;
   if ((Ty->isIntegerType() &&
           getContext().getIntWidth(Ty) < SlotSizeInBits) ||
@@ -8433,23 +8287,18 @@
       : DefaultABIInfo(CGT), ParamRegs(NPR), RetRegs(NRR) {}
 
   ABIArgInfo classifyReturnType(QualType Ty, bool &LargeRet) const {
-    // On AVR, a return struct with size less than or equals to 8 bytes is
-    // returned directly via registers R18-R25. On AVRTiny, a return struct
-    // with size less than or equals to 4 bytes is returned directly via
-    // registers R22-R25.
-    if (isAggregateTypeForABI(Ty) &&
-        getContext().getTypeSize(Ty) <= RetRegs * 8)
-      return ABIArgInfo::getDirect();
-    // A return value (struct or scalar) with larger size is returned via a
-    // stack slot, along with a pointer as the function's implicit argument.
-    if (getContext().getTypeSize(Ty) > RetRegs * 8) {
+    if (isAggregateTypeForABI(Ty)) {
+      // On AVR, a return struct with size less than or equals to 8 bytes is
+      // returned directly via registers R18-R25. On AVRTiny, a return struct
+      // with size less than or equals to 4 bytes is returned directly via
+      // registers R22-R25.
+      if (getContext().getTypeSize(Ty) <= RetRegs * 8)
+        return ABIArgInfo::getDirect();
+      // A return struct with larger size is returned via a stack
+      // slot, along with a pointer to it as the function's implicit argument.
       LargeRet = true;
       return getNaturalAlignIndirect(Ty);
     }
-    // An i8 return value should not be extended to i16, since AVR has 8-bit
-    // registers.
-    if (Ty->isIntegralOrEnumerationType() && getContext().getTypeSize(Ty) <= 8)
-      return ABIArgInfo::getDirect();
     // Otherwise we follow the default way which is compatible.
     return DefaultABIInfo::classifyReturnType(Ty);
   }
@@ -8813,10 +8662,9 @@
   // Get the type of the argument from memory and bitcast
   // overflow area pointer to the argument type.
   llvm::Type *PTy = CGF.ConvertTypeForMem(Ty);
-  Address AddrTyped = CGF.Builder.CreateElementBitCast(
-      Address(__overflow_area_pointer, CGF.Int8Ty,
-              CharUnits::fromQuantity(Align)),
-      PTy);
+  Address AddrTyped = CGF.Builder.CreateBitCast(
+      Address(__overflow_area_pointer, CharUnits::fromQuantity(Align)),
+      llvm::PointerType::getUnqual(PTy));
 
   // Round up to the minimum stack alignment for varargs which is 4 bytes.
   uint64_t Offset = llvm::alignTo(CGF.getContext().getTypeSize(Ty) / 8, 4);
@@ -8835,8 +8683,9 @@
                                             QualType Ty) const {
   // FIXME: Need to handle alignment
   llvm::Type *BP = CGF.Int8PtrTy;
+  llvm::Type *BPP = CGF.Int8PtrPtrTy;
   CGBuilderTy &Builder = CGF.Builder;
-  Address VAListAddrAsBPP = Builder.CreateElementBitCast(VAListAddr, BP, "ap");
+  Address VAListAddrAsBPP = Builder.CreateBitCast(VAListAddr, BPP, "ap");
   llvm::Value *Addr = Builder.CreateLoad(VAListAddrAsBPP, "ap.cur");
   // Handle address alignment for type alignment > 32 bits
   uint64_t TyAlign = CGF.getContext().getTypeAlign(Ty) / 8;
@@ -8847,9 +8696,9 @@
     AddrAsInt = Builder.CreateAnd(AddrAsInt, Builder.getInt32(~(TyAlign - 1)));
     Addr = Builder.CreateIntToPtr(AddrAsInt, BP);
   }
-  Address AddrTyped = Builder.CreateElementBitCast(
-      Address(Addr, CGF.Int8Ty, CharUnits::fromQuantity(TyAlign)),
-      CGF.ConvertType(Ty));
+  llvm::Type *PTy = llvm::PointerType::getUnqual(CGF.ConvertType(Ty));
+  Address AddrTyped = Builder.CreateBitCast(
+      Address(Addr, CharUnits::fromQuantity(TyAlign)), PTy);
 
   uint64_t Offset = llvm::alignTo(CGF.getContext().getTypeSize(Ty) / 8, 4);
   llvm::Value *NextAddr = Builder.CreateGEP(
@@ -9002,13 +8851,12 @@
   // Implement the ContBlock
   CGF.EmitBlock(ContBlock);
 
-  llvm::Type *MemTy = CGF.ConvertTypeForMem(Ty);
-  llvm::Type *MemPTy = llvm::PointerType::getUnqual(MemTy);
+  llvm::Type *MemPTy = llvm::PointerType::getUnqual(CGF.ConvertTypeForMem(Ty));
   llvm::PHINode *ArgAddr = CGF.Builder.CreatePHI(MemPTy, 2, "vaarg.addr");
   ArgAddr->addIncoming(__saved_reg_area_p, InRegBlock);
   ArgAddr->addIncoming(__overflow_area_p, OnStackBlock);
 
-  return Address(ArgAddr, MemTy, CharUnits::fromQuantity(ArgAlign));
+  return Address(ArgAddr, CharUnits::fromQuantity(ArgAlign));
 }
 
 Address HexagonABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
@@ -9428,7 +9276,7 @@
   llvm::Function *
   createEnqueuedBlockKernel(CodeGenFunction &CGF,
                             llvm::Function *BlockInvokeFunc,
-                            llvm::Type *BlockTy) const override;
+                            llvm::Value *BlockLiteral) const override;
   bool shouldEmitStaticExternCAliases() const override;
   void setCUDAKernelCallingConvention(const FunctionType *&FT) const override;
 };
@@ -9543,12 +9391,8 @@
 
   const bool IsHIPKernel =
       M.getLangOpts().HIP && FD && FD->hasAttr<CUDAGlobalAttr>();
-  const bool IsOpenMPkernel =
-      M.getLangOpts().OpenMPIsDevice &&
-      (F->getCallingConv() == llvm::CallingConv::AMDGPU_KERNEL);
 
-  // TODO: This should be moved to language specific attributes instead.
-  if (IsHIPKernel || IsOpenMPkernel)
+  if (IsHIPKernel)
     F->addFnAttr("uniform-work-group-size", "true");
 
   if (M.getContext().getTargetInfo().allowAMDGPUUnsafeFPAtomics())
@@ -9600,7 +9444,7 @@
   if (CGM.isTypeConstant(D->getType(), false) &&
       D->hasConstantInitialization()) {
     if (auto ConstAS = CGM.getTarget().getConstantAddressSpace())
-      return *ConstAS;
+      return ConstAS.getValue();
   }
   return DefaultGlobalAS;
 }
@@ -9925,8 +9769,7 @@
   CharUnits SlotSize = CharUnits::fromQuantity(8);
 
   CGBuilderTy &Builder = CGF.Builder;
-  Address Addr = Address(Builder.CreateLoad(VAListAddr, "ap.cur"),
-                         getVAListElementType(CGF), SlotSize);
+  Address Addr(Builder.CreateLoad(VAListAddr, "ap.cur"), SlotSize);
   llvm::Type *ArgPtrTy = llvm::PointerType::getUnqual(ArgTy);
 
   auto TypeInfo = getContext().getTypeInfoInChars(Ty);
@@ -9957,19 +9800,19 @@
   case ABIArgInfo::IndirectAliased:
     Stride = SlotSize;
     ArgAddr = Builder.CreateElementBitCast(Addr, ArgPtrTy, "indirect");
-    ArgAddr = Address(Builder.CreateLoad(ArgAddr, "indirect.arg"), ArgTy,
+    ArgAddr = Address(Builder.CreateLoad(ArgAddr, "indirect.arg"),
                       TypeInfo.Align);
     break;
 
   case ABIArgInfo::Ignore:
-    return Address(llvm::UndefValue::get(ArgPtrTy), ArgTy, TypeInfo.Align);
+    return Address(llvm::UndefValue::get(ArgPtrTy), TypeInfo.Align);
   }
 
   // Update VAList.
   Address NextPtr = Builder.CreateConstInBoundsByteGEP(Addr, Stride, "ap.next");
   Builder.CreateStore(NextPtr.getPointer(), VAListAddr);
 
-  return Builder.CreateElementBitCast(ArgAddr, ArgTy, "arg.addr");
+  return Builder.CreateBitCast(ArgAddr, ArgPtrTy, "arg.addr");
 }
 
 void SparcV9ABIInfo::computeInfo(CGFunctionInfo &FI) const {
@@ -10303,8 +10146,7 @@
 
   // Get the VAList.
   CharUnits SlotSize = CharUnits::fromQuantity(4);
-  Address AP = Address(Builder.CreateLoad(VAListAddr),
-                       getVAListElementType(CGF), SlotSize);
+  Address AP(Builder.CreateLoad(VAListAddr), SlotSize);
 
   // Handle the argument.
   ABIArgInfo AI = classifyArgumentType(Ty);
@@ -10322,20 +10164,20 @@
   case ABIArgInfo::InAlloca:
     llvm_unreachable("Unsupported ABI kind for va_arg");
   case ABIArgInfo::Ignore:
-    Val = Address(llvm::UndefValue::get(ArgPtrTy), ArgTy, TypeAlign);
+    Val = Address(llvm::UndefValue::get(ArgPtrTy), TypeAlign);
     ArgSize = CharUnits::Zero();
     break;
   case ABIArgInfo::Extend:
   case ABIArgInfo::Direct:
-    Val = Builder.CreateElementBitCast(AP, ArgTy);
+    Val = Builder.CreateBitCast(AP, ArgPtrTy);
     ArgSize = CharUnits::fromQuantity(
-        getDataLayout().getTypeAllocSize(AI.getCoerceToType()));
+                       getDataLayout().getTypeAllocSize(AI.getCoerceToType()));
     ArgSize = ArgSize.alignTo(SlotSize);
     break;
   case ABIArgInfo::Indirect:
   case ABIArgInfo::IndirectAliased:
     Val = Builder.CreateElementBitCast(AP, ArgPtrTy);
-    Val = Address(Builder.CreateLoad(Val), ArgTy, TypeAlign);
+    Val = Address(Builder.CreateLoad(Val), TypeAlign);
     ArgSize = SlotSize;
     break;
   }
@@ -10404,7 +10246,7 @@
 void TypeStringCache::addIfComplete(const IdentifierInfo *ID, StringRef Str,
                                     bool IsRecursive) {
   if (!ID || IncompleteUsedCount)
-    return; // No key or it is an incomplete sub-type so don't add.
+    return; // No key or it is is an incomplete sub-type so don't add.
   Entry &E = Map[ID];
   if (IsRecursive && !E.Str.empty()) {
     assert(E.State==Recursive && E.Str.size() == Str.size() &&
@@ -10539,10 +10381,10 @@
 }
 
 ABIArgInfo SPIRVABIInfo::classifyKernelArgumentType(QualType Ty) const {
-  if (getContext().getLangOpts().CUDAIsDevice) {
+  if (getContext().getLangOpts().HIP) {
     // Coerce pointer arguments with default address space to CrossWorkGroup
-    // pointers for HIPSPV/CUDASPV. When the language mode is HIP/CUDA, the
-    // SPIRTargetInfo maps cuda_device to SPIR-V's CrossWorkGroup address space.
+    // pointers for HIPSPV. When the language mode is HIP, the SPIRTargetInfo
+    // maps cuda_device to SPIR-V's CrossWorkGroup address space.
     llvm::Type *LTy = CGT.ConvertType(Ty);
     auto DefaultAS = getContext().getTargetAddressSpace(LangAS::Default);
     auto GlobalAS = getContext().getTargetAddressSpace(LangAS::cuda_device);
@@ -10551,15 +10393,6 @@
       LTy = llvm::PointerType::getWithSamePointeeType(PtrTy, GlobalAS);
       return ABIArgInfo::getDirect(LTy, 0, nullptr, false);
     }
-
-    // Force copying aggregate type in kernel arguments by value when
-    // compiling CUDA targeting SPIR-V. This is required for the object
-    // copied to be valid on the device.
-    // This behavior follows the CUDA spec
-    // https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#global-function-argument-processing,
-    // and matches the NVPTX implementation.
-    if (isAggregateTypeForABI(Ty))
-      return getNaturalAlignIndirect(Ty, /* byval */ true);
   }
   return classifyArgumentType(Ty);
 }
@@ -11103,22 +10936,9 @@
     // Unions aren't eligible unless they're empty (which is caught above).
     if (RD->isUnion())
       return false;
-    const ASTRecordLayout &Layout = getContext().getASTRecordLayout(RD);
-    // If this is a C++ record, check the bases first.
-    if (const CXXRecordDecl *CXXRD = dyn_cast<CXXRecordDecl>(RD)) {
-      for (const CXXBaseSpecifier &B : CXXRD->bases()) {
-        const auto *BDecl =
-            cast<CXXRecordDecl>(B.getType()->castAs<RecordType>()->getDecl());
-        CharUnits BaseOff = Layout.getBaseClassOffset(BDecl);
-        bool Ret = detectFPCCEligibleStructHelper(B.getType(), CurOff + BaseOff,
-                                                  Field1Ty, Field1Off, Field2Ty,
-                                                  Field2Off);
-        if (!Ret)
-          return false;
-      }
-    }
     int ZeroWidthBitFieldCount = 0;
     for (const FieldDecl *FD : RD->fields()) {
+      const ASTRecordLayout &Layout = getContext().getASTRecordLayout(RD);
       uint64_t FieldOffInBits = Layout.getFieldOffset(FD->getFieldIndex());
       QualType QTy = FD->getType();
       if (FD->isBitField()) {
@@ -11371,8 +11191,7 @@
 
   // Empty records are ignored for parameter passing purposes.
   if (isEmptyRecord(getContext(), Ty, true)) {
-    Address Addr = Address(CGF.Builder.CreateLoad(VAListAddr),
-                           getVAListElementType(CGF), SlotSize);
+    Address Addr(CGF.Builder.CreateLoad(VAListAddr), SlotSize);
     Addr = CGF.Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(Ty));
     return Addr;
   }
@@ -11478,683 +11297,6 @@
 } // end anonymous namespace
 
 //===----------------------------------------------------------------------===//
-// CSKY ABI Implementation
-//===----------------------------------------------------------------------===//
-namespace {
-class CSKYABIInfo : public DefaultABIInfo {
-  static const int NumArgGPRs = 4;
-  static const int NumArgFPRs = 4;
-
-  static const unsigned XLen = 32;
-  unsigned FLen;
-
-public:
-  CSKYABIInfo(CodeGen::CodeGenTypes &CGT, unsigned FLen)
-      : DefaultABIInfo(CGT), FLen(FLen) {}
-
-  void computeInfo(CGFunctionInfo &FI) const override;
-  ABIArgInfo classifyArgumentType(QualType Ty, int &ArgGPRsLeft,
-                                  int &ArgFPRsLeft,
-                                  bool isReturnType = false) const;
-  ABIArgInfo classifyReturnType(QualType RetTy) const;
-
-  Address EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
-                    QualType Ty) const override;
-};
-
-} // end anonymous namespace
-
-void CSKYABIInfo::computeInfo(CGFunctionInfo &FI) const {
-  QualType RetTy = FI.getReturnType();
-  if (!getCXXABI().classifyReturnType(FI))
-    FI.getReturnInfo() = classifyReturnType(RetTy);
-
-  bool IsRetIndirect = FI.getReturnInfo().getKind() == ABIArgInfo::Indirect;
-
-  // We must track the number of GPRs used in order to conform to the CSKY
-  // ABI, as integer scalars passed in registers should have signext/zeroext
-  // when promoted.
-  int ArgGPRsLeft = IsRetIndirect ? NumArgGPRs - 1 : NumArgGPRs;
-  int ArgFPRsLeft = FLen ? NumArgFPRs : 0;
-
-  for (auto &ArgInfo : FI.arguments()) {
-    ArgInfo.info = classifyArgumentType(ArgInfo.type, ArgGPRsLeft, ArgFPRsLeft);
-  }
-}
-
-Address CSKYABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
-                               QualType Ty) const {
-  CharUnits SlotSize = CharUnits::fromQuantity(XLen / 8);
-
-  // Empty records are ignored for parameter passing purposes.
-  if (isEmptyRecord(getContext(), Ty, true)) {
-    Address Addr = Address(CGF.Builder.CreateLoad(VAListAddr),
-                           getVAListElementType(CGF), SlotSize);
-    Addr = CGF.Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(Ty));
-    return Addr;
-  }
-
-  auto TInfo = getContext().getTypeInfoInChars(Ty);
-
-  return emitVoidPtrVAArg(CGF, VAListAddr, Ty, false, TInfo, SlotSize,
-                          /*AllowHigherAlign=*/true);
-}
-
-ABIArgInfo CSKYABIInfo::classifyArgumentType(QualType Ty, int &ArgGPRsLeft,
-                                             int &ArgFPRsLeft,
-                                             bool isReturnType) const {
-  assert(ArgGPRsLeft <= NumArgGPRs && "Arg GPR tracking underflow");
-  Ty = useFirstFieldIfTransparentUnion(Ty);
-
-  // Structures with either a non-trivial destructor or a non-trivial
-  // copy constructor are always passed indirectly.
-  if (CGCXXABI::RecordArgABI RAA = getRecordArgABI(Ty, getCXXABI())) {
-    if (ArgGPRsLeft)
-      ArgGPRsLeft -= 1;
-    return getNaturalAlignIndirect(Ty, /*ByVal=*/RAA ==
-                                           CGCXXABI::RAA_DirectInMemory);
-  }
-
-  // Ignore empty structs/unions.
-  if (isEmptyRecord(getContext(), Ty, true))
-    return ABIArgInfo::getIgnore();
-
-  if (!Ty->getAsUnionType())
-    if (const Type *SeltTy = isSingleElementStruct(Ty, getContext()))
-      return ABIArgInfo::getDirect(CGT.ConvertType(QualType(SeltTy, 0)));
-
-  uint64_t Size = getContext().getTypeSize(Ty);
-  // Pass floating point values via FPRs if possible.
-  if (Ty->isFloatingType() && !Ty->isComplexType() && FLen >= Size &&
-      ArgFPRsLeft) {
-    ArgFPRsLeft--;
-    return ABIArgInfo::getDirect();
-  }
-
-  // Complex types for the hard float ABI must be passed direct rather than
-  // using CoerceAndExpand.
-  if (Ty->isComplexType() && FLen && !isReturnType) {
-    QualType EltTy = Ty->castAs<ComplexType>()->getElementType();
-    if (getContext().getTypeSize(EltTy) <= FLen) {
-      ArgFPRsLeft -= 2;
-      return ABIArgInfo::getDirect();
-    }
-  }
-
-  if (!isAggregateTypeForABI(Ty)) {
-    // Treat an enum type as its underlying type.
-    if (const EnumType *EnumTy = Ty->getAs<EnumType>())
-      Ty = EnumTy->getDecl()->getIntegerType();
-
-    // All integral types are promoted to XLen width, unless passed on the
-    // stack.
-    if (Size < XLen && Ty->isIntegralOrEnumerationType())
-      return ABIArgInfo::getExtend(Ty);
-
-    if (const auto *EIT = Ty->getAs<BitIntType>()) {
-      if (EIT->getNumBits() < XLen)
-        return ABIArgInfo::getExtend(Ty);
-    }
-
-    return ABIArgInfo::getDirect();
-  }
-
-  // For argument type, the first 4*XLen parts of aggregate will be passed
-  // in registers, and the rest will be passed in stack.
-  // So we can coerce to integers directly and let backend handle it correctly.
-  // For return type, aggregate which <= 2*XLen will be returned in registers.
-  // Otherwise, aggregate will be returned indirectly.
-  if (!isReturnType || (isReturnType && Size <= 2 * XLen)) {
-    if (Size <= XLen) {
-      return ABIArgInfo::getDirect(
-          llvm::IntegerType::get(getVMContext(), XLen));
-    } else {
-      return ABIArgInfo::getDirect(llvm::ArrayType::get(
-          llvm::IntegerType::get(getVMContext(), XLen), (Size + 31) / XLen));
-    }
-  }
-  return getNaturalAlignIndirect(Ty, /*ByVal=*/false);
-}
-
-ABIArgInfo CSKYABIInfo::classifyReturnType(QualType RetTy) const {
-  if (RetTy->isVoidType())
-    return ABIArgInfo::getIgnore();
-
-  int ArgGPRsLeft = 2;
-  int ArgFPRsLeft = FLen ? 1 : 0;
-
-  // The rules for return and argument types are the same, so defer to
-  // classifyArgumentType.
-  return classifyArgumentType(RetTy, ArgGPRsLeft, ArgFPRsLeft, true);
-}
-
-namespace {
-class CSKYTargetCodeGenInfo : public TargetCodeGenInfo {
-public:
-  CSKYTargetCodeGenInfo(CodeGen::CodeGenTypes &CGT, unsigned FLen)
-      : TargetCodeGenInfo(std::make_unique<CSKYABIInfo>(CGT, FLen)) {}
-};
-} // end anonymous namespace
-
-//===----------------------------------------------------------------------===//
-// BPF ABI Implementation
-//===----------------------------------------------------------------------===//
-
-namespace {
-
-class BPFABIInfo : public DefaultABIInfo {
-public:
-  BPFABIInfo(CodeGenTypes &CGT) : DefaultABIInfo(CGT) {}
-
-  ABIArgInfo classifyArgumentType(QualType Ty) const {
-    Ty = useFirstFieldIfTransparentUnion(Ty);
-
-    if (isAggregateTypeForABI(Ty)) {
-      uint64_t Bits = getContext().getTypeSize(Ty);
-      if (Bits == 0)
-        return ABIArgInfo::getIgnore();
-
-      // If the aggregate needs 1 or 2 registers, do not use reference.
-      if (Bits <= 128) {
-        llvm::Type *CoerceTy;
-        if (Bits <= 64) {
-          CoerceTy =
-              llvm::IntegerType::get(getVMContext(), llvm::alignTo(Bits, 8));
-        } else {
-          llvm::Type *RegTy = llvm::IntegerType::get(getVMContext(), 64);
-          CoerceTy = llvm::ArrayType::get(RegTy, 2);
-        }
-        return ABIArgInfo::getDirect(CoerceTy);
-      } else {
-        return getNaturalAlignIndirect(Ty);
-      }
-    }
-
-    if (const EnumType *EnumTy = Ty->getAs<EnumType>())
-      Ty = EnumTy->getDecl()->getIntegerType();
-
-    ASTContext &Context = getContext();
-    if (const auto *EIT = Ty->getAs<BitIntType>())
-      if (EIT->getNumBits() > Context.getTypeSize(Context.Int128Ty))
-        return getNaturalAlignIndirect(Ty);
-
-    return (isPromotableIntegerTypeForABI(Ty) ? ABIArgInfo::getExtend(Ty)
-                                              : ABIArgInfo::getDirect());
-  }
-
-  ABIArgInfo classifyReturnType(QualType RetTy) const {
-    if (RetTy->isVoidType())
-      return ABIArgInfo::getIgnore();
-
-    if (isAggregateTypeForABI(RetTy))
-      return getNaturalAlignIndirect(RetTy);
-
-    // Treat an enum type as its underlying type.
-    if (const EnumType *EnumTy = RetTy->getAs<EnumType>())
-      RetTy = EnumTy->getDecl()->getIntegerType();
-
-    ASTContext &Context = getContext();
-    if (const auto *EIT = RetTy->getAs<BitIntType>())
-      if (EIT->getNumBits() > Context.getTypeSize(Context.Int128Ty))
-        return getNaturalAlignIndirect(RetTy);
-
-    // Caller will do necessary sign/zero extension.
-    return ABIArgInfo::getDirect();
-  }
-
-  void computeInfo(CGFunctionInfo &FI) const override {
-    FI.getReturnInfo() = classifyReturnType(FI.getReturnType());
-    for (auto &I : FI.arguments())
-      I.info = classifyArgumentType(I.type);
-  }
-
-};
-
-class BPFTargetCodeGenInfo : public TargetCodeGenInfo {
-public:
-  BPFTargetCodeGenInfo(CodeGenTypes &CGT)
-      : TargetCodeGenInfo(std::make_unique<BPFABIInfo>(CGT)) {}
-
-  const BPFABIInfo &getABIInfo() const {
-    return static_cast<const BPFABIInfo&>(TargetCodeGenInfo::getABIInfo());
-  }
-};
-
-}
-
-// LoongArch ABI Implementation. Documented at
-// https://loongson.github.io/LoongArch-Documentation/LoongArch-ELF-ABI-EN.html
-//
-//===----------------------------------------------------------------------===//
-
-namespace {
-class LoongArchABIInfo : public DefaultABIInfo {
-private:
-  // Size of the integer ('r') registers in bits.
-  unsigned GRLen;
-  // Size of the floating point ('f') registers in bits.
-  unsigned FRLen;
-  // Number of general-purpose argument registers.
-  static const int NumGARs = 8;
-  // Number of floating-point argument registers.
-  static const int NumFARs = 8;
-  bool detectFARsEligibleStructHelper(QualType Ty, CharUnits CurOff,
-                                      llvm::Type *&Field1Ty,
-                                      CharUnits &Field1Off,
-                                      llvm::Type *&Field2Ty,
-                                      CharUnits &Field2Off) const;
-
-public:
-  LoongArchABIInfo(CodeGen::CodeGenTypes &CGT, unsigned GRLen, unsigned FRLen)
-      : DefaultABIInfo(CGT), GRLen(GRLen), FRLen(FRLen) {}
-
-  void computeInfo(CGFunctionInfo &FI) const override;
-
-  ABIArgInfo classifyArgumentType(QualType Ty, bool IsFixed, int &GARsLeft,
-                                  int &FARsLeft) const;
-  ABIArgInfo classifyReturnType(QualType RetTy) const;
-
-  Address EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
-                    QualType Ty) const override;
-
-  ABIArgInfo extendType(QualType Ty) const;
-
-  bool detectFARsEligibleStruct(QualType Ty, llvm::Type *&Field1Ty,
-                                CharUnits &Field1Off, llvm::Type *&Field2Ty,
-                                CharUnits &Field2Off, int &NeededArgGPRs,
-                                int &NeededArgFPRs) const;
-  ABIArgInfo coerceAndExpandFARsEligibleStruct(llvm::Type *Field1Ty,
-                                               CharUnits Field1Off,
-                                               llvm::Type *Field2Ty,
-                                               CharUnits Field2Off) const;
-};
-} // end anonymous namespace
-
-void LoongArchABIInfo::computeInfo(CGFunctionInfo &FI) const {
-  QualType RetTy = FI.getReturnType();
-  if (!getCXXABI().classifyReturnType(FI))
-    FI.getReturnInfo() = classifyReturnType(RetTy);
-
-  // IsRetIndirect is true if classifyArgumentType indicated the value should
-  // be passed indirect, or if the type size is a scalar greater than 2*GRLen
-  // and not a complex type with elements <= FRLen. e.g. fp128 is passed direct
-  // in LLVM IR, relying on the backend lowering code to rewrite the argument
-  // list and pass indirectly on LA32.
-  bool IsRetIndirect = FI.getReturnInfo().getKind() == ABIArgInfo::Indirect;
-  if (!IsRetIndirect && RetTy->isScalarType() &&
-      getContext().getTypeSize(RetTy) > (2 * GRLen)) {
-    if (RetTy->isComplexType() && FRLen) {
-      QualType EltTy = RetTy->castAs<ComplexType>()->getElementType();
-      IsRetIndirect = getContext().getTypeSize(EltTy) > FRLen;
-    } else {
-      // This is a normal scalar > 2*GRLen, such as fp128 on LA32.
-      IsRetIndirect = true;
-    }
-  }
-
-  // We must track the number of GARs and FARs used in order to conform to the
-  // LoongArch ABI. As GAR usage is different for variadic arguments, we must
-  // also track whether we are examining a vararg or not.
-  int GARsLeft = IsRetIndirect ? NumGARs - 1 : NumGARs;
-  int FARsLeft = FRLen ? NumFARs : 0;
-  int NumFixedArgs = FI.getNumRequiredArgs();
-
-  int ArgNum = 0;
-  for (auto &ArgInfo : FI.arguments()) {
-    ArgInfo.info = classifyArgumentType(
-        ArgInfo.type, /*IsFixed=*/ArgNum < NumFixedArgs, GARsLeft, FARsLeft);
-    ArgNum++;
-  }
-}
-
-// Returns true if the struct is a potential candidate to be passed in FARs (and
-// GARs). If this function returns true, the caller is responsible for checking
-// that if there is only a single field then that field is a float.
-bool LoongArchABIInfo::detectFARsEligibleStructHelper(
-    QualType Ty, CharUnits CurOff, llvm::Type *&Field1Ty, CharUnits &Field1Off,
-    llvm::Type *&Field2Ty, CharUnits &Field2Off) const {
-  bool IsInt = Ty->isIntegralOrEnumerationType();
-  bool IsFloat = Ty->isRealFloatingType();
-
-  if (IsInt || IsFloat) {
-    uint64_t Size = getContext().getTypeSize(Ty);
-    if (IsInt && Size > GRLen)
-      return false;
-    // Can't be eligible if larger than the FP registers. Half precision isn't
-    // currently supported on LoongArch and the ABI hasn't been confirmed, so
-    // default to the integer ABI in that case.
-    if (IsFloat && (Size > FRLen || Size < 32))
-      return false;
-    // Can't be eligible if an integer type was already found (int+int pairs
-    // are not eligible).
-    if (IsInt && Field1Ty && Field1Ty->isIntegerTy())
-      return false;
-    if (!Field1Ty) {
-      Field1Ty = CGT.ConvertType(Ty);
-      Field1Off = CurOff;
-      return true;
-    }
-    if (!Field2Ty) {
-      Field2Ty = CGT.ConvertType(Ty);
-      Field2Off = CurOff;
-      return true;
-    }
-    return false;
-  }
-
-  if (auto CTy = Ty->getAs<ComplexType>()) {
-    if (Field1Ty)
-      return false;
-    QualType EltTy = CTy->getElementType();
-    if (getContext().getTypeSize(EltTy) > FRLen)
-      return false;
-    Field1Ty = CGT.ConvertType(EltTy);
-    Field1Off = CurOff;
-    Field2Ty = Field1Ty;
-    Field2Off = Field1Off + getContext().getTypeSizeInChars(EltTy);
-    return true;
-  }
-
-  if (const ConstantArrayType *ATy = getContext().getAsConstantArrayType(Ty)) {
-    uint64_t ArraySize = ATy->getSize().getZExtValue();
-    QualType EltTy = ATy->getElementType();
-    CharUnits EltSize = getContext().getTypeSizeInChars(EltTy);
-    for (uint64_t i = 0; i < ArraySize; ++i) {
-      if (!detectFARsEligibleStructHelper(EltTy, CurOff, Field1Ty, Field1Off,
-                                          Field2Ty, Field2Off))
-        return false;
-      CurOff += EltSize;
-    }
-    return true;
-  }
-
-  if (const auto *RTy = Ty->getAs<RecordType>()) {
-    // Structures with either a non-trivial destructor or a non-trivial
-    // copy constructor are not eligible for the FP calling convention.
-    if (getRecordArgABI(Ty, CGT.getCXXABI()))
-      return false;
-    if (isEmptyRecord(getContext(), Ty, true))
-      return true;
-    const RecordDecl *RD = RTy->getDecl();
-    // Unions aren't eligible unless they're empty (which is caught above).
-    if (RD->isUnion())
-      return false;
-    const ASTRecordLayout &Layout = getContext().getASTRecordLayout(RD);
-    // If this is a C++ record, check the bases first.
-    if (const CXXRecordDecl *CXXRD = dyn_cast<CXXRecordDecl>(RD)) {
-      for (const CXXBaseSpecifier &B : CXXRD->bases()) {
-        const auto *BDecl =
-            cast<CXXRecordDecl>(B.getType()->castAs<RecordType>()->getDecl());
-        if (!detectFARsEligibleStructHelper(
-                B.getType(), CurOff + Layout.getBaseClassOffset(BDecl),
-                Field1Ty, Field1Off, Field2Ty, Field2Off))
-          return false;
-      }
-    }
-    for (const FieldDecl *FD : RD->fields()) {
-      QualType QTy = FD->getType();
-      if (FD->isBitField()) {
-        unsigned BitWidth = FD->getBitWidthValue(getContext());
-        // Zero-width bitfields are ignored.
-        if (BitWidth == 0)
-          continue;
-        // Allow a bitfield with a type greater than GRLen as long as the
-        // bitwidth is GRLen or less.
-        if (getContext().getTypeSize(QTy) > GRLen && BitWidth <= GRLen) {
-          QTy = getContext().getIntTypeForBitwidth(GRLen, false);
-        }
-      }
-
-      if (!detectFARsEligibleStructHelper(
-              QTy,
-              CurOff + getContext().toCharUnitsFromBits(
-                           Layout.getFieldOffset(FD->getFieldIndex())),
-              Field1Ty, Field1Off, Field2Ty, Field2Off))
-        return false;
-    }
-    return Field1Ty != nullptr;
-  }
-
-  return false;
-}
-
-// Determine if a struct is eligible to be passed in FARs (and GARs) (i.e., when
-// flattened it contains a single fp value, fp+fp, or int+fp of appropriate
-// size). If so, NeededFARs and NeededGARs are incremented appropriately.
-bool LoongArchABIInfo::detectFARsEligibleStruct(
-    QualType Ty, llvm::Type *&Field1Ty, CharUnits &Field1Off,
-    llvm::Type *&Field2Ty, CharUnits &Field2Off, int &NeededGARs,
-    int &NeededFARs) const {
-  Field1Ty = nullptr;
-  Field2Ty = nullptr;
-  NeededGARs = 0;
-  NeededFARs = 0;
-  if (!detectFARsEligibleStructHelper(Ty, CharUnits::Zero(), Field1Ty,
-                                      Field1Off, Field2Ty, Field2Off))
-    return false;
-  // Not really a candidate if we have a single int but no float.
-  if (Field1Ty && !Field2Ty && !Field1Ty->isFloatingPointTy())
-    return false;
-  if (Field1Ty && Field1Ty->isFloatingPointTy())
-    NeededFARs++;
-  else if (Field1Ty)
-    NeededGARs++;
-  if (Field2Ty && Field2Ty->isFloatingPointTy())
-    NeededFARs++;
-  else if (Field2Ty)
-    NeededGARs++;
-  return true;
-}
-
-// Call getCoerceAndExpand for the two-element flattened struct described by
-// Field1Ty, Field1Off, Field2Ty, Field2Off. This method will create an
-// appropriate coerceToType and unpaddedCoerceToType.
-ABIArgInfo LoongArchABIInfo::coerceAndExpandFARsEligibleStruct(
-    llvm::Type *Field1Ty, CharUnits Field1Off, llvm::Type *Field2Ty,
-    CharUnits Field2Off) const {
-  SmallVector<llvm::Type *, 3> CoerceElts;
-  SmallVector<llvm::Type *, 2> UnpaddedCoerceElts;
-  if (!Field1Off.isZero())
-    CoerceElts.push_back(llvm::ArrayType::get(
-        llvm::Type::getInt8Ty(getVMContext()), Field1Off.getQuantity()));
-
-  CoerceElts.push_back(Field1Ty);
-  UnpaddedCoerceElts.push_back(Field1Ty);
-
-  if (!Field2Ty) {
-    return ABIArgInfo::getCoerceAndExpand(
-        llvm::StructType::get(getVMContext(), CoerceElts, !Field1Off.isZero()),
-        UnpaddedCoerceElts[0]);
-  }
-
-  CharUnits Field2Align =
-      CharUnits::fromQuantity(getDataLayout().getABITypeAlignment(Field2Ty));
-  CharUnits Field1End =
-      Field1Off +
-      CharUnits::fromQuantity(getDataLayout().getTypeStoreSize(Field1Ty));
-  CharUnits Field2OffNoPadNoPack = Field1End.alignTo(Field2Align);
-
-  CharUnits Padding = CharUnits::Zero();
-  if (Field2Off > Field2OffNoPadNoPack)
-    Padding = Field2Off - Field2OffNoPadNoPack;
-  else if (Field2Off != Field2Align && Field2Off > Field1End)
-    Padding = Field2Off - Field1End;
-
-  bool IsPacked = !Field2Off.isMultipleOf(Field2Align);
-
-  if (!Padding.isZero())
-    CoerceElts.push_back(llvm::ArrayType::get(
-        llvm::Type::getInt8Ty(getVMContext()), Padding.getQuantity()));
-
-  CoerceElts.push_back(Field2Ty);
-  UnpaddedCoerceElts.push_back(Field2Ty);
-
-  return ABIArgInfo::getCoerceAndExpand(
-      llvm::StructType::get(getVMContext(), CoerceElts, IsPacked),
-      llvm::StructType::get(getVMContext(), UnpaddedCoerceElts, IsPacked));
-}
-
-ABIArgInfo LoongArchABIInfo::classifyArgumentType(QualType Ty, bool IsFixed,
-                                                  int &GARsLeft,
-                                                  int &FARsLeft) const {
-  assert(GARsLeft <= NumGARs && "GAR tracking underflow");
-  Ty = useFirstFieldIfTransparentUnion(Ty);
-
-  // Structures with either a non-trivial destructor or a non-trivial
-  // copy constructor are always passed indirectly.
-  if (CGCXXABI::RecordArgABI RAA = getRecordArgABI(Ty, getCXXABI())) {
-    if (GARsLeft)
-      GARsLeft -= 1;
-    return getNaturalAlignIndirect(Ty, /*ByVal=*/RAA ==
-                                           CGCXXABI::RAA_DirectInMemory);
-  }
-
-  // Ignore empty structs/unions.
-  if (isEmptyRecord(getContext(), Ty, true))
-    return ABIArgInfo::getIgnore();
-
-  uint64_t Size = getContext().getTypeSize(Ty);
-
-  // Pass floating point values via FARs if possible.
-  if (IsFixed && Ty->isFloatingType() && !Ty->isComplexType() &&
-      FRLen >= Size && FARsLeft) {
-    FARsLeft--;
-    return ABIArgInfo::getDirect();
-  }
-
-  // Complex types for the *f or *d ABI must be passed directly rather than
-  // using CoerceAndExpand.
-  if (IsFixed && Ty->isComplexType() && FRLen && FARsLeft >= 2) {
-    QualType EltTy = Ty->castAs<ComplexType>()->getElementType();
-    if (getContext().getTypeSize(EltTy) <= FRLen) {
-      FARsLeft -= 2;
-      return ABIArgInfo::getDirect();
-    }
-  }
-
-  if (IsFixed && FRLen && Ty->isStructureOrClassType()) {
-    llvm::Type *Field1Ty = nullptr;
-    llvm::Type *Field2Ty = nullptr;
-    CharUnits Field1Off = CharUnits::Zero();
-    CharUnits Field2Off = CharUnits::Zero();
-    int NeededGARs = 0;
-    int NeededFARs = 0;
-    bool IsCandidate = detectFARsEligibleStruct(
-        Ty, Field1Ty, Field1Off, Field2Ty, Field2Off, NeededGARs, NeededFARs);
-    if (IsCandidate && NeededGARs <= GARsLeft && NeededFARs <= FARsLeft) {
-      GARsLeft -= NeededGARs;
-      FARsLeft -= NeededFARs;
-      return coerceAndExpandFARsEligibleStruct(Field1Ty, Field1Off, Field2Ty,
-                                               Field2Off);
-    }
-  }
-
-  uint64_t NeededAlign = getContext().getTypeAlign(Ty);
-  // Determine the number of GARs needed to pass the current argument
-  // according to the ABI. 2*GRLen-aligned varargs are passed in "aligned"
-  // register pairs, so may consume 3 registers.
-  int NeededGARs = 1;
-  if (!IsFixed && NeededAlign == 2 * GRLen)
-    NeededGARs = 2 + (GARsLeft % 2);
-  else if (Size > GRLen && Size <= 2 * GRLen)
-    NeededGARs = 2;
-
-  if (NeededGARs > GARsLeft)
-    NeededGARs = GARsLeft;
-
-  GARsLeft -= NeededGARs;
-
-  if (!isAggregateTypeForABI(Ty) && !Ty->isVectorType()) {
-    // Treat an enum type as its underlying type.
-    if (const EnumType *EnumTy = Ty->getAs<EnumType>())
-      Ty = EnumTy->getDecl()->getIntegerType();
-
-    // All integral types are promoted to GRLen width.
-    if (Size < GRLen && Ty->isIntegralOrEnumerationType())
-      return extendType(Ty);
-
-    if (const auto *EIT = Ty->getAs<BitIntType>()) {
-      if (EIT->getNumBits() < GRLen)
-        return extendType(Ty);
-      if (EIT->getNumBits() > 128 ||
-          (!getContext().getTargetInfo().hasInt128Type() &&
-           EIT->getNumBits() > 64))
-        return getNaturalAlignIndirect(Ty, /*ByVal=*/false);
-    }
-
-    return ABIArgInfo::getDirect();
-  }
-
-  // Aggregates which are <= 2*GRLen will be passed in registers if possible,
-  // so coerce to integers.
-  if (Size <= 2 * GRLen) {
-    // Use a single GRLen int if possible, 2*GRLen if 2*GRLen alignment is
-    // required, and a 2-element GRLen array if only GRLen alignment is
-    // required.
-    if (Size <= GRLen) {
-      return ABIArgInfo::getDirect(
-          llvm::IntegerType::get(getVMContext(), GRLen));
-    }
-    if (getContext().getTypeAlign(Ty) == 2 * GRLen) {
-      return ABIArgInfo::getDirect(
-          llvm::IntegerType::get(getVMContext(), 2 * GRLen));
-    }
-    return ABIArgInfo::getDirect(
-        llvm::ArrayType::get(llvm::IntegerType::get(getVMContext(), GRLen), 2));
-  }
-  return getNaturalAlignIndirect(Ty, /*ByVal=*/false);
-}
-
-ABIArgInfo LoongArchABIInfo::classifyReturnType(QualType RetTy) const {
-  if (RetTy->isVoidType())
-    return ABIArgInfo::getIgnore();
-  // The rules for return and argument types are the same, so defer to
-  // classifyArgumentType.
-  int GARsLeft = 2;
-  int FARsLeft = FRLen ? 2 : 0;
-  return classifyArgumentType(RetTy, /*IsFixed=*/true, GARsLeft, FARsLeft);
-}
-
-Address LoongArchABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
-                                    QualType Ty) const {
-  CharUnits SlotSize = CharUnits::fromQuantity(GRLen / 8);
-
-  // Empty records are ignored for parameter passing purposes.
-  if (isEmptyRecord(getContext(), Ty, true)) {
-    Address Addr = Address(CGF.Builder.CreateLoad(VAListAddr),
-                           getVAListElementType(CGF), SlotSize);
-    Addr = CGF.Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(Ty));
-    return Addr;
-  }
-
-  auto TInfo = getContext().getTypeInfoInChars(Ty);
-
-  // Arguments bigger than 2*GRLen bytes are passed indirectly.
-  return emitVoidPtrVAArg(CGF, VAListAddr, Ty,
-                          /*IsIndirect=*/TInfo.Width > 2 * SlotSize, TInfo,
-                          SlotSize,
-                          /*AllowHigherAlign=*/true);
-}
-
-ABIArgInfo LoongArchABIInfo::extendType(QualType Ty) const {
-  int TySize = getContext().getTypeSize(Ty);
-  // LA64 ABI requires unsigned 32 bit integers to be sign extended.
-  if (GRLen == 64 && Ty->isUnsignedIntegerOrEnumerationType() && TySize == 32)
-    return ABIArgInfo::getSignExtend(Ty);
-  return ABIArgInfo::getExtend(Ty);
-}
-
-namespace {
-class LoongArchTargetCodeGenInfo : public TargetCodeGenInfo {
-public:
-  LoongArchTargetCodeGenInfo(CodeGen::CodeGenTypes &CGT, unsigned GRLen,
-                             unsigned FRLen)
-      : TargetCodeGenInfo(
-            std::make_unique<LoongArchABIInfo>(CGT, GRLen, FRLen)) {}
-};
-} // namespace
-
-//===----------------------------------------------------------------------===//
 // Driver code
 //===----------------------------------------------------------------------===//
 
@@ -12299,7 +11441,7 @@
   case llvm::Triple::riscv32:
   case llvm::Triple::riscv64: {
     StringRef ABIStr = getTarget().getABI();
-    unsigned XLen = getTarget().getPointerWidth(LangAS::Default);
+    unsigned XLen = getTarget().getPointerWidth(0);
     unsigned ABIFLen = 0;
     if (ABIStr.endswith("f"))
       ABIFLen = 32;
@@ -12374,29 +11516,7 @@
     return SetCGInfo(new SPIRVTargetCodeGenInfo(Types));
   case llvm::Triple::ve:
     return SetCGInfo(new VETargetCodeGenInfo(Types));
-  case llvm::Triple::csky: {
-    bool IsSoftFloat = !getTarget().hasFeature("hard-float-abi");
-    bool hasFP64 = getTarget().hasFeature("fpuv2_df") ||
-                   getTarget().hasFeature("fpuv3_df");
-    return SetCGInfo(new CSKYTargetCodeGenInfo(Types, IsSoftFloat ? 0
-                                                      : hasFP64   ? 64
-                                                                  : 32));
   }
-  case llvm::Triple::bpfeb:
-  case llvm::Triple::bpfel:
-    return SetCGInfo(new BPFTargetCodeGenInfo(Types));
-  case llvm::Triple::loongarch32:
-  case llvm::Triple::loongarch64: {
-    StringRef ABIStr = getTarget().getABI();
-    unsigned ABIFRLen = 0;
-    if (ABIStr.endswith("f"))
-      ABIFRLen = 32;
-    else if (ABIStr.endswith("d"))
-      ABIFRLen = 64;
-    return SetCGInfo(new LoongArchTargetCodeGenInfo(
-        Types, getTarget().getPointerWidth(LangAS::Default), ABIFRLen));
-  }
-  }
 }
 
 /// Create an OpenCL kernel for an enqueued block.
@@ -12407,19 +11527,23 @@
 llvm::Function *
 TargetCodeGenInfo::createEnqueuedBlockKernel(CodeGenFunction &CGF,
                                              llvm::Function *Invoke,
-                                             llvm::Type *BlockTy) const {
+                                             llvm::Value *BlockLiteral) const {
   auto *InvokeFT = Invoke->getFunctionType();
+  llvm::SmallVector<llvm::Type *, 2> ArgTys;
+  for (auto &P : InvokeFT->params())
+    ArgTys.push_back(P);
   auto &C = CGF.getLLVMContext();
   std::string Name = Invoke->getName().str() + "_kernel";
-  auto *FT = llvm::FunctionType::get(llvm::Type::getVoidTy(C),
-                                     InvokeFT->params(), false);
+  auto *FT = llvm::FunctionType::get(llvm::Type::getVoidTy(C), ArgTys, false);
   auto *F = llvm::Function::Create(FT, llvm::GlobalValue::ExternalLinkage, Name,
                                    &CGF.CGM.getModule());
   auto IP = CGF.Builder.saveIP();
   auto *BB = llvm::BasicBlock::Create(C, "entry", F);
   auto &Builder = CGF.Builder;
   Builder.SetInsertPoint(BB);
-  llvm::SmallVector<llvm::Value *, 2> Args(llvm::make_pointer_range(F->args()));
+  llvm::SmallVector<llvm::Value *, 2> Args;
+  for (auto &A : F->args())
+    Args.push_back(&A);
   llvm::CallInst *call = Builder.CreateCall(Invoke, Args);
   call->setCallingConv(Invoke->getCallingConv());
   Builder.CreateRetVoid();
@@ -12437,10 +11561,11 @@
 /// has "enqueued-block" function attribute and kernel argument metadata.
 llvm::Function *AMDGPUTargetCodeGenInfo::createEnqueuedBlockKernel(
     CodeGenFunction &CGF, llvm::Function *Invoke,
-    llvm::Type *BlockTy) const {
+    llvm::Value *BlockLiteral) const {
   auto &Builder = CGF.Builder;
   auto &C = CGF.getLLVMContext();
 
+  auto *BlockTy = BlockLiteral->getType()->getPointerElementType();
   auto *InvokeFT = Invoke->getFunctionType();
   llvm::SmallVector<llvm::Type *, 2> ArgTys;
   llvm::SmallVector<llvm::Metadata *, 8> AddressQuals;
@@ -12482,8 +11607,8 @@
   auto *Cast = Builder.CreatePointerCast(BlockPtr, InvokeFT->getParamType(0));
   llvm::SmallVector<llvm::Value *, 2> Args;
   Args.push_back(Cast);
-  for (llvm::Argument &A : llvm::drop_begin(F->args()))
-    Args.push_back(&A);
+  for (auto I = F->arg_begin() + 1, E = F->arg_end(); I != E; ++I)
+    Args.push_back(I);
   llvm::CallInst *call = Builder.CreateCall(Invoke, Args);
   call->setCallingConv(Invoke->getCallingConv());
   Builder.CreateRetVoid();
